[
  {
    "objectID": "mmm/index.html",
    "href": "mmm/index.html",
    "title": "Marketing Mix Modeling",
    "section": "",
    "text": "Marketing Mix Modeling (MMM) is a statistical analysis technique that helps in obtaining insights and planning marketing strategies. It is tightly related to Time Series Analysis — we can think of MMM as a special case of Time Series forecasting, where the goal is to understand the incrementality of different exogenous variables on the target variable.\nWhen Prophetverse was created, the objective was to provide a more up-to-date implementation of Facebook’s Prophet model and to add features and customization options that were not available in the original implementation. However, as the library evolved, it became clear that it could be used for more than just forecasting and that it could be a powerful tool for MMM.\nProphetverse has the following features that make it a great choice for MMM:\nThe following effects may be of interest if you are working on MMM:",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "mmm/index.html#related-libraries",
    "href": "mmm/index.html#related-libraries",
    "title": "Marketing Mix Modeling",
    "section": "Related Libraries",
    "text": "Related Libraries\nI invite you to check out other libraries for MMM. Two of them are:\n\nPyMC-Marketing: This is an amazing project by PyMC’s developers. It is a library that provides a set of tools for building Bayesian models for marketing analytics. The documentation is very comprehensive and a great source of information.\nLightweight-MMM: This library, as far as I know, was created by Google developers based on NumPyro. Now, they are developing a new one called Meridian.",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "mmm/budget_allocation.html",
    "href": "mmm/budget_allocation.html",
    "title": "Budget Optimization for Single and Multiple Time Series",
    "section": "",
    "text": "In this tutorial, you’ll learn how to use Prophetverse’s budget-optimization module to:\nYou’ll also see how to switch between different parametrizations without hassle, such as:\nBy the end, you’ll know how to pick the right setup for your campaign goals and make adjustments in seconds.",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "mmm/budget_allocation.html#setting-up-the-problem",
    "href": "mmm/budget_allocation.html#setting-up-the-problem",
    "title": "Budget Optimization for Single and Multiple Time Series",
    "section": "1.1. Setting Up the Problem",
    "text": "1.1. Setting Up the Problem\nFirst, let’s set up our environment and load the data for a single time series optimization.\n\nimport numpyro\n\nnumpyro.enable_x64()\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport pandas as pd\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n\n1.1.1. Load synthetic data\nWe will load a synthetic dataset and a pre-fitted Prophetverse model.\n\nfrom prophetverse.datasets._mmm.dataset1 import get_dataset\n\ny, X, lift_tests, true_components, model = get_dataset()\n\n\n\n1.1.2. Utility plotting functions\nThis helper function will allow us to compare spend before and after optimization.\n\n\nCode\ndef plot_spend_comparison(\n    X_baseline,\n    X_optimized,\n    channels,\n    indexer,\n    *,\n    baseline_title=\"Baseline Spend: Pre-Optimization\",\n    optimized_title=\"Optimized Spend: Maximizing KPI\",\n    figsize=(8, 4),\n):\n    fig, ax = plt.subplots(1, 2, figsize=figsize)\n\n    X_baseline.loc[indexer, channels].plot(ax=ax[0], linewidth=2)\n    X_optimized.loc[indexer, channels].plot(ax=ax[1], linewidth=2, linestyle=\"--\")\n\n    ax[0].set_title(baseline_title, fontsize=14, weight=\"bold\")\n    ax[1].set_title(optimized_title, fontsize=14, weight=\"bold\")\n\n    for a in ax:\n        a.set_ylabel(\"Spend\")\n        a.set_xlabel(\"Date\")\n        a.legend(loc=\"upper right\", frameon=True)\n        a.grid(axis=\"x\", visible=False)\n        a.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n        a.xaxis.set_major_formatter(mdates.DateFormatter(\"%b\"))\n\n    # Align y-axis\n    y_max = max(\n        X_baseline.loc[indexer, channels].max().max(),\n        X_optimized.loc[indexer, channels].max().max(),\n    )\n    for a in ax:\n        a.set_ylim(0, y_max * 1.05)\n\n    plt.tight_layout()\n    return fig, ax",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "mmm/budget_allocation.html#budget-optimization",
    "href": "mmm/budget_allocation.html#budget-optimization",
    "title": "Budget Optimization for Single and Multiple Time Series",
    "section": "1.2. Budget Optimization",
    "text": "1.2. Budget Optimization\nThe budget-optimization module is composed of three main components:\n\nThe objective function: What you want to optimize (e.g., maximize KPI).\nThe constraints: Rules the optimization must follow (e.g., total budget).\nThe parametrization transform: How the problem is parametrized (e.g., daily spend vs. channel shares).\n\n\n1.2.1. Maximizing a KPI\nThe BudgetOptimizer class is the main entry point. By default, it optimizes the daily spend for each channel to maximize a given KPI.\n\nfrom prophetverse.budget_optimization import (\n    BudgetOptimizer,\n    TotalBudgetConstraint,\n    MaximizeKPI,\n)\n\nbudget_optimizer = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[TotalBudgetConstraint()],\n    options={\"disp\": True, \"maxiter\":1000},\n)\n\nLet’s define our optimization horizon:\n\nhorizon = pd.period_range(\"2004-12-01\", \"2004-12-31\", freq=\"D\")\n\nNow, we run the optimization:\n\nimport time\n\nstart_time = time.time()\nX_opt = budget_optimizer.optimize(\n    model=model,\n    X=X,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\noptimization_time = time.time() - start_time\nprint(f\"Optimization completed in {optimization_time:.2f} seconds\")\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -754679704.680454\n            Iterations: 101\n            Function evaluations: 109\n            Gradient evaluations: 98\nOptimization completed in 2.69 seconds\n\n\n\nBaseline vs. optimized spend\nLet’s compare the model’s predictions before and after the optimization.\n\ny_pred_baseline = model.predict(X=X, fh=horizon)\ny_pred_opt = model.predict(X=X_opt, fh=horizon)\n\nfig, ax = plot_spend_comparison(\n    X,\n    X_opt,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    horizon,\n)\n\nkpi_gain = y_pred_opt.sum() / y_pred_baseline.sum() - 1\nfig.suptitle(f\"KPI gain: +{kpi_gain:.2%}\", fontsize=16,weight=\"bold\", y=1.02)\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n\n\n\n\n\n\n1.2.2. Reparametrization: Optimizing channel share\nInstead of daily spend, we can optimize the share of the budget for each channel. This is useful for keeping a fixed spending pattern (e.g., for seasonal campaigns).\n\nfrom prophetverse.budget_optimization import InvestmentPerChannelTransform\n\nbudget_optimizer_reparam = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[TotalBudgetConstraint()],\n    parametrization_transform=InvestmentPerChannelTransform(),\n    options={\"disp\": True},\n)\n\nX_opt_reparam = budget_optimizer_reparam.optimize(\n    model=model,\n    X=X,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -738733788.6357936\n            Iterations: 9\n            Function evaluations: 19\n            Gradient evaluations: 8\n\n\n\nBaseline vs. optimized spend\n\ny_pred_opt_reparam = model.predict(X=X_opt_reparam, fh=horizon)\n\nfig, ax = plot_spend_comparison(\n    X,\n    X_opt_reparam,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    horizon,\n)\n\nkpi_gain = y_pred_opt_reparam.sum() / y_pred_baseline.sum() - 1\nfig.suptitle(f\"KPI gain: +{kpi_gain:.2%}\", fontsize=16, weight=\"bold\", y=1.02)\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n\n\n\n\n\n\n1.2.3. Minimizing budget to reach a target\nWe can also change the objective to find the minimum investment required to achieve a specific KPI target. Let’s say we want a 30% increase in KPI compared to 2003.\n\nfrom prophetverse.budget_optimization import (\n    MinimizeBudget,\n    MinimumTargetResponse,\n)\n\ntarget = y.loc[\"2003-12\"].sum() * 1.30\n\nbudget_optimizer_min = BudgetOptimizer(\n    objective=MinimizeBudget(),\n    constraints=[MinimumTargetResponse(target_response=target, constraint_type=\"eq\")],\n    options={\"disp\": True, \"maxiter\" : 300},\n)\n\nX0 = X.copy()\nX_opt_min = budget_optimizer_min.optimize(\n    model=model,\n    X=X0,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 3796555.3065513894\n            Iterations: 231\n            Function evaluations: 234\n            Gradient evaluations: 231\n\n\n\nBudget and prediction comparison\n\nplot_spend_comparison(\n    X0,\n    X_opt_min,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    indexer=horizon,\n)\nplt.show()\n\ny_pred_baseline_min = model.predict(X=X0, fh=horizon)\ny_pred_opt_min = model.predict(X=X_opt_min, fh=horizon)\n\nprint(\n    f\"MMM Predictions \\n\",\n    f\"Baseline KPI: {y_pred_baseline_min.sum()/1e9:.2f} B \\n\",\n    f\"Optimized KPI: {y_pred_opt_min.sum()/1e9:.2f} B \\n\",\n    f\"Target KPI: {target/1e9:.2f} B \\n\",\n    \"Baseline spend: \",\n    X0.loc[horizon, [\"ad_spend_search\", \"ad_spend_social_media\"]].sum().sum(),\n    \"\\n\",\n    \"Optimized spend: \",\n    X_opt_min.loc[horizon, [\"ad_spend_search\", \"ad_spend_social_media\"]].sum().sum(),\n    \"\\n\",\n)\n\n\n\n\n\n\n\n\nMMM Predictions \n Baseline KPI: 0.73 B \n Optimized KPI: 0.97 B \n Target KPI: 0.97 B \n Baseline spend:  1250679.3427392421 \n Optimized spend:  3796555.3065513903",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "mmm/budget_allocation.html#setting-up-the-problem-for-panel-data",
    "href": "mmm/budget_allocation.html#setting-up-the-problem-for-panel-data",
    "title": "Budget Optimization for Single and Multiple Time Series",
    "section": "2.1. Setting Up the Problem for Panel Data",
    "text": "2.1. Setting Up the Problem for Panel Data\nThe main difference is that for panel data, we use a multi-index DataFrame, following sktime conventions.\n\n2.1.1. Load synthetic panel data\n\nfrom prophetverse.datasets._mmm.dataset1_panel import get_dataset\n\ny_panel, X_panel, lift_tests_panel, true_components_panel, fitted_model_panel = get_dataset()\n\ny_panel\n\n\n\n\n\n\n\n\n\n0\n\n\ngroup\ndate\n\n\n\n\n\na\n2000-01-01\n1.120218e+07\n\n\n2000-01-02\n1.146048e+07\n\n\n2000-01-03\n1.156324e+07\n\n\n2000-01-04\n1.161396e+07\n\n\n2000-01-05\n1.162758e+07\n\n\n...\n...\n...\n\n\nb\n2004-12-28\n2.473478e+07\n\n\n2004-12-29\n2.718986e+07\n\n\n2004-12-30\n2.554932e+07\n\n\n2004-12-31\n2.343510e+07\n\n\n2005-01-01\n2.078426e+07\n\n\n\n\n3656 rows × 1 columns\n\n\n\n\n\n2.1.2. Utility plotting functions for panel data\nWe’ll define a new plotting function to handle the multi-indexed data.\n\n\nCode\ndef plot_spend_comparison_panel(\n    X_baseline,\n    X_optimized,\n    channels,\n    indexer,\n    *,\n    baseline_title=\"Baseline Spend: Pre-Optimization\",\n    optimized_title=\"Optimized Spend: Maximizing KPI\",\n    figsize=(8, 4),\n):\n    series_idx = X_baseline.index.droplevel(-1).unique().tolist()\n    fig, axs = plt.subplots(len(series_idx), 2, figsize=figsize, squeeze=False)\n\n    for i, series in enumerate(series_idx):\n        _X_baseline = X_baseline.loc[series]\n        _X_optimized = X_optimized.loc[series]\n        ax_row = axs[i]\n        _X_baseline.loc[indexer, channels].plot(ax=ax_row[0], linewidth=2)\n        _X_optimized.loc[indexer, channels].plot(\n            ax=ax_row[1], linewidth=2, linestyle=\"--\"\n        )\n\n        ax_row[0].set_title(f\"{series}: {baseline_title}\", fontsize=14, weight=\"bold\")\n        ax_row[1].set_title(f\"{series}: {optimized_title}\", fontsize=14, weight=\"bold\")\n\n        for a in ax_row:\n            a.set_ylabel(\"Spend\")\n            a.set_xlabel(\"Date\")\n            a.legend(loc=\"upper right\", frameon=True)\n            a.grid(axis=\"x\", visible=False)\n            a.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n            a.xaxis.set_major_formatter(mdates.DateFormatter(\"%b\"))\n\n        y_max = max(\n            _X_baseline.loc[indexer, channels].max().max(),\n            _X_optimized.loc[indexer, channels].max().max(),\n        )\n        for a in ax_row:\n            a.set_ylim(0, y_max * 1.05)\n\n    plt.tight_layout()\n    return fig, axs",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "mmm/budget_allocation.html#budget-optimization-for-panel-data",
    "href": "mmm/budget_allocation.html#budget-optimization-for-panel-data",
    "title": "Budget Optimization for Single and Multiple Time Series",
    "section": "2.2. Budget Optimization for Panel Data",
    "text": "2.2. Budget Optimization for Panel Data\n\n2.2.1. Maximizing a KPI\nBy default, BudgetOptimizer will optimize the daily spend for each channel for each series.\n\nbudget_optimizer_panel = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[TotalBudgetConstraint()],\n    options={\"disp\": True, \"maxiter\": 1000},\n)\n\nX_opt_panel = budget_optimizer_panel.optimize(\n    model=fitted_model_panel,\n    X=X_panel,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -1723014730.7621593\n            Iterations: 204\n            Function evaluations: 203\n            Gradient evaluations: 203\n\n\n\nBaseline vs. optimized spend\n\ny_pred_baseline_panel = fitted_model_panel.predict(X=X_panel, fh=horizon)\ny_pred_opt_panel = fitted_model_panel.predict(X=X_opt_panel, fh=horizon)\n\nfig, ax = plot_spend_comparison_panel(\n    X_panel,\n    X_opt_panel,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    horizon,\n)\n\nkpi_gain = y_pred_opt_panel.values.sum() / y_pred_baseline_panel.values.sum() - 1\nfig.suptitle(f\"Total KPI gain: +{kpi_gain:.2%}\", fontsize=16, weight=\"bold\", y=1.03)\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n\n\n\n\n\n\n2.2.2. Reparametrization for Panel Data\nWith panel data, we have more reparametrization options.\n\nOptimizing channel share (globally)\nThis optimizes the share of budget for each channel, keeping the total investment and spending pattern fixed. The channel shares are the same across all series.\n\nfrom prophetverse.budget_optimization import InvestmentPerChannelTransform\n\nbudget_optimizer_panel_ch = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[TotalBudgetConstraint()],\n    parametrization_transform=InvestmentPerChannelTransform(),\n    options={\"disp\": True},\n)\n\nX_opt_panel_ch = budget_optimizer_panel_ch.optimize(\n    model=fitted_model_panel,\n    X=X_panel,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n# You can plot the results using plot_spend_comparison_panel\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -1674672737.6774194\n            Iterations: 13\n            Function evaluations: 13\n            Gradient evaluations: 13\n\n\n\n\nOptimizing investment per series\nThis keeps the channel shares fixed within each series but optimizes the allocation of the total budget across the different series.\n\nfrom prophetverse.budget_optimization.parametrization_transformations import InvestmentPerSeries\n\nbudget_optimizer_panel_s = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[TotalBudgetConstraint()],\n    parametrization_transform=InvestmentPerSeries(),\n    options={\"disp\": True},\n)\n\nX_opt_panel_s = budget_optimizer_panel_s.optimize(\n    model=fitted_model_panel,\n    X=X_panel,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n# You can plot the results using plot_spend_comparison_panel\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -1680449341.603541\n            Iterations: 13\n            Function evaluations: 13\n            Gradient evaluations: 13\n\n\n\n\nOptimizing share per channel and series\nThis is the most granular reparametrization, optimizing the share of budget for each channel within each series.\n\nfrom prophetverse.budget_optimization.parametrization_transformations import InvestmentPerChannelAndSeries\n\nbudget_optimizer_panel_cs = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[TotalBudgetConstraint()],\n    parametrization_transform=InvestmentPerChannelAndSeries(),\n    options={\"disp\": True},\n)\n\nX_opt_panel_cs = budget_optimizer_panel_cs.optimize(\n    model=fitted_model_panel,\n    X=X_panel,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n# You can plot the results using plot_spend_comparison_panel\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -1717879098.7459538\n            Iterations: 27\n            Function evaluations: 26\n            Gradient evaluations: 26\n\n\n\n\n\n2.2.3. Minimizing budget to reach a target with Panel Data\nLet’s find the minimum budget to achieve a 20% KPI increase across all series.\n\ntarget_panel = y_panel.loc[pd.IndexSlice[:, horizon],].values.sum() * 1.2\n\nbudget_optimizer_min_panel = BudgetOptimizer(\n    objective=MinimizeBudget(),\n    constraints=[MinimumTargetResponse(target_response=target_panel, constraint_type=\"eq\")],\n    options={\"disp\": True, \"maxiter\": 300},\n)\n\nX0_panel = X_panel.copy()\nX_opt_min_panel = budget_optimizer_min_panel.optimize(\n    model=fitted_model_panel,\n    X=X0_panel,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 7755666.679734465\n            Iterations: 285\n            Function evaluations: 287\n            Gradient evaluations: 285\n\n\n\nBudget and prediction comparison\n\nplot_spend_comparison_panel(\n    X0_panel,\n    X_opt_min_panel,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    indexer=horizon,\n)\nplt.show()\n\ny_pred_baseline_min_panel = fitted_model_panel.predict(X=X0_panel, fh=horizon)\ny_pred_opt_min_panel = fitted_model_panel.predict(X=X_opt_min_panel, fh=horizon)\n\nprint(\n    f\"MMM Predictions \\n\",\n    f\"Baseline KPI: {y_pred_baseline_min_panel.values.sum()/1e9:.2f} B \\n\",\n    f\"Optimized KPI: {y_pred_opt_min_panel.values.sum()/1e9:.2f} B \\n\",\n    f\"Target KPI: {target_panel/1e9:.2f} B \\n\",\n    \"Baseline spend: \",\n    X0_panel.loc[\n        pd.IndexSlice[:, horizon], [\"ad_spend_search\", \"ad_spend_social_media\"]\n    ]\n    .sum()\n    .sum(),\n    \"\\n\",\n    \"Optimized spend: \",\n    X_opt_min_panel.loc[\n        pd.IndexSlice[:, horizon], [\"ad_spend_search\", \"ad_spend_social_media\"]\n    ]\n    .sum()\n    .sum(),\n    \"\\n\",\n)\n\n\n\n\n\n\n\n\nMMM Predictions \n Baseline KPI: 1.63 B \n Optimized KPI: 1.96 B \n Target KPI: 1.96 B \n Baseline spend:  4179163.3068621266 \n Optimized spend:  7755666.679734466",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "the-theory.html",
    "href": "the-theory.html",
    "title": "Mathematical formulation",
    "section": "",
    "text": "Figure 1: Generalized Additive Models are versatile. Prophet is one of the many models that can be built on top of it. The idea of Prophetverse is giving access to that universe.\nProphetverse leverages the Generalized Additive Model (GAM) idea in the original Prophet model and extends it to be more flexible and customizable. The core principle of GAMs is to model the expected value \\(y_{mean}\\) of the endogenous variable \\(Y\\) as the sum of many functions \\(\\{f_i\\}_{i=1}^n\\) of exogenous variables \\(\\{x_i\\}_{i=1}^n\\).\n\\[\ny_{mean} = f_1(x_1) + f_2(x_2) + \\ldots + f_n(t)\\text{, }\\quad n \\in \\mathbb{N}\n\\]\nThe innovation in Prophet is the use of Bayesian GAMs to model time series data. Instead of approximating the time series through auto-regressive models, Prophet treats it as a curve-fitting exercise. This approach results in fast, interpretable, and accurate forecasts. The Prophet formulation is:\n\\[\ny_{\\text{mean}} = \\begin{cases} y_{\\text{mean}}(t) = \\tau(t) + s(t) + h(t) + v(t) & \\text{if additive} \\\\ y_{\\text{mean}}(t) = \\tau(t) + \\tau(t) \\cdot s(t) + \\tau(t) \\cdot h(t) + \\tau(t) \\cdot v(t) & \\text{if multiplicative} \\end{cases}\n\\]\nwhere \\(\\tau(t)\\) is the trend component, \\(s(t)\\) is the seasonality component, \\(h(t)\\) is the holiday component, and \\(v(t)\\) is other regressors components. Those components are hard-coded as linear in the original formulation of Facebook Prophet, but in Prophetverse they are versatile and can be defined by the user. This is the first main difference between Prophet and Prophetverse. The \\(f_i\\) functions are defined by the Effects API , where the user can create their own components and priors, by using the already available ones or by creating new BaseEffect subclasses.\n\\[\\begin{align}\ny_{mean} &= \\sum\\limits_{i=1}^n f_i(x_i(t), \\{f_j(x_j)\\}_{j&lt;i}) \\\\\n         &= f_1(x_1(t)) + f_2(x_2(t), f_1(x_1(t))) + \\ldots + f_n(t, \\{f_j(x_j)\\}_{j&lt;n})\n\\end{align}\\]\nwhere\nThis definition superseeds the Prophet formulation because effects are ordered, so that the output of previous effects can be used as input for the next ones. This allows for complex interactions between exogenous variables."
  },
  {
    "objectID": "the-theory.html#likelihood",
    "href": "the-theory.html#likelihood",
    "title": "Mathematical formulation",
    "section": "Likelihood",
    "text": "Likelihood\nIn the original Prophet, the likelihood is a Normal distribution, but in Prophetverse it can be Normal, Gamma, or Negative Binomial.\n\\[\ny \\sim \\mathcal{likelihood}(\\phi(\\hat{y}_{mean}), \\sigma^2)\\quad \\text{where} \\quad\n\\sigma \\sim HalfNormal(\\sigma_{hyper})\n\\]\nwhere \\(\\sigma_{hyper}\\) is a hyperparameter and \\(\\phi\\) is a function that maps the mean to the support of the likelihood. For normal likelihood, \\(\\phi\\) is the identity function, but for Gamma and Negative Binomial, it is\n\\[\n\\phi(k) = \\begin{cases}\nk & \\text{if } k &gt; z \\\\\nz\\exp(k-z) & \\text{if } k \\leq z\n\\end{cases}\n\\]\nfor some small threshold \\(z\\). We set \\(z = 10^{-5}\\) in our implementation. The reason for this is to avoid zero or negative values in the support of the likelihood, which can lead to error."
  },
  {
    "objectID": "the-theory.html#trend",
    "href": "the-theory.html#trend",
    "title": "Mathematical formulation",
    "section": "Trend",
    "text": "Trend\nThere are mainly two types of trends supported: linear and logistic. We will first take a look at the original mathematical formulation of Prophet’s paper, and then simplify it to obtain a simpler and more interpretable version.\n\nLinear trend\n\nOriginal formulation\nThe linear trend is modeled as a piecewise linear functions with changepoints. Let \\(M\\) be the number of changepoints, \\(\\delta \\in \\mathbb{R}^M\\) be the rate adjustment at each changepoint, \\(\\{\\kappa_i\\}_{i=1}^M\\) be the changepoint times, and be \\(a(t) \\in \\{0,1\\}^M\\) be a vector which assumes, at each index, 1 if the corresponding changepoint is greater than \\(t\\) and 0 otherwise. In addition, let \\(k\\) represent the global rate and \\(m\\) the global offset. Then, the linear trend is defined as:\n\\[\n\\tau(t) = (k + a(t)^T\\delta)t + (m + a(t)^T\\gamma), \\quad \\text{where} \\quad \\gamma_i = \\kappa_i\\delta_i\n\\]\nThe first part accounts for the rate adjustment at each changepoint, and the second part corrects the offset at each changepoint, so that the trend is continuous.\n\n\nProphetverse’s equivalent formulation\nThis can be simplified as a first-order spline regression with \\(M\\) knots (changepoints). Let \\(b(t) \\in \\mathbb{R}^M\\) be a vector so that \\(b(t)_i = (t - \\kappa_i)_+\\) (the positive part of \\(t - \\kappa_i\\)). Then, the piecewise linear trend value for time \\(t\\) can be written as:\n\\[\n\\tau(t) = b(t)^T \\delta + kt + m\n\\]\nWe can also write the trend for all \\(t \\in \\{t_1,\\dots, t_T\\}\\) as a matrix multiplication. Let \\(\\mathbf{B} \\in \\mathbb{R}^{T \\times M+2}\\) be the matrix whose rows are \\(b'(t) = \\left[ b(t), t, 1 \\right]\\) for each time \\(t\\). In other words, it is the spline basis matrix. The \\(t\\) and \\(1\\) at the end of the vector are included to account for the global rate and offset. Furthermore, consider the vector \\(\\delta' = \\left[ \\delta, k, m \\right]\\). Then, the trend vector \\(G \\in \\mathbb{R}^T\\), \\(G_i = \\tau(\\mathbf{t}_i)\\), can be written as:\n\\[\\begin{align}\nG &= \\mathbf{B}\\delta' \\\\\n\\end{align}\\]\n\\[\\begin{align}\nG &=  \\begin{bmatrix}\n(t_0 - \\kappa_0)_+ & (t_0 - \\kappa_1)_+ & \\ldots & (t_0 - \\kappa_{M-1})_+ & t_0 & 1 \\\\\n(t_1 - \\kappa_0)_+ & (t_1 - \\kappa_1)_+ & \\ldots & (t_1 - \\kappa_{M-1})_+ & t_1 & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n(t_{T-1} - \\kappa_0)_+ & (t_{T-1} - \\kappa_1)_+ & \\ldots & (t_{T-1} - \\kappa_{M-1})_+ & t_{T-1} & 1 \\\\\n\\end{bmatrix} \\begin{bmatrix}\n\\delta_0 \\\\\n\\delta_1 \\\\\n\\vdots \\\\\n\\delta_{M-1} \\\\\nk \\\\\nm \\\\\n\\end{bmatrix}\n\\end{align}\\]\n\n\n\n\n\n\nExample\n\n\n\nOne possible realization of \\(\\mathbf{B}\\) is:\n\\[\n\\begin{bmatrix}\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 1 \\\\\n1 & 0 & 0 & 0 & 2 & 1 \\\\\n2 & 1 & 0 & 0 & 3 & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\nT-1 & T-2 & T-3 & T-4 & T-1 & 1 \\\\\n\\end{bmatrix}\n\\]\n\n\n\n\n\nLogistic trend\nThe logistic trend of the original model uses the piecewise logistic linear trend to change the rate at which \\(t\\) grows. We will not explain the mathematical formulation of the original paper here, and will already leverage what we have learned about the linear trend to simplify it.\n\\[\nG = \\frac{C}{1 + \\exp(-\\mathbf{B}\\delta')}\n\\]\nwhere \\(C\\) is the logistic capacity, which should be passed as input to Prophet, but is a random variable in Prophetverse.\n\n\nChangepoint priors\nA Laplace prior is put on the rate adjustment \\(\\delta_i \\sim Laplace(0, \\sigma_{\\delta})\\) where \\(\\sigma_{\\delta}\\) is a hyperparameter. The changepoint times \\(\\kappa_i\\) can be predefined by the user, or can be uniformly distributed in the training data. The offset and rate prior location are set in a “smart” way, by checking analytically what would be the values that fit the maximum and minimum points of the time series.\n\n\n\n\n\n\nNote\n\n\n\nAlthough those trend are the ones that come with the library, the user can define any trend, including a trend that depends on some exogenous variable. Flexibility is the key here."
  },
  {
    "objectID": "the-theory.html#seasonality",
    "href": "the-theory.html#seasonality",
    "title": "Mathematical formulation",
    "section": "Seasonality",
    "text": "Seasonality\nTo model seasonality, Prophetverse uses a Fourier series to approximate periodic functions, allowing the model to fit complex seasonal patterns flexibly. This approach involves determining the number of Fourier terms (K), which corresponds to the complexity of the seasonality. The formula for a seasonal component s(t) in terms of a Fourier series is given as:\n\\[\ns(t) = \\sum_{k=1}^K \\left( a_k \\cos\\left(\\frac{2\\pi kt}{P}\\right) + b_k \\sin\\left(\\frac{2\\pi kt}{P}\\right) \\right)\n\\]\nHere, P is the period (e.g., 365.25 for yearly seasonality), and \\(a_k\\) and \\(b_k\\) are the Fourier coefficients that the model estimates. The choice of K depends on the granularity of the seasonal changes one wishes to capture. A Normal prior is placed on the coefficients, \\(a_k, b_k \\sim \\mathcal{N}(0, \\sigma_s)\\), where \\(\\sigma_s\\) is a hyperparameter.\nSee LinearFourierSeasonality for more details on the hyperparameters of the effect\n\nMatrix Formulation of Fourier Series\nTo efficiently compute the seasonality for multiple time points, we can represent the Fourier series in a matrix form. This method is especially useful for handling large datasets and simplifies the implementation of the model in computational software. Let \\(T\\) be the number of time points, and create a design matrix \\(X\\) of size \\(T \\times 2K\\). Each row of \\(X\\) corresponds to a time point and contains all Fourier basis functions evaluated at that time:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n\\cos\\left(\\frac{2\\pi \\cdot 1 \\cdot t_1}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot 1 \\cdot t_1}{P}\\right) & \\cdots & \\cos\\left(\\frac{2\\pi \\cdot K \\cdot t_1}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot K \\cdot t_1}{P}\\right) \\\\\n\\cos\\left(\\frac{2\\pi \\cdot 1 \\cdot t_2}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot 1 \\cdot t_2}{P}\\right) & \\cdots & \\cos\\left(\\frac{2\\pi \\cdot K \\cdot t_2}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot K \\cdot t_2}{P}\\right) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\cos\\left(\\frac{2\\pi \\cdot 1 \\cdot t_T}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot 1 \\cdot t_T}{P}\\right) & \\cdots & \\cos\\left(\\frac{2\\pi \\cdot K \\cdot t_T}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot K \\cdot t_T}{P}\\right)\n\\end{bmatrix}\n\\]\nCoefficient Vector:\nDefine a vector \\(\\beta\\) of length 2K containing the coefficients \\(a_1, b_1, \\dots, a_K, b_K\\):\n\\[\n\\mathbf{\\beta} = \\begin{bmatrix}\na_1 \\\\\nb_1 \\\\\n\\vdots \\\\\na_K \\\\\nb_K\n\\end{bmatrix}\n\\]\nThe seasonality for all time points can then be computed through the matrix product of \\(X\\) and \\(\\beta\\):\n\\[\n\\mathbf{s} = \\mathbf{X} \\mathbf{\\beta}\n\\]\nEach element of vector \\(s\\), denoted as \\(s_i\\), represents the seasonality at time \\(t_i\\).\nThis matrix approach not only makes the computation faster and more scalable but also simplifies integration with other components of the forecasting model. One drawback is that it assumes a constant seasonality, but an user can also define a seasonality that changes with time in Prophetverse, by creating a custom Effect class."
  },
  {
    "objectID": "the-theory.html#multivariate-model",
    "href": "the-theory.html#multivariate-model",
    "title": "Mathematical formulation",
    "section": "Multivariate model",
    "text": "Multivariate model\nProphetverse also supports multivariate forecasting. In this case, the model is essentially the same, but for now only Normal Likelihood is supported. Depending on the usage of the library, we may add other likelihoods in the future (please open an issue if you need it!). In that case, all other components are estimated in the same way, but the likelihood is a multivariate distribution. The mean of the distribution is a vector, and the covariance matrix prior is a LKJ distribution."
  },
  {
    "objectID": "reference/MultivariateNormal.html",
    "href": "reference/MultivariateNormal.html",
    "title": "MultivariateNormal",
    "section": "",
    "text": "MultivariateNormal\neffects.MultivariateNormal(\n    self,\n    noise_scale=0.05,\n    correlation_matrix_concentration=1,\n)\nBase class for effects.",
    "crumbs": [
      "Target Likelihoods",
      "MultivariateNormal"
    ]
  },
  {
    "objectID": "reference/ChainedEffects.html",
    "href": "reference/ChainedEffects.html",
    "title": "ChainedEffects",
    "section": "",
    "text": "effects.ChainedEffects(self, steps)\nChains multiple effects sequentially, applying them one after the other.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsteps\nList[BaseEffect]\nA list of effects to be applied sequentially.\nrequired",
    "crumbs": [
      "Exogenous effects",
      "ChainedEffects"
    ]
  },
  {
    "objectID": "reference/ChainedEffects.html#parameters",
    "href": "reference/ChainedEffects.html#parameters",
    "title": "ChainedEffects",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsteps\nList[BaseEffect]\nA list of effects to be applied sequentially.\nrequired",
    "crumbs": [
      "Exogenous effects",
      "ChainedEffects"
    ]
  },
  {
    "objectID": "reference/BudgetOptimizer.html",
    "href": "reference/BudgetOptimizer.html",
    "title": "BudgetOptimizer",
    "section": "",
    "text": "budget_optimization.optimizer.BudgetOptimizer(\n    self,\n    objective,\n    constraints,\n    parametrization_transform=None,\n    method='SLSQP',\n    tol=None,\n    bounds=None,\n    options=None,\n    callback=None,\n)\nBudget optimizer using scipy.optimize.minimize.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobjective\nBaseOptimizationObjective\nObjective function object\nrequired\n\n\nconstraints\nlist of BaseConstraint\nList of constraint objects\nrequired\n\n\ndecision_variable_transform\nBaseDecisionVariableTransform\nDecision variable transform object\nrequired\n\n\nmethod\nstr\nOptimization method to use. Default is “SLSQP”.\n'SLSQP'\n\n\ntol\nfloat\nTolerance for termination. Default is None.\nNone\n\n\nbounds\nUnion[List[tuple], dict[str, tuple]]\nBounds for decision variables. If a list, the value is used directly in scipy.optimize.minimize. If a dict, the keys are the column names and the values are the bounds for each column. Default is (0, np.inf) for each column.\nNone\n\n\noptions\ndict\nOptions for the optimization method. Default is None.\nNone\n\n\ncallback\ncallable\nCallback function to be called after each iteration. Default is None.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nwrap_func_with_inv_transform\nWrap a function with parametrization inverse transform\n\n\n\n\n\nbudget_optimization.optimizer.BudgetOptimizer.wrap_func_with_inv_transform(fun)\nWrap a function with parametrization inverse transform",
    "crumbs": [
      "Budget Optimization",
      "BudgetOptimizer"
    ]
  },
  {
    "objectID": "reference/BudgetOptimizer.html#parameters",
    "href": "reference/BudgetOptimizer.html#parameters",
    "title": "BudgetOptimizer",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nobjective\nBaseOptimizationObjective\nObjective function object\nrequired\n\n\nconstraints\nlist of BaseConstraint\nList of constraint objects\nrequired\n\n\ndecision_variable_transform\nBaseDecisionVariableTransform\nDecision variable transform object\nrequired\n\n\nmethod\nstr\nOptimization method to use. Default is “SLSQP”.\n'SLSQP'\n\n\ntol\nfloat\nTolerance for termination. Default is None.\nNone\n\n\nbounds\nUnion[List[tuple], dict[str, tuple]]\nBounds for decision variables. If a list, the value is used directly in scipy.optimize.minimize. If a dict, the keys are the column names and the values are the bounds for each column. Default is (0, np.inf) for each column.\nNone\n\n\noptions\ndict\nOptions for the optimization method. Default is None.\nNone\n\n\ncallback\ncallable\nCallback function to be called after each iteration. Default is None.\nNone",
    "crumbs": [
      "Budget Optimization",
      "BudgetOptimizer"
    ]
  },
  {
    "objectID": "reference/BudgetOptimizer.html#methods",
    "href": "reference/BudgetOptimizer.html#methods",
    "title": "BudgetOptimizer",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nwrap_func_with_inv_transform\nWrap a function with parametrization inverse transform\n\n\n\n\n\nbudget_optimization.optimizer.BudgetOptimizer.wrap_func_with_inv_transform(fun)\nWrap a function with parametrization inverse transform",
    "crumbs": [
      "Budget Optimization",
      "BudgetOptimizer"
    ]
  },
  {
    "objectID": "reference/SharedBudgetConstraint.html",
    "href": "reference/SharedBudgetConstraint.html",
    "title": "TotalBudgetConstraint",
    "section": "",
    "text": "budget_optimization.constraints.TotalBudgetConstraint(\n    self,\n    channels=None,\n    total=None,\n)\nShared budget constraint.\nThis constraint ensures that the sum of the budgets for the specified channels is equal to the total budget.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nchannels\nlist\nList of channels to be constrained. If None, all channels are used.\nNone\n\n\ntotal\nfloat\nTotal budget. If None, the total budget is computed from the input data.\nNone"
  },
  {
    "objectID": "reference/SharedBudgetConstraint.html#parameters",
    "href": "reference/SharedBudgetConstraint.html#parameters",
    "title": "TotalBudgetConstraint",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nchannels\nlist\nList of channels to be constrained. If None, all channels are used.\nNone\n\n\ntotal\nfloat\nTotal budget. If None, the total budget is computed from the input data.\nNone"
  },
  {
    "objectID": "reference/HierarchicalProphet.html",
    "href": "reference/HierarchicalProphet.html",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "sktime.HierarchicalProphet(\n    self,\n    trend='linear',\n    feature_transformer=None,\n    exogenous_effects=None,\n    default_effect=None,\n    shared_features=None,\n    noise_scale=0.05,\n    correlation_matrix_concentration=1.0,\n    rng_key=None,\n    inference_engine=None,\n    likelihood=None,\n)\nA Bayesian hierarchical time series forecasting model based on Meta’s Prophet.\nThis method forecasts all bottom series in a hierarchy at once, using a MultivariateNormal as the likelihood function and LKJ priors for the correlation matrix.\nThis forecaster is particularly interesting if you want to fit shared coefficients across series. In that case, shared_features parameter should be a list of feature names that should have that behaviour.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[BaseEffect, str]\nTrend component of the model.\n\"linear\"\n\n\nfeature_transformer\nBaseTransformer\nTransformer for features preprocessing.\nNone\n\n\nexogenous_effects\noptional\nEffects to model exogenous variables.\nNone\n\n\ndefault_effect\noptional\nDefault effect specification.\nNone\n\n\nshared_features\noptional\nFeatures shared across time series.\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the noise distribution.\n0.05\n\n\ncorrelation_matrix_concentration\nfloat\nConcentration parameter for the correlation matrix.\n1.0\n\n\nrng_key\noptional\nRandom number generator key.\nNone\n\n\ninference_engine\noptional\nEngine used for inference.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; from sktime.forecasting.naive import NaiveForecaster\n&gt;&gt;&gt; from sktime.transformations.hierarchical.aggregate import Aggregator\n&gt;&gt;&gt; from sktime.utils._testing.hierarchical import _bottom_hier_datagen\n&gt;&gt;&gt; from prophetverse.sktime.multivariate import HierarchicalProphet\n&gt;&gt;&gt; agg = Aggregator()\n&gt;&gt;&gt; y = _bottom_hier_datagen(\n...     no_bottom_nodes=3,\n...     no_levels=1,\n...     random_seed=123,\n...     length=7,\n... )\n&gt;&gt;&gt; y = agg.fit_transform(y)\n&gt;&gt;&gt; forecaster = HierarchicalProphet()\n&gt;&gt;&gt; forecaster = forecaster.fit(y)\n&gt;&gt;&gt; y_pred = forecaster.predict(fh=[1])\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nn_series\nGet the number of series.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_test_params\nParams to be used in sktime unit tests.\n\n\npredict_samples\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nsktime.HierarchicalProphet.get_test_params(parameter_set='default')\nParams to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set to be used (ignored in this implementation)\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing the test parameters.\n\n\n\n\n\n\n\nsktime.HierarchicalProphet.predict_samples(fh, X=None)\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame\nExogenous variables.\nNone\n\n\nfh\nForecastingHorizon\nForecasting horizon.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nPredicted samples.",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html#parameters",
    "href": "reference/HierarchicalProphet.html#parameters",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[BaseEffect, str]\nTrend component of the model.\n\"linear\"\n\n\nfeature_transformer\nBaseTransformer\nTransformer for features preprocessing.\nNone\n\n\nexogenous_effects\noptional\nEffects to model exogenous variables.\nNone\n\n\ndefault_effect\noptional\nDefault effect specification.\nNone\n\n\nshared_features\noptional\nFeatures shared across time series.\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the noise distribution.\n0.05\n\n\ncorrelation_matrix_concentration\nfloat\nConcentration parameter for the correlation matrix.\n1.0\n\n\nrng_key\noptional\nRandom number generator key.\nNone\n\n\ninference_engine\noptional\nEngine used for inference.\nNone",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html#examples",
    "href": "reference/HierarchicalProphet.html#examples",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "&gt;&gt;&gt; from sktime.forecasting.naive import NaiveForecaster\n&gt;&gt;&gt; from sktime.transformations.hierarchical.aggregate import Aggregator\n&gt;&gt;&gt; from sktime.utils._testing.hierarchical import _bottom_hier_datagen\n&gt;&gt;&gt; from prophetverse.sktime.multivariate import HierarchicalProphet\n&gt;&gt;&gt; agg = Aggregator()\n&gt;&gt;&gt; y = _bottom_hier_datagen(\n...     no_bottom_nodes=3,\n...     no_levels=1,\n...     random_seed=123,\n...     length=7,\n... )\n&gt;&gt;&gt; y = agg.fit_transform(y)\n&gt;&gt;&gt; forecaster = HierarchicalProphet()\n&gt;&gt;&gt; forecaster = forecaster.fit(y)\n&gt;&gt;&gt; y_pred = forecaster.predict(fh=[1])",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html#attributes",
    "href": "reference/HierarchicalProphet.html#attributes",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nn_series\nGet the number of series.",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html#methods",
    "href": "reference/HierarchicalProphet.html#methods",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_test_params\nParams to be used in sktime unit tests.\n\n\npredict_samples\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nsktime.HierarchicalProphet.get_test_params(parameter_set='default')\nParams to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set to be used (ignored in this implementation)\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing the test parameters.\n\n\n\n\n\n\n\nsktime.HierarchicalProphet.predict_samples(fh, X=None)\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame\nExogenous variables.\nNone\n\n\nfh\nForecastingHorizon\nForecasting horizon.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nPredicted samples.",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/GammaTargetLikelihood.html",
    "href": "reference/GammaTargetLikelihood.html",
    "title": "GammaTargetLikelihood",
    "section": "",
    "text": "GammaTargetLikelihood\neffects.GammaTargetLikelihood(self, noise_scale=0.05, epsilon=1e-05)",
    "crumbs": [
      "Target Likelihoods",
      "GammaTargetLikelihood"
    ]
  },
  {
    "objectID": "reference/NegativeBinomialTargetLikelihood.html",
    "href": "reference/NegativeBinomialTargetLikelihood.html",
    "title": "NegativeBinomialTargetLikelihood",
    "section": "",
    "text": "NegativeBinomialTargetLikelihood\neffects.NegativeBinomialTargetLikelihood(self, noise_scale=0.05, epsilon=1e-05)",
    "crumbs": [
      "Target Likelihoods",
      "NegativeBinomialTargetLikelihood"
    ]
  },
  {
    "objectID": "reference/MaximizeKPI.html",
    "href": "reference/MaximizeKPI.html",
    "title": "MaximizeKPI",
    "section": "",
    "text": "MaximizeKPI\nbudget_optimization.objectives.MaximizeKPI(self)\nMaximize the KPI objective function.",
    "crumbs": [
      "Objective Functions",
      "MaximizeKPI"
    ]
  },
  {
    "objectID": "reference/budget_optimization.BudgetOptimizer.html",
    "href": "reference/budget_optimization.BudgetOptimizer.html",
    "title": "budget_optimization.BudgetOptimizer",
    "section": "",
    "text": "budget_optimization.BudgetOptimizer(\n    self,\n    objective,\n    constraints,\n    parametrization_transform=None,\n    method='SLSQP',\n    tol=None,\n    bounds=None,\n    options=None,\n    callback=None,\n)\nBudget optimizer using scipy.optimize.minimize.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobjective\nBaseOptimizationObjective\nObjective function object\nrequired\n\n\nconstraints\nlist of BaseConstraint\nList of constraint objects\nrequired\n\n\ndecision_variable_transform\nBaseDecisionVariableTransform\nDecision variable transform object\nrequired\n\n\nmethod\nstr\nOptimization method to use. Default is “SLSQP”.\n'SLSQP'\n\n\ntol\nfloat\nTolerance for termination. Default is None.\nNone\n\n\nbounds\nUnion[List[tuple], dict[str, tuple]]\nBounds for decision variables. If a list, the value is used directly in scipy.optimize.minimize. If a dict, the keys are the column names and the values are the bounds for each column. Default is (0, np.inf) for each column.\nNone\n\n\noptions\ndict\nOptions for the optimization method. Default is None.\nNone\n\n\ncallback\ncallable\nCallback function to be called after each iteration. Default is None.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nwrap_func_with_inv_transform\nWrap a function with parametrization inverse transform\n\n\n\n\n\nbudget_optimization.BudgetOptimizer.wrap_func_with_inv_transform(\n    fun,\n)\nWrap a function with parametrization inverse transform"
  },
  {
    "objectID": "reference/budget_optimization.BudgetOptimizer.html#parameters",
    "href": "reference/budget_optimization.BudgetOptimizer.html#parameters",
    "title": "budget_optimization.BudgetOptimizer",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nobjective\nBaseOptimizationObjective\nObjective function object\nrequired\n\n\nconstraints\nlist of BaseConstraint\nList of constraint objects\nrequired\n\n\ndecision_variable_transform\nBaseDecisionVariableTransform\nDecision variable transform object\nrequired\n\n\nmethod\nstr\nOptimization method to use. Default is “SLSQP”.\n'SLSQP'\n\n\ntol\nfloat\nTolerance for termination. Default is None.\nNone\n\n\nbounds\nUnion[List[tuple], dict[str, tuple]]\nBounds for decision variables. If a list, the value is used directly in scipy.optimize.minimize. If a dict, the keys are the column names and the values are the bounds for each column. Default is (0, np.inf) for each column.\nNone\n\n\noptions\ndict\nOptions for the optimization method. Default is None.\nNone\n\n\ncallback\ncallable\nCallback function to be called after each iteration. Default is None.\nNone"
  },
  {
    "objectID": "reference/budget_optimization.BudgetOptimizer.html#methods",
    "href": "reference/budget_optimization.BudgetOptimizer.html#methods",
    "title": "budget_optimization.BudgetOptimizer",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nwrap_func_with_inv_transform\nWrap a function with parametrization inverse transform\n\n\n\n\n\nbudget_optimization.BudgetOptimizer.wrap_func_with_inv_transform(\n    fun,\n)\nWrap a function with parametrization inverse transform"
  },
  {
    "objectID": "reference/NormalTargetLikelihood.html",
    "href": "reference/NormalTargetLikelihood.html",
    "title": "NormalTargetLikelihood",
    "section": "",
    "text": "NormalTargetLikelihood\neffects.NormalTargetLikelihood(self, noise_scale=0.05)",
    "crumbs": [
      "Target Likelihoods",
      "NormalTargetLikelihood"
    ]
  },
  {
    "objectID": "reference/FlatTrend.html",
    "href": "reference/FlatTrend.html",
    "title": "FlatTrend",
    "section": "",
    "text": "effects.FlatTrend(self, changepoint_prior_scale=0.1)\nFlat trend model.\nThe mean of the target variable is used as the prior location for the trend.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nchangepoint_prior_scale\nfloat\nThe scale of the prior distribution on the trend changepoints. Defaults to 0.1.\n0.1",
    "crumbs": [
      "Trends",
      "FlatTrend"
    ]
  },
  {
    "objectID": "reference/FlatTrend.html#parameters",
    "href": "reference/FlatTrend.html#parameters",
    "title": "FlatTrend",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nchangepoint_prior_scale\nfloat\nThe scale of the prior distribution on the trend changepoints. Defaults to 0.1.\n0.1",
    "crumbs": [
      "Trends",
      "FlatTrend"
    ]
  },
  {
    "objectID": "reference/MinimumTargetResponse.html",
    "href": "reference/MinimumTargetResponse.html",
    "title": "MinimumTargetResponse",
    "section": "",
    "text": "budget_optimization.constraints.MinimumTargetResponse(\n    self,\n    target_response,\n    constraint_type='ineq',\n)\nMinimum target response constraint.\nThis constraint ensures that the target response is greater than or equal to a specified value. This imposes a restriction on the output of the model, instead of the input.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntarget_response\nfloat\nTarget response value. The model output must be greater than or equal to this value.\nrequired",
    "crumbs": [
      "Budget Constraints",
      "MinimumTargetResponse"
    ]
  },
  {
    "objectID": "reference/MinimumTargetResponse.html#parameters",
    "href": "reference/MinimumTargetResponse.html#parameters",
    "title": "MinimumTargetResponse",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntarget_response\nfloat\nTarget response value. The model output must be greater than or equal to this value.\nrequired",
    "crumbs": [
      "Budget Constraints",
      "MinimumTargetResponse"
    ]
  },
  {
    "objectID": "reference/LogEffect.html",
    "href": "reference/LogEffect.html",
    "title": "LogEffect",
    "section": "",
    "text": "effects.LogEffect(\n    self,\n    effect_mode='multiplicative',\n    scale_prior=None,\n    rate_prior=None,\n)\nRepresents a log effect as effect = scale * log(rate * data + 1).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nscale_prior\nOptional[Distribution]\nThe prior distribution for the scale parameter., by default Gamma\nNone\n\n\nrate_prior\nOptional[Distribution]\nThe prior distribution for the rate parameter., by default Gamma\nNone\n\n\neffect_mode\neffects_application\nEither “additive” or “multiplicative”, by default “multiplicative”\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "LogEffect"
    ]
  },
  {
    "objectID": "reference/LogEffect.html#parameters",
    "href": "reference/LogEffect.html#parameters",
    "title": "LogEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nscale_prior\nOptional[Distribution]\nThe prior distribution for the scale parameter., by default Gamma\nNone\n\n\nrate_prior\nOptional[Distribution]\nThe prior distribution for the rate parameter., by default Gamma\nNone\n\n\neffect_mode\neffects_application\nEither “additive” or “multiplicative”, by default “multiplicative”\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "LogEffect"
    ]
  },
  {
    "objectID": "reference/LinearFourierSeasonality.html",
    "href": "reference/LinearFourierSeasonality.html",
    "title": "LinearFourierSeasonality",
    "section": "",
    "text": "effects.LinearFourierSeasonality(\n    self,\n    sp_list,\n    fourier_terms_list,\n    freq,\n    prior_scale=1.0,\n    effect_mode='additive',\n    linear_effect=None,\n)\nLinear Fourier Seasonality effect.\nCompute the linear seasonality using Fourier features.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsp_list\nList[float]\nList of seasonal periods.\nrequired\n\n\nfourier_terms_list\nList[int]\nList of number of Fourier terms to use for each seasonal period.\nrequired\n\n\nfreq\nstr\nFrequency of the time series. Example: “D” for daily, “W” for weekly, etc.\nrequired\n\n\nprior_scale\nfloat\nScale of the prior distribution for the effect, by default 1.0.\n1.0\n\n\neffect_mode\nstr\nEither “multiplicative” or “additive” by default “additive”.\n'additive'",
    "crumbs": [
      "Exogenous effects",
      "LinearFourierSeasonality"
    ]
  },
  {
    "objectID": "reference/LinearFourierSeasonality.html#parameters",
    "href": "reference/LinearFourierSeasonality.html#parameters",
    "title": "LinearFourierSeasonality",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsp_list\nList[float]\nList of seasonal periods.\nrequired\n\n\nfourier_terms_list\nList[int]\nList of number of Fourier terms to use for each seasonal period.\nrequired\n\n\nfreq\nstr\nFrequency of the time series. Example: “D” for daily, “W” for weekly, etc.\nrequired\n\n\nprior_scale\nfloat\nScale of the prior distribution for the effect, by default 1.0.\n1.0\n\n\neffect_mode\nstr\nEither “multiplicative” or “additive” by default “additive”.\n'additive'",
    "crumbs": [
      "Exogenous effects",
      "LinearFourierSeasonality"
    ]
  },
  {
    "objectID": "howto/index.html",
    "href": "howto/index.html",
    "title": "How-to",
    "section": "",
    "text": "In this documentation section, you will find how you can create more advanced patterns\n\n  \n    \n      \n        **Custom Time Series Component**\n        Learn how to craft a custom time series component and unlock powerful forecasting enhancements.\n        → Custom Effect\n      \n    \n  \n\n  \n    \n      \n        Custom Trend\n        Follow a hands-on example to create a custom trend component and integrate it seamlessly into your forecasts.\n        → Custom Trend\n      \n    \n  \n\n  \n    \n      \n        Composition of Time Series Components\n        Discover how to combine multiple time series components for more nuanced and accurate forecasting models.\n        → Composite Exogenous Effect"
  },
  {
    "objectID": "howto/effect_api_intro.html",
    "href": "howto/effect_api_intro.html",
    "title": "Customizing exogenous effects",
    "section": "",
    "text": "Custom effect\n\n\n\n\n\nCheckout how to create a non-linear effect using the exogenous effect API, and expand the model with custom effects.\n\nClick here!\n\n\n\n\n\n\n\n\n\n\n\nCustom trend\n\n\n\n\n\nYou may also wish to customize the trend component of the model. We have an interesting applied example!\n\nSee here!\n\n\n\n\n\n\n\n\n\n\n\nComposition of effects\n\n\n\n\n\nYou can create effect that capture a custom interaction between different exogenous variables.\n\nSee here!\nThe exogenous effect API allows you to create custom exogenous components for the Prophetverse model. This is useful when we want to model specific patterns or relationships between the exogenous variables and the target variable. For example, enforcing a positive effect of a variable on the mean, or modeling a non-linear relationship.\nIf you have read the theory section, by effect we mean each function \\(f_i\\). You can implement those custom functions by subclassing the BaseEffect class, and then use them in the Prophetverse model. Some effects are already implemented in the library, and you can find them in the prophetverse.effects module.\nWhen creating a model instance, effects can be specified through exogenous_effects parameter of the Prophetverse model. This parameter is a list of tuples of three values: the name, the effect object, and a regex to filter columns related to that effect. The regex is what defines \\(x_i\\) in the previous section. The prophetverse.utils.regex module provides some useful functions to create regex patterns for common use cases, include starts_with, ends_with, contains, and no_input_columns.\nFor example:\nThe effects can be any object that implements the BaseEffect interface, and you can create your own effects by subclassing BaseEffect and implementing _fit, _transform and _predict methods.",
    "crumbs": [
      "Introduction to Effects"
    ]
  },
  {
    "objectID": "howto/effect_api_intro.html#example",
    "href": "howto/effect_api_intro.html#example",
    "title": "Customizing exogenous effects",
    "section": "Example",
    "text": "Example\n\nLog Effect\nThe BaseAdditiveOrMultiplicativeEffect provides an init argument effect_mode that allows you to specify if the effect is additive or multiplicative. Let’s take as an example the LogEffect:\n#prophetverse/effects/log.py\n\nfrom typing import Dict, Optional\n\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro.distributions import Distribution\n\nfrom prophetverse.effects.base import (\n    EFFECT_APPLICATION_TYPE,\n    BaseAdditiveOrMultiplicativeEffect,\n)\n\n__all__ = [\"LogEffect\"]\n\n\nclass LogEffect(BaseAdditiveOrMultiplicativeEffect):\n    \"\"\"Represents a log effect as effect = scale * log(rate * data + 1).\n\n    Parameters\n    ----------\n    scale_prior : Optional[Distribution], optional\n        The prior distribution for the scale parameter., by default Gamma\n    rate_prior : Optional[Distribution], optional\n        The prior distribution for the rate parameter., by default Gamma\n    effect_mode : effects_application, optional\n        Either \"additive\" or \"multiplicative\", by default \"multiplicative\"\n    \"\"\"\n\n    def __init__(\n        self,\n        effect_mode: EFFECT_APPLICATION_TYPE = \"multiplicative\",\n        scale_prior: Optional[Distribution] = None,\n        rate_prior: Optional[Distribution] = None,\n    ):\n        self.scale_prior = scale_prior or dist.Gamma(1, 1)\n        self.rate_prior = rate_prior or dist.Gamma(1, 1)\n        super().__init__(effect_mode=effect_mode)\n\n    def _predict(  # type: ignore[override]\n        self,\n        data: jnp.ndarray,\n        predicted_effects: Optional[Dict[str, jnp.ndarray]] = None,\n    ) -&gt; jnp.ndarray:\n        \"\"\"Apply and return the effect values.\n\n        Parameters\n        ----------\n        data : Any\n            Data obtained from the transformed method.\n\n        predicted_effects : Dict[str, jnp.ndarray], optional\n            A dictionary containing the predicted effects, by default None.\n\n        Returns\n        -------\n        jnp.ndarray\n            An array with shape (T,1) for univariate timeseries, or (N, T, 1) for\n            multivariate timeseries, where T is the number of timepoints and N is the\n            number of series.\n        \"\"\"\n        scale = numpyro.sample(\"log_scale\", self.scale_prior)\n        rate = numpyro.sample(\"log_rate\", self.rate_prior)\n        effect = scale * jnp.log(jnp.clip(rate * data + 1, 1e-8, None))\n\n        return effect\nThe _fit and _transform methods are not implemented, and the default behaviour is preserved (the columns of the dataframe that match the regex pattern are selected, and the result is converted to a jnp.ndarray with key “data”).\n\n\nComposition of effects\nWe can go further and create a custom effect that adds a likelihood term to the model. The LiftExperimentLikelihood tackles the use case of having a lift experiment, and wanting to incorporate it to guide the exogenous effect. The likelihood term is added in the _predict method, and the observed lift preprocessed in _transform method.\n\n\n\n\n\n\nTip\n\n\n\nTo see more, check the custom effect how-to.",
    "crumbs": [
      "Introduction to Effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html",
    "href": "howto/composite_effects.html",
    "title": "Composition of effects",
    "section": "",
    "text": "In previous examples, we saw how to create a simple custom effect, which applies a simple transformation to the input data. However, the effect’s interface allows us to apply more complex transformations, such as using the output of previous components as input for the current component, or creating a composite effect that wraps an effect and applies some sort of transformation. This example will cover these topics.",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#creating-a-custom-effect",
    "href": "howto/composite_effects.html#creating-a-custom-effect",
    "title": "Composition of effects",
    "section": "Creating a custom effect",
    "text": "Creating a custom effect\nThe idea here is to create an effect that uses another predicted component to scale the impact of an exogenous variable.\nOne classic use-case for this would be using seasonality to scale the effect of investment, that might be proportional to it. Marketing investments are a good example of this. We will implement such a composite effect in this section.\n\nExample dataset\nThe dataset we use is synthetic, and the relation between the exogenous variable and the target is known. However, let’s pretend we don’t know this relation, and analize the data to find some insights that motivate the creation of a custom effect. The dataset has a target variable, which is a time series, and an exogenous variable, which is the investment made for each date.\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom matplotlib import pyplot as plt\nfrom sktime.split import temporal_train_test_split\nfrom sktime.utils.plotting import plot_series\n\nfrom prophetverse.datasets.synthetic import load_composite_effect_example\n\nnumpyro.enable_x64()\n\ny, X = load_composite_effect_example()\n\ny_train, y_test, X_train, X_test = temporal_train_test_split(y, X, test_size=365)\n\ndisplay(y_train.head())\ndisplay(X_train.head())\n\n/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n\n\n\n\n\n\ntarget\n\n\ntime\n\n\n\n\n\n2010-01-01\n29.375431\n\n\n2010-01-02\n30.268786\n\n\n2010-01-03\n29.128912\n\n\n2010-01-04\n31.014165\n\n\n2010-01-05\n31.890928\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvestment\n\n\ntime\n\n\n\n\n\n2010-01-01\n0.198274\n\n\n2010-01-02\n0.198274\n\n\n2010-01-03\n0.198274\n\n\n2010-01-04\n0.198274\n\n\n2010-01-05\n0.207695\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8,5))\nplot_series(y_train, y_test, labels=[\"Train\", \"Test\"], title=\"Target series\", ax=ax)\nfig.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_series(X[\"investment\"], labels=[\"investment\"], title=\"Features\", ax=ax)\nfig.show()\n\n\n\n\n\n\n\n\nThe timeseries has a yearly seasonality, and it seems that some oscillations are proportional to the investment. Below, we model the timeseries with a simple linear effect between the investment and the target, and a yearly seasonality based on fourier terms. Then, we will analize the residuals to see if there is any pattern that we can capture with a custom effect.\n\nfrom prophetverse.effects import LinearEffect\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.engine.optimizer import LBFGSSolver\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import exact, no_input_columns\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-500,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                freq=\"D\",\n                sp_list=[365.25],\n                fourier_terms_list=[5],\n                prior_scale=1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n        (\n            \"investment\",\n            LinearEffect(\"multiplicative\", prior=dist.Normal(0, 1)),\n            exact(\"investment\"),\n        ),\n    ],\n    inference_engine=MAPInferenceEngine(\n        optimizer=LBFGSSolver(memory_size=100, max_linesearch_steps=100),\n        progress_bar=True,\n    ),\n)\n\nmodel.fit(y=y_train, X=X_train)\nmodel\n\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:244: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n  0%|          | 0/1 [00:00&lt;?, ?it/s]100%|██████████| 1/1 [00:07&lt;00:00,  7.27s/it, init loss: -5352.1071, avg. loss [1-1]: -5352.1071]100%|██████████| 1/1 [00:07&lt;00:00,  7.27s/it, init loss: -5352.1071, avg. loss [1-1]: -5352.1071]\n\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=1,\n                                                          sp_list=[365.25]),\n                                 '^$'),\n                                ('investment',\n                                 LinearEffect(prior=&lt;numpyro.distributions.continuous.Normal object at 0x7eff03043210 with batch shape () and event shape ()&gt;),\n                                 '^investment$')],\n             inference_engine=MAPInferenceEngine(optimizer=LBFGSSolver(max_linesearch_steps=100,\n                                                                       memory_size=100),\n                                                 progress_bar=True),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=1,\n                                                          sp_list=[365.25]),\n                                 '^$'),\n                                ('investment',\n                                 LinearEffect(prior=&lt;numpyro.distributions.continuous.Normal object at 0x7eff03043210 with batch shape () and event shape ()&gt;),\n                                 '^investment$')],\n             inference_engine=MAPInferenceEngine(optimizer=LBFGSSolver(max_linesearch_steps=100,\n                                                                       memory_size=100),\n                                                 progress_bar=True),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=1, sp_list=[365.25])LinearEffectLinearEffect(prior=&lt;numpyro.distributions.continuous.Normal object at 0x7eff03043210 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine(optimizer=LBFGSSolver(max_linesearch_steps=100,\n                                         memory_size=100),\n                   progress_bar=True)\n\n\nWe plot the predictions on training set to see if the model performs well.\n\ny_pred = model.predict(X=X_train, fh=y_train.index)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_series(y_train, y_pred, labels=[\"Train\", \"Pred\"], title=\"Target series\", ax=ax)\nfig.show()\n\n\n\n\n\n\n\n\nWe can see that some peaks are not captured by the model. Our hypothesis to explain this phenomenon is that the investment has more impact on the target when it is done during the positive seasonality periods. To test this, we plot the residuals of the model against the investment, and color the points based on the seasonality component. We can see that slopes are different for positive and negative seasonality, which indicates that our hypothesis is possibly correct.\n\ncomponents = model.predict_components(X=X_train, fh=y_train.index)\n\nresidual = y_train[\"target\"] - components[\"mean\"]\n\nfig, ax = plt.subplots()\nax.scatter(\n    X_train[\"investment\"],\n    residual,\n    c=components[\"seasonality\"] &lt; 0,\n    cmap=\"Accent\",\n    alpha=0.9,\n)\n# Create legend manually\ncolors = plt.cm.get_cmap(\"Accent\").colors\nax.scatter([], [], color=colors[0], label=\"Positive seasonality\")\nax.scatter([], [], color=colors[1], label=\"Negative seasonality\")\nax.legend()\nax.set(xlabel=\"Investment\", ylabel=\"Residual\", title=\"Residuals vs Investment\")\nfig.show()\n\n/tmp/ipykernel_3260/182972736.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  colors = plt.cm.get_cmap(\"Accent\").colors",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#creating-the-composite-effect",
    "href": "howto/composite_effects.html#creating-the-composite-effect",
    "title": "Composition of effects",
    "section": "Creating the composite effect",
    "text": "Creating the composite effect\nTo model this behaviour with Prophetverse, we will create a custom effect, that scales a new effect by the output of a previous component. The _fit and _transform methods call the inner effect’s methods, and the predict method multiplies the inner effect’s predictions by the seasonality, which is passed as base_effect_name.\n\nfrom typing import Any, Dict, List\n\nimport jax.numpy as jnp\nimport pandas as pd\n\nfrom prophetverse.effects.base import BaseEffect\n\n\nclass WrapEffectAndScaleByAnother(BaseEffect):\n    \"\"\"Wrap an effect and scale it by another effect.\n\n    Parameters\n    ----------\n    effect : BaseEffect\n        The effect to wrap.\n\n    \"\"\"\n\n    _tags = {\"requires_X\": False, \"hierarchical_prophet_compliant\": False}\n\n    def __init__(\n        self,\n        effect: BaseEffect,\n        base_effect_name: str,\n    ):\n\n        self.effect = effect\n        self.base_effect_name = base_effect_name\n\n        super().__init__()\n\n        self.clone_tags(effect)\n\n    def _fit(self, y: pd.DataFrame, X: pd.DataFrame, scale: float = 1):\n        \"\"\"Initialize the effect.\"\"\"\n        self.effect.fit(X=X, y=y, scale=scale)\n\n    def _transform(self, X: pd.DataFrame, fh: pd.Index) -&gt; Dict[str, Any]:\n        \"\"\"Prepare input data to be passed to numpyro model.\"\"\"\n        return self.effect.transform(X=X, fh=fh)\n\n    def _predict(\n        self, data: Dict, predicted_effects: Dict[str, jnp.ndarray], *args, **kwargs\n    ) -&gt; jnp.ndarray:\n        \"\"\"Apply and return the effect values.\"\"\"\n        out = self.effect.predict(data=data, predicted_effects=predicted_effects)\n\n        base_effect = predicted_effects[self.base_effect_name]\n        return base_effect * out",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#instantiating-the-model-with-the-composite-effect",
    "href": "howto/composite_effects.html#instantiating-the-model-with-the-composite-effect",
    "title": "Composition of effects",
    "section": "Instantiating the model with the composite effect",
    "text": "Instantiating the model with the composite effect\nTo create the model, we use the model instance we have, and the rshift operator to append the composite effect to the model.\n\nimport numpyro.distributions as dist\nfrom prophetverse.engine.optimizer import AdamOptimizer\n\ncomposite_effect_tuple = (\n    \"investment_seasonality\",  # The effect ID, can be what you want\n    WrapEffectAndScaleByAnother(\n        effect=LinearEffect(\"multiplicative\", prior=dist.HalfNormal(1)),\n        base_effect_name=\"seasonality\",\n    ),\n    exact(\"investment\"),\n)\n\n\n# We use the rshift operator to append an effect to the model\nmodel_composite = model &gt;&gt; composite_effect_tuple\n\nmodel_composite.fit(y=y_train, X=X_train)\ny_pred_composite = model_composite.predict(X=X_train, fh=y_train.index)\n\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:244: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:244: UserWarning: Columns {'investment'} are already set\n  self._fit_effects(X, y)\n  0%|          | 0/1 [00:00&lt;?, ?it/s]100%|██████████| 1/1 [00:07&lt;00:00,  7.86s/it, init loss: -6054.6937, avg. loss [1-1]: -6054.6937]100%|██████████| 1/1 [00:07&lt;00:00,  7.86s/it, init loss: -6054.6937, avg. loss [1-1]: -6054.6937]\n\n\nWe can see below how these oscilations are captured by the model correctly when adding this joint effect.\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_series(y_train, y_pred_composite, labels=[\"Train\", \"Pred\"], title=\"Target series\",ax=ax)\nfig.show()",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#evaluating-the-model-on-test-set",
    "href": "howto/composite_effects.html#evaluating-the-model-on-test-set",
    "title": "Composition of effects",
    "section": "Evaluating the model on test set",
    "text": "Evaluating the model on test set\nWe compare to the previous model to see if the new effect improved the predictions on test set:\n\ny_pred_composite = model_composite.predict(X=X_test, fh=y_test.index)\ny_pred = model.predict(X=X_test, fh=y_test.index)\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_series(\n    y_test,\n    y_pred,\n    y_pred_composite,\n    labels=[\"Test\", \"Pred\", \"Pred composite\"],\n    title=\"Target series\",\n    ax=ax,\n)\n\nplt.show()",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#extracting-the-components",
    "href": "howto/composite_effects.html#extracting-the-components",
    "title": "Composition of effects",
    "section": "Extracting the components",
    "text": "Extracting the components\nThe components can be extracted as usual, with the predict_components method.\n\ncomponents = model_composite.predict_components(fh=y_test.index, X=X_test)\n\nfig, ax = plt.subplots(figsize=(10, 5))\ncomponents.plot.line(ax=ax)\nax.set_title(\"Predicted Components\")\nfig.show()",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "tutorials/hierarchical.html",
    "href": "tutorials/hierarchical.html",
    "title": "Hierarchical Bayesian Model",
    "section": "",
    "text": "In this example, we will show how to forecast panel timeseries with the Prophetverse model.\nThe univariate Prophetverse model can seamlessly handle hierarchical timeseries due to the package’s compatibility with sktime.\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom prophetverse.datasets.loaders import load_tourism",
    "crumbs": [
      "Hierarchical Bayesian Model"
    ]
  },
  {
    "objectID": "tutorials/hierarchical.html#import-dataset",
    "href": "tutorials/hierarchical.html#import-dataset",
    "title": "Hierarchical Bayesian Model",
    "section": "Import dataset",
    "text": "Import dataset\nHere we use the tourism dataset with purpose-level aggregation.\n\ny = load_tourism(groupby=\"Purpose\")\ndisplay(y)\n\n\n\n\n\n\n\n\n\nTrips\n\n\nPurpose\nQuarter\n\n\n\n\n\nBusiness\n1998Q1\n7391.962068\n\n\n1998Q2\n7701.153191\n\n\n1998Q3\n8911.852065\n\n\n1998Q4\n7777.766525\n\n\n1999Q1\n6917.257864\n\n\n...\n...\n...\n\n\n__total\n2015Q4\n51518.858354\n\n\n2016Q1\n54984.720748\n\n\n2016Q2\n49583.595515\n\n\n2016Q3\n49392.159616\n\n\n2016Q4\n54034.155613\n\n\n\n\n380 rows × 1 columns\n\n\n\nWe define the helper function below to plot the predictions and the observations.\n\nLEVELS = y.index.get_level_values(0).unique()\n\n\ndef plot_preds(y=None, preds={}, axs=None):\n\n    if axs is None:\n        fig, axs = plt.subplots(\n            figsize=(12, 8), nrows=int(np.ceil(len(LEVELS) / 2)), ncols=2\n        )\n    ax_generator = iter(axs.flatten())\n    for level in LEVELS:\n        ax = next(ax_generator)\n        if y is not None:\n            y.loc[level].iloc[:, 0].rename(\"Observation\").plot(\n                ax=ax, label=\"truth\", color=\"black\"\n            )\n        for name, _preds in preds.items():\n            _preds.loc[level].iloc[:, 0].rename(name).plot(ax=ax, legend=True)\n        ax.set_title(level)\n\n    # Tight layout\n    plt.tight_layout()\n    return ax",
    "crumbs": [
      "Hierarchical Bayesian Model"
    ]
  },
  {
    "objectID": "tutorials/hierarchical.html#automatic-upcasting",
    "href": "tutorials/hierarchical.html#automatic-upcasting",
    "title": "Hierarchical Bayesian Model",
    "section": "Automatic upcasting",
    "text": "Automatic upcasting\nBecause of sktime’s amazing interface, we can use the univariate Prophet seamlessly with hierarchical data.\n\nimport jax.numpy as jnp\n\nfrom prophetverse.effects import LinearFourierSeasonality\nfrom prophetverse.effects.trend import PiecewiseLinearTrend, PiecewiseLogisticTrend\nfrom prophetverse.engine import MAPInferenceEngine, MCMCInferenceEngine\nfrom prophetverse.sktime.univariate import Prophetverse\nfrom prophetverse.utils import no_input_columns\nfrom prophetverse.engine.optimizer import LBFGSSolver\n\nmodel = Prophetverse(\n    trend=PiecewiseLogisticTrend(\n        changepoint_prior_scale=0.1,\n        changepoint_interval=8,\n        changepoint_range=-8,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                sp_list=[\"YE\"],\n                fourier_terms_list=[1],\n                freq=\"Q\",\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        )\n    ],\n    inference_engine=MCMCInferenceEngine(\n        num_warmup=500,\n        num_samples=1000,\n    ),\n)\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[1],\n                                                          freq='Q',\n                                                          prior_scale=0.1,\n                                                          sp_list=['YE']),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=500),\n             trend=PiecewiseLogisticTrend(changepoint_interval=8,\n                                          changepoint_prior_scale=0.1,\n                                          changepoint_range=-8))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[1],\n                                                          freq='Q',\n                                                          prior_scale=0.1,\n                                                          sp_list=['YE']),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=500),\n             trend=PiecewiseLogisticTrend(changepoint_interval=8,\n                                          changepoint_prior_scale=0.1,\n                                          changepoint_range=-8))effectsPiecewiseLogisticTrendPiecewiseLogisticTrend(changepoint_interval=8, changepoint_prior_scale=0.1,\n                       changepoint_range=-8)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[1],\n                         freq='Q', prior_scale=0.1, sp_list=['YE'])inference_engineMCMCInferenceEngineMCMCInferenceEngine(num_warmup=500)\n\n\nWe can see how, internally, sktime creates clones of the model for each timeseries instance:\n\nmodel.forecasters_\n\n\n\n\n\n\n\n\nforecasters\n\n\n\n\nBusiness\nProphetverse(exogenous_effects=[('seasonality'...\n\n\nHoliday\nProphetverse(exogenous_effects=[('seasonality'...\n\n\nOther\nProphetverse(exogenous_effects=[('seasonality'...\n\n\nVisiting\nProphetverse(exogenous_effects=[('seasonality'...\n\n\n__total\nProphetverse(exogenous_effects=[('seasonality'...\n\n\n\n\n\n\n\nTo call the same methods we used in the univariate case, we do not need to change a single line of code. The only difference is that the output will be a pd.DataFrame with more rows and index levels.\n\nforecast_horizon = pd.period_range(\"1997Q1\",\n                                   \"2020Q4\",\n                                   freq=\"Q\")\npreds = model.predict(fh=forecast_horizon)\ndisplay(preds.head())\n\n# Plot\nplot_preds(y, {\"Prophet\": preds})\nplt.show()\n\n\n\n\n\n\n\n\n\nTrips\n\n\nPurpose\nQuarter\n\n\n\n\n\nBusiness\n1997Q1\n7077.881836\n\n\n1997Q2\n8005.375977\n\n\n1997Q3\n8924.370117\n\n\n1997Q4\n7988.610352\n\n\n1998Q1\n7077.881836\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe same applies to the decomposition method:\n\ndecomposition = model.predict_components(fh=forecast_horizon)\ndecomposition.head()\n\n\n\n\n\n\n\n\n\nmean\nobs\nseasonality\ntrend\n\n\n\n\nBusiness\n1997Q1\n7077.881836\n7085.598633\n-923.084595\n8000.966309\n\n\n1997Q2\n8005.375977\n7965.196289\n4.408181\n8000.966309\n\n\n1997Q3\n8924.370117\n8941.210938\n923.403625\n8000.966309\n\n\n1997Q4\n7988.610352\n7965.733398\n-12.355662\n8000.966309\n\n\n1998Q1\n7077.881836\n7063.054199\n-923.084595\n8000.966309",
    "crumbs": [
      "Hierarchical Bayesian Model"
    ]
  },
  {
    "objectID": "tutorials/hierarchical.html#hierarchical-bayesian-model",
    "href": "tutorials/hierarchical.html#hierarchical-bayesian-model",
    "title": "Hierarchical Bayesian Model",
    "section": "Hierarchical Bayesian model",
    "text": "Hierarchical Bayesian model\nSometimes, we want to capture patterns shared between the different series, such as seasonality or trend. In this case, we can use a Bayesian Hierarchical Model.\nA Bayesian Hierarchical Model sets hyperpriors: global priors over the priors for each timeseries. These hyperpriors allow us to share information across the different series, which can lead to better forecasts, especially when some series have very few observations.\nTo do that, we just need to use PanelBHLinearEffect as the seasonality__linear_effect parameter in the model. This effect will automatically create a hierarchical model for the linear effects, allowing us to share information across the different series. Also, let us set the broadcast_mode to “off” to use a single model to all the series.\n\nfrom prophetverse.effects.linear import PanelBHLinearEffect\n\nfrom numpyro import distributions as dist\n\nmodel_hier = model.clone()\nmodel_hier.set_params(\n    seasonality__linear_effect=PanelBHLinearEffect(\n        scale_hyperprior=dist.HalfNormal(0.1)\n    ),\n    broadcast_mode=\"effect\",\n)\nmodel_hier.fit(y=y)\n\nProphetverse(broadcast_mode='effect',\n             exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[1],\n                                                          freq='Q',\n                                                          linear_effect=PanelBHLinearEffect(scale_hyperprior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f0d57bf3690 with batch shape () and event shape ()&gt;),\n                                                          prior_scale=0.1,\n                                                          sp_list=['YE']),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=500),\n             trend=PiecewiseLogisticTrend(changepoint_interval=8,\n                                          changepoint_prior_scale=0.1,\n                                          changepoint_range=-8))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(broadcast_mode='effect',\n             exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[1],\n                                                          freq='Q',\n                                                          linear_effect=PanelBHLinearEffect(scale_hyperprior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f0d57bf3690 with batch shape () and event shape ()&gt;),\n                                                          prior_scale=0.1,\n                                                          sp_list=['YE']),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=500),\n             trend=PiecewiseLogisticTrend(changepoint_interval=8,\n                                          changepoint_prior_scale=0.1,\n                                          changepoint_range=-8))effectsPiecewiseLogisticTrendPiecewiseLogisticTrend(changepoint_interval=8, changepoint_prior_scale=0.1,\n                       changepoint_range=-8)seasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[1],\n                         freq='Q',\n                         linear_effect=PanelBHLinearEffect(scale_hyperprior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f0d57bf3690 with batch shape () and event shape ()&gt;),\n                         prior_scale=0.1, sp_list=['YE'])linear_effect: PanelBHLinearEffectPanelBHLinearEffect(scale_hyperprior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f0d57bf3690 with batch shape () and event shape ()&gt;)PanelBHLinearEffectPanelBHLinearEffect(scale_hyperprior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f0d57bf3690 with batch shape () and event shape ()&gt;)inference_engineMCMCInferenceEngineMCMCInferenceEngine(num_warmup=500)\n\n\n\npreds_hier = model_hier.predict(fh=forecast_horizon)\n\nplot_preds(\n    y,\n    preds={\n        \"Prophet\": preds,\n        \"HierarchicalProphet\": preds_hier,\n    },\n)",
    "crumbs": [
      "Hierarchical Bayesian Model"
    ]
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "In this section, you can find a collection of tutorials that will help you understand the basics of Prophetverse for forecasting."
  },
  {
    "objectID": "tutorials/tuning.html",
    "href": "tutorials/tuning.html",
    "title": "Tuning Prophetverse with sktime",
    "section": "",
    "text": "Prophetverse is compatible with sktime’s tuning framework. You can define a parameter grid for components (such as trend and seasonality) and then use cross-validation tools (e.g., GridSearchCV) to search for the best parameters.",
    "crumbs": [
      "Hyperparameter tuning"
    ]
  },
  {
    "objectID": "tutorials/tuning.html#overview",
    "href": "tutorials/tuning.html#overview",
    "title": "Tuning Prophetverse with sktime",
    "section": "",
    "text": "Prophetverse is compatible with sktime’s tuning framework. You can define a parameter grid for components (such as trend and seasonality) and then use cross-validation tools (e.g., GridSearchCV) to search for the best parameters.",
    "crumbs": [
      "Hyperparameter tuning"
    ]
  },
  {
    "objectID": "tutorials/tuning.html#example-using-gridsearchcv-with-prophetverse",
    "href": "tutorials/tuning.html#example-using-gridsearchcv-with-prophetverse",
    "title": "Tuning Prophetverse with sktime",
    "section": "Example: Using GridSearchCV with Prophetverse",
    "text": "Example: Using GridSearchCV with Prophetverse\n\nImport necessary modules and load your dataset.\nDefine the hyperparameter grid for components (e.g., changepoint_interval and changepoint_prior_scale in the trend).\nCreate a Prophetverse instance with initial settings.\nWrap the model with sktime’s GridSearchCV and run the tuning process.\n\n\nLoading the data\n\nimport pandas as pd\nfrom sktime.forecasting.model_selection import ForecastingGridSearchCV\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.utils import no_input_columns\n\n# Load example dataset (replace with your own data as needed)\nfrom prophetverse.datasets.loaders import load_peyton_manning\n\ny = load_peyton_manning()\ny.head()\n\n/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n\n\n\n\n\n\ny\n\n\nds\n\n\n\n\n\n2007-12-10\n9.590761\n\n\n2007-12-11\n8.519590\n\n\n2007-12-12\n8.183677\n\n\n2007-12-13\n8.072467\n\n\n2007-12-14\n7.893572\n\n\n\n\n\n\n\n\n\nSetting the model\nWe create our model instance, before passing it to tuning.\n\n# Create the initial Prophetverse model.\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-250,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                freq=\"D\",\n                sp_list=[7, 365.25],\n                fourier_terms_list=[3, 10],\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n    ],\n    inference_engine=MAPInferenceEngine(),\n)\nmodel\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\n\nDefine the searcher\nIn sktime, the tuner is also an estimator/forecaster, so we can use the same interface as for any other sktime forecaster. We can use GridSearchCV to search for the best parameters in a given parameter grid.\n\n# Set up cv strategy\nfrom sktime.split import ExpandingWindowSplitter\n\ncv = ExpandingWindowSplitter(fh=[1, 2, 3], step_length=1000, initial_window=1000)\n\nparam_grid = {\n    \"trend__changepoint_interval\": [300, 700],\n    \"trend__changepoint_prior_scale\": [0.0001, 0.00001],\n    \"seasonality__prior_scale\": [0.1],\n}\n\n\n# Set up GridSearchCV with 3-fold cross-validation.\ngrid_search = ForecastingGridSearchCV(\n                model,\n                param_grid=param_grid,\n                cv=cv\n            )\ngrid_search\n\nForecastingGridSearchCV(cv=ExpandingWindowSplitter(fh=[1, 2, 3],\n                                                   initial_window=1000,\n                                                   step_length=1000),\n                        forecaster=Prophetverse(exogenous_effects=[('seasonality',\n                                                                    LinearFourierSeasonality(effect_mode='multiplicative',\n                                                                                             fourier_terms_list=[3,\n                                                                                                                 10],\n                                                                                             freq='D',\n                                                                                             prior_scale=0.1,\n                                                                                             sp_list=[7,\n                                                                                                      365.25]),\n                                                                    '^$')],\n                                                inference_engine=MAPInferenceEngine(),\n                                                trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                                                           changepoint_prior_scale=1e-05,\n                                                                           changepoint_range=-250)),\n                        param_grid={'seasonality__prior_scale': [0.1],\n                                    'trend__changepoint_interval': [300, 700],\n                                    'trend__changepoint_prior_scale': [0.0001,\n                                                                       1e-05]})Please rerun this cell to show the HTML repr or trust the notebook.ForecastingGridSearchCV?Documentation for ForecastingGridSearchCVForecastingGridSearchCV(cv=ExpandingWindowSplitter(fh=[1, 2, 3],\n                                                   initial_window=1000,\n                                                   step_length=1000),\n                        forecaster=Prophetverse(exogenous_effects=[('seasonality',\n                                                                    LinearFourierSeasonality(effect_mode='multiplicative',\n                                                                                             fourier_terms_list=[3,\n                                                                                                                 10],\n                                                                                             freq='D',\n                                                                                             prior_scale=0.1,\n                                                                                             sp_list=[7,\n                                                                                                      365.25]),\n                                                                    '^$')],\n                                                inference_engine=MAPInferenceEngine(),\n                                                trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                                                           changepoint_prior_scale=1e-05,\n                                                                           changepoint_range=-250)),\n                        param_grid={'seasonality__prior_scale': [0.1],\n                                    'trend__changepoint_interval': [300, 700],\n                                    'trend__changepoint_prior_scale': [0.0001,\n                                                                       1e-05]})forecaster: ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\nNow, we can call fit.\n\n# Run the grid search.\ngrid_search.fit(y=y, X=None)\n\n# Display the best parameters found.\nprint(\"Best parameters:\", grid_search.best_params_)\n\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:244: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:244: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:244: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:244: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:244: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:244: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:244: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:244: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n\n\nBest parameters: {'seasonality__prior_scale': 0.1, 'trend__changepoint_interval': 700, 'trend__changepoint_prior_scale': 1e-05}\n\n\nWe can also see the performance of each parameter combination:\n\ngrid_search.cv_results_\n\n\n\n\n\n\n\n\nmean_test_MeanAbsolutePercentageError\nmean_fit_time\nmean_pred_time\nparams\nrank_test_MeanAbsolutePercentageError\n\n\n\n\n0\n0.053898\n17.056888\n0.700231\n{'seasonality__prior_scale': 0.1, 'trend__chan...\n4.0\n\n\n1\n0.049264\n15.862377\n0.735250\n{'seasonality__prior_scale': 0.1, 'trend__chan...\n3.0\n\n\n2\n0.036172\n15.050416\n0.729718\n{'seasonality__prior_scale': 0.1, 'trend__chan...\n2.0\n\n\n3\n0.033533\n18.430322\n0.625824\n{'seasonality__prior_scale': 0.1, 'trend__chan...\n1.0\n\n\n\n\n\n\n\nOptionally, extract the best model from the grid search results.\n\nbest_model = grid_search.best_forecaster_\nbest_model\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=700,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=700,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=700, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()",
    "crumbs": [
      "Hyperparameter tuning"
    ]
  },
  {
    "objectID": "tutorials/univariate.html",
    "href": "tutorials/univariate.html",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "",
    "text": "This example shows how to use Prophetverse to perform univariate forecasting with a time series dataset, using sktime-style interface.\nBecause of this compatibility, you can benefit from all the features of sktime, such as hierarchical reconciliation, ensemble models, pipelines, etc. There are two main methods to use Prophetverse with sktime:\nLater in this example, we will also show additional methods to make predictions, such as predict_quantiles and predict_components.\nimport warnings\nwarnings.simplefilter(action=\"ignore\")\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom numpyro import distributions as dist\nimport numpyro\n\nnumpyro.enable_x64()  # To avoid computational issues",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#import-dataset",
    "href": "tutorials/univariate.html#import-dataset",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Import dataset",
    "text": "Import dataset\nWe import a dataset from Prophet’s original repository. We then put it into sktime-friendly format, where the index is a pd.PeriodIndex and the colums are the time series.\n\nfrom prophetverse.datasets.loaders import load_peyton_manning\n\ny = load_peyton_manning()\ndisplay(y.head())\n\n\n\n\n\n\n\n\ny\n\n\nds\n\n\n\n\n\n2007-12-10\n9.590761\n\n\n2007-12-11\n8.519590\n\n\n2007-12-12\n8.183677\n\n\n2007-12-13\n8.072467\n\n\n2007-12-14\n7.893572\n\n\n\n\n\n\n\nThe full dataset looks like this:\n\ny.plot.line(figsize=(8, 6))\nplt.show()",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#fit-model",
    "href": "tutorials/univariate.html#fit-model",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Fit model",
    "text": "Fit model\nHere, we will show how you can fit a simple model with Prophetverse. We first fit a model without seasonal components, and then fit a full model. We also show how easy it is to switch between Maximum A Posteriori (MAP) inference and Markov Chain Monte Carlo (MCMC).\n\nNo seasonality\n\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils import no_input_columns\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-250,\n    ),\n    inference_engine=MAPInferenceEngine(),\n)\nmodel.fit(y=y)\n\nProphetverse(inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(8, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)\n\n\n\n\n\n\n\n\n\n\nWith seasonality\nHere, we fit the univariate Prophet and pass an exogenous effect as hyperparameter. The exogenous_effects parameter let us add new components to the model and control the relationship between exogenous variables and the target variable.\nIn this case, the LinearFourierSeasonality effect creates sinusoidal and cosine terms to model the seasonality of the time series, which are then multiplied by linear coefficients and added to the model.\nThis argument is a list of tuples of the form (effect_name, effect, regex_to_filter_relevant_columns), where effect_name is a string and effect is an instance of a subclass of prophetverse.effects.BaseEffect. The regex is used to filter the columns of X that are relevant for the effect, but can also be None (or its alias prophetverse.utils.no_input_columns) if no input in X is needed for the effect.\n\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.utils import no_input_columns\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-500,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                freq=\"D\",\n                sp_list=[7, 365.25],\n                fourier_terms_list=[3, 10],\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n    ],\n    inference_engine=MAPInferenceEngine(),\n)\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(8, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#probabilistic-forecasting",
    "href": "tutorials/univariate.html#probabilistic-forecasting",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Probabilistic forecasting",
    "text": "Probabilistic forecasting\nWe can also make probabilistic forecasts with Prophetverse, in sktime fashion. The predict_quantiles method returns the quantiles of the predictive distribution in a pd.DataFrame\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.1, 0.9])\nquantiles.head()\n\n\n\n\n\n\n\n\ny\n\n\n\n0.1\n0.9\n\n\n\n\n2007-01-01\n8.047412\n9.356146\n\n\n2007-01-02\n7.836059\n9.125930\n\n\n2007-01-03\n7.785226\n9.044309\n\n\n2007-01-04\n7.715282\n9.009478\n\n\n2007-01-05\n7.754124\n9.043622\n\n\n\n\n\n\n\nThe plot below shows the (0.1, 0.9) quantiles of the predictive distribution\n\nfig, ax = plt.subplots(figsize=(8, 5))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=1)",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#timeseries-decomposition",
    "href": "tutorials/univariate.html#timeseries-decomposition",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Timeseries decomposition",
    "text": "Timeseries decomposition\nWe can easily extract the components of the time series with the predict_components method. This method, in particular, is not implemented in sktime’s BaseForecaster, but it is a method of prophetverse.Prophetverse class.\n\nsites = model.predict_components(fh=forecast_horizon)\nsites.head()\n\n\n\n\n\n\n\n\nmean\nobs\nseasonality\ntrend\n\n\n\n\n2007-01-01\n8.716897\n8.707676\n0.916926\n7.799971\n\n\n2007-01-02\n8.520570\n8.500814\n0.720600\n7.799971\n\n\n2007-01-03\n8.366813\n8.392820\n0.566843\n7.799971\n\n\n2007-01-04\n8.387161\n8.369109\n0.587191\n7.799971\n\n\n2007-01-05\n8.415625\n8.393918\n0.615654\n7.799971\n\n\n\n\n\n\n\n\nfor column in sites.columns:\n    fig, ax = plt.subplots(figsize=(8, 2))\n    ax.plot(sites.index.to_timestamp(), sites[column], label=column)\n    ax.set_title(column)\n    fig.show()",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#fitting-with-mcmc",
    "href": "tutorials/univariate.html#fitting-with-mcmc",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Fitting with MCMC",
    "text": "Fitting with MCMC\nIn the previous examples, we used MAP inference to fit the model. However, we can also use Markov Chain Monte Carlo (MCMC) to fit the model. To do this, we just need to change the inference_engine parameter to MCMCInferenceEngine. The rest of the code remains the same.\nThe set_params method is used to set the parameters of the model, in sklearn fashion.\n\nfrom prophetverse.engine import MCMCInferenceEngine\n\nmodel.set_params(inference_engine=MCMCInferenceEngine(num_warmup=1000))\n\n\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=1000),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=1000),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMCMCInferenceEngineMCMCInferenceEngine(num_warmup=1000)\n\n\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.75, 0.25])\nfig, ax = plt.subplots(figsize=(8, 5))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=1)\n\n\n\n\n\n\n\n\nOne interesting feature of MCMC is that it allows us to obtain samples from the posterior distribution of the parameters. In other words, we can also obtain probabilistic forecasts for the TS components.\n\nsamples = model.predict_component_samples(fh=forecast_horizon)\nsamples\n\n\n\n\n\n\n\n\n\nmean\nobs\nseasonality\ntrend\n\n\nsample\n\n\n\n\n\n\n\n\n\n0\n2007-01-01\n8.637136\n8.864427\n0.860843\n7.776293\n\n\n2007-01-02\n8.420668\n8.779047\n0.644375\n7.776293\n\n\n2007-01-03\n8.318044\n7.538993\n0.541751\n7.776293\n\n\n2007-01-04\n8.337156\n9.041995\n0.560863\n7.776293\n\n\n2007-01-05\n8.339964\n8.353505\n0.563671\n7.776293\n\n\n...\n...\n...\n...\n...\n...\n\n\n999\n2017-12-28\n7.525187\n6.629009\n0.389360\n7.135827\n\n\n2017-12-29\n7.550371\n6.918302\n0.415293\n7.135079\n\n\n2017-12-30\n7.321095\n7.873543\n0.186765\n7.134330\n\n\n2017-12-31\n7.704518\n7.170688\n0.570936\n7.133582\n\n\n2018-01-01\n8.000491\n8.564353\n0.867657\n7.132834\n\n\n\n\n4019000 rows × 4 columns",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#extra-syntax-sugar",
    "href": "tutorials/univariate.html#extra-syntax-sugar",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Extra: syntax sugar",
    "text": "Extra: syntax sugar\nIn Prophetverse, we’ve implemented the &gt;&gt; operator, which makes it easier to set trend, exogenous_effects and inference_engine parameters.\n\ntrend = PiecewiseLinearTrend(\n    changepoint_interval=300,\n    changepoint_prior_scale=0.0001,\n    changepoint_range=0.8,\n)\nexogenous_effects = [\n    (\n        \"seasonality\",\n        LinearFourierSeasonality(\n            freq=\"D\",\n            sp_list=[7, 365.25],\n            fourier_terms_list=[3, 10],\n            prior_scale=0.1,\n            effect_mode=\"multiplicative\",\n        ),\n        no_input_columns,\n    ),\n]\ninference_engine = MAPInferenceEngine()\n\nmodel = Prophetverse() &gt;&gt; trend &gt;&gt; exogenous_effects &gt;&gt; inference_engine\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=300,\n                                        changepoint_prior_scale=0.0001))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=300,\n                                        changepoint_prior_scale=0.0001))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=300, changepoint_prior_scale=0.0001)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(8, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/count_data.html",
    "href": "tutorials/count_data.html",
    "title": "Forecasting count data",
    "section": "",
    "text": "This tutorial shows how to use the prophetverse library to model count data, with a prophet-like model that uses Negative Binomial likelihood. Many timeseries are composed of counts, which are non-negative integers. For example, the number of cars that pass through a toll booth in a given hour, the number of people who visit a website in a given day, or the number of sales of a product. The original Prophet model struggles to handle this type of data, as it assumes that the data is continuous and normally distributed.\n# Disable warnings\nimport warnings\n\nwarnings.simplefilter(action=\"ignore\")\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom numpyro import distributions as dist\nfrom sktime.forecasting.compose import ForecastingPipeline\nfrom sktime.transformations.series.fourier import FourierFeatures\n\n\nfrom prophetverse.datasets.loaders import load_pedestrian_count\nimport numpyro\n\nnumpyro.enable_x64()",
    "crumbs": [
      "Count data forecasting"
    ]
  },
  {
    "objectID": "tutorials/count_data.html#import-dataset",
    "href": "tutorials/count_data.html#import-dataset",
    "title": "Forecasting count data",
    "section": "Import dataset",
    "text": "Import dataset\nWe use a dataset Melbourne Pedestrian Counts from forecastingdata, which contains the hourly pedestrian counts in Melbourne, Australia, from a set of sensors located in different parts of the city.\n\ny = load_pedestrian_count()\n\n# We take only one time series for simplicity\ny = y.loc[\"T2\"]\n\nsplit_index = 24 * 365\ny_train, y_test = y.iloc[:split_index], y.iloc[split_index + 1 : split_index * 2 + 1]\n\nLet’s plot a section of the time series to see how it looks like:\n\ndisplay(y_train.head())\ny_train.iloc[: 24 * 21].plot(figsize=(10, 3), marker=\"o\", color=\"black\", legend=True)\nplt.show()\n\n\n\n\n\n\n\n\npedestrian_count\n\n\ntimestamp\n\n\n\n\n\n2009-05-01 00:00\n52.0\n\n\n2009-05-01 01:00\n34.0\n\n\n2009-05-01 02:00\n19.0\n\n\n2009-05-01 03:00\n14.0\n\n\n2009-05-01 04:00\n15.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe full dataset is actually large, and plotting it all at once does not help a lot. Either way, let’s plot the full dataset to see how it looks like:\n\nax = y_train[\"pedestrian_count\"].rename(\"Train\").plot(figsize=(20, 7))\ny_test[\"pedestrian_count\"].rename(\"Test\").plot(ax=ax)\nax.legend()\nplt.show()",
    "crumbs": [
      "Count data forecasting"
    ]
  },
  {
    "objectID": "tutorials/count_data.html#fitting-models",
    "href": "tutorials/count_data.html#fitting-models",
    "title": "Forecasting count data",
    "section": "Fitting models",
    "text": "Fitting models\nThe series has some clear patterns: a daily seasonality, a weekly seasonality, and a yearly seasonality. It also has many zeros, and a model assuming normal distributed observations would not be able to capture this.\nFirst, let’s fit and forecast with the standard prophet, then see how the negative binomial model performs.",
    "crumbs": [
      "Count data forecasting"
    ]
  },
  {
    "objectID": "tutorials/count_data.html#prophet-with-normal-likelihood",
    "href": "tutorials/count_data.html#prophet-with-normal-likelihood",
    "title": "Forecasting count data",
    "section": "Prophet with normal likelihood",
    "text": "Prophet with normal likelihood\nIn this case, we will see how the model will output non-sensical negative values. The probabilistic intervals, mainly, will output values much lower than the support of the timeseries.\n\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.effects.trend import FlatTrend\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.engine.optimizer import CosineScheduleAdamOptimizer, LBFGSSolver\n\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import no_input_columns\n\n# Here we set the prior for the seasonality effect\n# And the coefficients for it\nexogenous_effects = [\n    (\n        \"seasonality\",\n        LinearFourierSeasonality(\n            sp_list=[24, 24 * 7, 24 * 365.5],\n            fourier_terms_list=[2, 2, 10],\n            freq=\"H\",\n            prior_scale=0.1,\n            effect_mode=\"multiplicative\",\n        ),\n        no_input_columns,\n    ),\n]\n\nmodel = Prophetverse(\n    trend=FlatTrend(),\n    exogenous_effects=exogenous_effects,\n    inference_engine=MAPInferenceEngine(),\n)\nmodel.fit(y=y_train)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[2,\n                                                                              2,\n                                                                              10],\n                                                          freq='H',\n                                                          prior_scale=0.1,\n                                                          sp_list=[24, 168,\n                                                                   8772.0]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(), trend=FlatTrend())Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[2,\n                                                                              2,\n                                                                              10],\n                                                          freq='H',\n                                                          prior_scale=0.1,\n                                                          sp_list=[24, 168,\n                                                                   8772.0]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(), trend=FlatTrend())effectsFlatTrendFlatTrend()LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[2, 2, 10], freq='H',\n                         prior_scale=0.1, sp_list=[24, 168, 8772.0])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nForecasting with the normal model\nBelow we see the negative predictions, which is clear a limitation of this gaussian likelihood for this kind of data.\n\nforecast_horizon = y_train.index[-100:].union(y_test.index[:300])\nfig, ax = plt.subplots(figsize=(10, 3))\npreds_normal = model.predict(fh=forecast_horizon)\npreds_normal[\"pedestrian_count\"].rename(\"Normal model\").plot.line(\n    ax=ax, legend=False, color=\"tab:blue\"\n)\nax.scatter(y_train.index, y_train, marker=\"o\", color=\"k\", s=2, alpha=0.5, label=\"Train\")\nax.scatter(\n    y_test.index, y_test, marker=\"o\", color=\"green\", s=2, alpha=0.5, label=\"Test\"\n)\nax.set_title(\"Prophet with normal likelihood\")\nax.legend()\nfig.show()\n\n\n\n\n\n\n\n\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.1, 0.9])\nfig, ax = plt.subplots(figsize=(10, 3))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(\n    forecast_horizon.to_timestamp(),\n    y.loc[forecast_horizon],\n    marker=\"o\",\n    color=\"k\",\n    s=2,\n    alpha=1,\n)\nax.axvline(y_train.index[-1].to_timestamp(), color=\"r\", linestyle=\"--\")\nfig.show()",
    "crumbs": [
      "Count data forecasting"
    ]
  },
  {
    "objectID": "tutorials/count_data.html#prophet-with-negative-binomial-likelihood",
    "href": "tutorials/count_data.html#prophet-with-negative-binomial-likelihood",
    "title": "Forecasting count data",
    "section": "Prophet with negative binomial likelihood",
    "text": "Prophet with negative binomial likelihood\nThe negative binomial likehood has support on the non-negative integers, which makes it perfect for count data. We change the likelihood of the model, and fit it again.\n\nmodel.set_params(likelihood=\"negbinomial\")\nmodel.fit(y=y_train)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[2,\n                                                                              2,\n                                                                              10],\n                                                          freq='H',\n                                                          prior_scale=0.1,\n                                                          sp_list=[24, 168,\n                                                                   8772.0]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(), likelihood='negbinomial',\n             trend=FlatTrend())Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[2,\n                                                                              2,\n                                                                              10],\n                                                          freq='H',\n                                                          prior_scale=0.1,\n                                                          sp_list=[24, 168,\n                                                                   8772.0]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(), likelihood='negbinomial',\n             trend=FlatTrend())effectsFlatTrendFlatTrend()LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[2, 2, 10], freq='H',\n                         prior_scale=0.1, sp_list=[24, 168, 8772.0])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nForecasting with the negative binomial model\n\nfig, ax = plt.subplots(figsize=(10, 3))\npreds_negbin = model.predict(fh=forecast_horizon)\npreds_negbin[\"pedestrian_count\"].rename(\"Neg. Binomial model\").plot.line(\n    ax=ax, legend=False, color=\"tab:purple\"\n)\nax.scatter(y_train.index, y_train, marker=\"o\", color=\"k\", s=2, alpha=0.5, label=\"Train\")\nax.scatter(\n    y_test.index, y_test, marker=\"o\", color=\"green\", s=2, alpha=0.5, label=\"Test\"\n)\nax.set_title(\"Prophet with Negative Binomial likelihood\")\nax.legend()\nfig.show()\n\n\n\n\n\n\n\n\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.1, 0.9])\nfig, ax = plt.subplots(figsize=(10, 3))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(\n    forecast_horizon.to_timestamp(),\n    y.loc[forecast_horizon],\n    marker=\"o\",\n    color=\"k\",\n    s=2,\n    alpha=1,\n)\nax.axvline(y_train.index[-1].to_timestamp(), color=\"r\", linestyle=\"--\")\nfig.show()",
    "crumbs": [
      "Count data forecasting"
    ]
  },
  {
    "objectID": "tutorials/count_data.html#comparing-both-forecasts-side-by-side",
    "href": "tutorials/count_data.html#comparing-both-forecasts-side-by-side",
    "title": "Forecasting count data",
    "section": "Comparing both forecasts side by side",
    "text": "Comparing both forecasts side by side\nTo make our point clear, we plot both forecasts side by side. Isn’t it nice to have forecasts that make sense? :smile:\n\nfig, ax = plt.subplots(figsize=(9, 5))\npreds_negbin[\"pedestrian_count\"].rename(\"Neg. Binomial model\").plot.line(\n    ax=ax, legend=False, color=\"tab:purple\"\n)\npreds_normal[\"pedestrian_count\"].rename(\"Normal model\").plot.line(\n    ax=ax, legend=False, color=\"tab:blue\"\n)\nax.scatter(y_train.index, y_train, marker=\"o\", color=\"k\", s=6, alpha=0.5, label=\"Train\")\nax.scatter(\n    y_test.index, y_test, marker=\"o\", color=\"green\", s=6, alpha=0.5, label=\"Test\"\n)\nax.set_title(\"Forecasting pedestrian counts\")\n# Remove xlabel\nax.set_xlabel(\"\")\nax.axvline(\n    y_train.index[-1].to_timestamp(),\n    color=\"black\",\n    linestyle=\"--\",\n    alpha=0.3,\n    zorder=-1,\n)\nfig.legend(loc=\"center\", ncol=4, bbox_to_anchor=(0.5, 0.8))\nfig.tight_layout()\nfig.show()",
    "crumbs": [
      "Count data forecasting"
    ]
  },
  {
    "objectID": "howto/custom_trend.html",
    "href": "howto/custom_trend.html",
    "title": "Custom Trend in Prophetverse",
    "section": "",
    "text": "Diffusion of innovations is a theory that seeks to explain how, why, and at what rate new ideas and technology spread through cultures. This theory was formulated by E.M. Rogers in 1962 and is often used to understand the adoption or spread of new products and technologies among different groups of people.\nAn innovation is something new or significantly improved. This can include products, ideas, or practices that are perceived as new by an individual or other unit of adoption. Diffusion refers to the process by which an innovation is communicated over time among the participants in a social system.\nThe diffusion of innovations theory applies to a variety of new ideas. Here are a few examples:",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#use-case-forecasting-product-adoption",
    "href": "howto/custom_trend.html#use-case-forecasting-product-adoption",
    "title": "Custom Trend in Prophetverse",
    "section": "",
    "text": "Diffusion of innovations is a theory that seeks to explain how, why, and at what rate new ideas and technology spread through cultures. This theory was formulated by E.M. Rogers in 1962 and is often used to understand the adoption or spread of new products and technologies among different groups of people.\nAn innovation is something new or significantly improved. This can include products, ideas, or practices that are perceived as new by an individual or other unit of adoption. Diffusion refers to the process by which an innovation is communicated over time among the participants in a social system.\nThe diffusion of innovations theory applies to a variety of new ideas. Here are a few examples:",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#examples-of-processes-following-diffusion-of-innovations",
    "href": "howto/custom_trend.html#examples-of-processes-following-diffusion-of-innovations",
    "title": "Custom Trend in Prophetverse",
    "section": "Examples of Processes Following Diffusion of Innovations",
    "text": "Examples of Processes Following Diffusion of Innovations\n\nNumber of new unique users: The number of new unique users of a product or service can be modeled using the diffusion of innovations theory. This can help businesses forecast their growth and plan for future expansion.\nTechnology Adoption: Perhaps the most common application of the theory, technology adoption refers to how new gadgets, software, or platforms spread among users. For instance, the adoption of smartphones followed this diffusion process, starting with innovators and tech enthusiasts before reaching the broader public.\nHealthcare Practices: New medical practices, treatments, or health campaigns spread among medical professionals and the public using the diffusion framework. An example could be the adoption of telemedicine, which has seen increased acceptance over recent years.\nSustainable Practices: The adoption of renewable energy sources like solar panels or wind turbines often follows the diffusion of innovations model. Innovators begin by testing and using these technologies, which gradually become more mainstream as their advantages and efficiencies are recognized.\nAgricultural Techniques: New farming technologies or methods, such as hydroponics or genetically modified crops, also spread through agricultural communities by following the principles of diffusion of innovations.",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#the-bell-shaped-curve",
    "href": "howto/custom_trend.html#the-bell-shaped-curve",
    "title": "Custom Trend in Prophetverse",
    "section": "The Bell-Shaped Curve",
    "text": "The Bell-Shaped Curve\nThe diffusion of innovations can be visualized using a bell-shaped curve, often called the “diffusion curve.” This curve is crucial for understanding the rate at which new ideas and technologies are adopted in a society. Here’s how it aligns with the categories of adopters:\n\nInnovators make up the first small section on the left of the curve. These are the first few who adopt the innovation.\nEarly Adopters follow next and represent a slightly larger segment as the curve starts to ascend.\nEarly Majority forms the first large segment of the curve, where it reaches and crosses the mean. Adoption is becoming more common and widespread here.\nLate Majority comes next, at the point where the curve starts to descend. This group adopts just as the new idea or technology begins to feel outdated.\nLaggards are the last segment, where the curve tails off. Adoption within this group occurs very slowly and often only when necessary.\n\nThe bell-shaped curve reflects the cumulative adoption of innovations over time, demonstrating that the speed of adoption typically starts slow, accelerates until it reaches the majority of the potential market, and then slows down as fewer non-adopters remain.\nThis curve is central to strategic decisions in marketing, product development, and policy-making, helping stakeholders identify when and how to best introduce new ideas or technologies to different segments of society.",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#the-dataset",
    "href": "howto/custom_trend.html#the-dataset",
    "title": "Custom Trend in Prophetverse",
    "section": "The dataset",
    "text": "The dataset\nAs a proxy for diffusion of innovations, we will use the number of stars received by Tensorflow Repository over time. Although this is not a perfect measure of adoption, it can give us an idea of how the popularity of the repository has grown since its inception.\nThis repository had an initial explosion of stars during the first ~10 days, which we will ignore since the daily granularity is not enough to capture the initial growth (hourly might work). After that, the number of starts grew by following a bell-shaped curve, which we will try to model. This curve might be related to the popularity of deep learning itself.\n\n\n\n\n\n\nNote\n\n\n\nThis dataset was obtained from https://github.com/emanuelef/daily-stars-explorer\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom prophetverse.datasets.loaders import load_tensorflow_github_stars\n\ny = load_tensorflow_github_stars()\n\nfig, ax = plt.subplots()\n# First 30 days\ny.iloc[:30].plot.line(ax=ax)\ny.iloc[:30].cumsum()[\"day-stars\"].rename(\"Cumulative sum\").plot.line(ax=ax, legend=True)\nax.set_title(\"First 30 days\")\nfig.show()\n\nfig, axs = plt.subplots(nrows=2, sharex=True)\ny.iloc[30:].plot.line(ax=axs[0])\ny.iloc[30:].cumsum()[\"day-stars\"].rename(\"Cumulative sum\").plot.line(ax=axs[1])\n# FIgure title\nfig.suptitle(\"After the first 30 days\")\nfig.show()\n\n/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#modeling-the-diffusion-of-innovations",
    "href": "howto/custom_trend.html#modeling-the-diffusion-of-innovations",
    "title": "Custom Trend in Prophetverse",
    "section": "Modeling the Diffusion of Innovations",
    "text": "Modeling the Diffusion of Innovations\nTo model this behaviour with Prophetverse, we will use the custom trend feature.\nWe will define a trend model class that implements the generalized logistic curve, which accepts assymetric curves. We will also add another premise: a varying capacity, which will allow us to model a linear growth of the total addressable market (TAM) over time. Let \\(G(t)\\) be the logistic curve defining the acumulated number of stars at time \\(t\\):\n\\[\n\\begin{align*}\nG(t) &= \\frac{C_1(t-t_0) + C_2}{\\left(1 + \\exp(-\\alpha v (t - t_0))\\right)^{\\frac{1}{v}}} \\\n\\text{where} & \\\\\nC_2 \\in \\mathbb{R}_+ &= \\text{is the constant capacity term} \\\\\nC_1 \\in \\mathbb{R}_+ &= \\text{is the linear increasing rate of the capacity} \\\\\nt_0 \\in \\mathbb{R} &= \\text{is the time offset term} \\\\\nv \\in \\mathbb{R}_+ &= \\text{determines the shape of the curve} \\\\\n\\alpha \\in \\mathbb{R} &= \\text{is the rate}\n\\end{align*}\n\\]\nIts derivative is:\n\\[\n\\begin{align*}\ng(t) &= \\alpha\\left(1 - \\frac{G(T)}{C_1(t-t_0) + C_2}\\right) G(T)  + \\frac{C_1}{C_1(t-t_0) + C_2}G(T)\n\\end{align*}\n\\]\nThat curve can be used as trend to model a diffusion process. Below, we plot it for a combination of parameters\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef g(t, C1, C2, t0, v, alpha):\n    return (C1 * (t - t0) + C2) / ((1 + np.exp(-alpha * v * (t - t0))) ** (1 / v))\n\n\ndef normalized_generalized_logistic(x, A, v, t0):\n    return 1 / (1 + np.exp(-A * v * (x - t0))) ** (1 / v)\n\n\n# Define the generalized logistic function\ndef generalized_logistic(x, C1, C2, alpha, v, t0):\n    return (C1 * x + C2) * normalized_generalized_logistic(x, alpha, v, t0)\n\n\ndef dgeneralized_logistic(x, C1, C2, alpha, v, t0):\n    return alpha * (\n        1 - (generalized_logistic(x, C1, C2, alpha, v, t0) / (C1 * x + C2)) ** v\n    ) * generalized_logistic(\n        x, C1, C2, alpha, v, t0\n    ) + C1 * normalized_generalized_logistic(\n        x, alpha, v, t0\n    )\n\n\nC1 = 1\nC2 = 50\nt0 = -2\nv = 1.2\nalpha = 0.5\nlabel = (f\"C1={C1:.1f}, C2={C2:.1f}, t0={t0:.1f}, v={v:.1f}, alpha={alpha:.1f}\",)\n\nt = np.linspace(-10, 10, 1000)\ngt = dgeneralized_logistic(t, C1=C1, C2=C2, t0=t0, v=v, alpha=alpha)\n\nfig, axs = plt.subplots(figsize=(12, 6), nrows=2, sharex=True)\naxs[0].plot(\n    t,\n    gt,\n    label=label,\n)\naxs[0].set_title(\"Visualization of g(t)\")\naxs[0].set_xlabel(\"t\")\naxs[0].set_ylabel(\"g(t)\")\n\nGt = generalized_logistic(t, C1=C1, C2=C2, t0=t0, v=v, alpha=alpha)\naxs[1].plot(\n    t,\n    Gt,\n    label=label,\n)\naxs[1].set_title(\"Visualization of G(t)\")\naxs[1].set_xlabel(\"t\")\naxs[1].set_ylabel(\"g(t)\")\n\n# axs[1].grid(True, alpha=0.3)\nfig.show()\n\n\n\n\n\n\n\n\nThat curve has the bell-shape and the flexiblity to not be symmetric depending on the parameters. Furthermore, it tends to a constant value (\\(C1\\)) as time goes to infinity, which represent our knowledge that the size of the “market” of tensorflow/neural networks users starts at a value and grows with time.",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#splitting-the-dataset",
    "href": "howto/custom_trend.html#splitting-the-dataset",
    "title": "Custom Trend in Prophetverse",
    "section": "Splitting the dataset",
    "text": "Splitting the dataset\nWe leave 7 years to forecast, and 1.5 year to train. Note that, without the prior information on the nature of the curve, a model could simply forecast a linear growth of the number of stars, which would be a very poor forecast.\n\nsplit_at = -int(365 * 5)\ny = y.iloc[20:]\ny_train, y_test = y.iloc[:split_at], y.iloc[split_at:]\n\n\nfig, axs = plt.subplots(nrows=2, sharex=True, figsize=(12, 6))\nax = axs[0]\nax = y_train[\"day-stars\"].rename(\"Train\").plot.line(legend=True, ax=ax)\ny_test[\"day-stars\"].rename(\"Test\").plot.line(ax=ax, alpha=0.2, legend=True)\nax.axvline(y_train.index[-1], color=\"red\", linestyle=\"--\", alpha=0.5, zorder=-1)\nax.set_title(\"Daily new stars\", loc=\"left\")\nax.set_xlabel(\"\")\nax.spines[\"left\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.spines[\"top\"].set_visible(False)\nax.set_xlim(y.index.min(), y.index.max())\nax.set_ylim(0, 300)\n\nax = axs[1]\nax = y_train[\"day-stars\"].rename(\"Train\").cumsum().plot.line(legend=True)\n(\n    y_test[\"day-stars\"].rename(\"Test\").cumsum()\n    + y_train[\"day-stars\"].rename(\"Train\").cumsum().max()\n).plot.line(ax=ax, alpha=0.2, legend=True)\nax.axvline(y_train.index[-1], color=\"red\", linestyle=\"--\", alpha=0.5, zorder=-1)\nax.set_title(\"Total stars\", loc=\"left\")\nax.set_xlabel(\"\")\nax.spines[\"left\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n# Superior\nax.spines[\"top\"].set_visible(False)\nax.set_xlim(y.index.min(), y.index.max())\n\nfig.suptitle(\"Tensorflow Stars\")\n\nText(0.5, 0.98, 'Tensorflow Stars')",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#creating-the-custom-trend",
    "href": "howto/custom_trend.html#creating-the-custom-trend",
    "title": "Custom Trend in Prophetverse",
    "section": "Creating the custom trend",
    "text": "Creating the custom trend\nTo create a custom trend model for use in the Prophetverse library, users can extend the TrendModel abstract base class and implement the required abstract methods. Here’s a step-by-step guide to create a custom trend model, using the GenLogisticTrend class as an example.\n\nStep 1: Define helper functions\nThe GenLogisticTrend class will use the following helper functions:\n\nimport jax\nimport jax.numpy as jnp\n\n\n@jax.jit\ndef normalized_generalized_logistic(x, A, v, t0):\n    return 1 / (1 + jnp.exp(-A * v * (x - t0))) ** (1 / v)\n\n\n@jax.jit\ndef dnormalized_generalized_logistic(x, A, v, t0):\n    return (\n        A\n        * (1 - normalized_generalized_logistic(x, A, v, t0) ** v)\n        * normalized_generalized_logistic(x, A, v, t0)\n    )\n\n\n# Define the generalized logistic function\n\n\n@jax.jit\ndef dgeneralized_logistic(x, K1, K2, A, v, M):\n    return dnormalized_generalized_logistic(x, A, v, M) * (\n        K1 * x + K2\n    ) + K1 * normalized_generalized_logistic(x, A, v, M)\n\n\n\nStep 2: Define the Custom Trend Model Class\nCreate a new class that extends the TrendModel abstract base class. Implement the abstract methods initialize, prepare_input_data, and compute_trend.\n\nimport numpyro\nfrom numpyro import distributions as dist\nimport pandas as pd\nfrom typing import Dict # Added for type hint Dict[str, jnp.ndarray]\n\nfrom prophetverse.distributions import GammaReparametrized\nfrom prophetverse.effects import BaseEffect\nfrom prophetverse.effects.trend import TrendEffectMixin\nfrom prophetverse.utils.frame_to_array import convert_index_to_days_since_epoch\n\n\nclass GenLogisticTrend(TrendEffectMixin, BaseEffect):\n    \"\"\"\n    Custom trend model based on the Generalized Logistic function.\n    \"\"\"\n\n    def __init__(\n        self,\n        logistic_capacity_dist=dist.HalfNormal(10),\n        logistic_capacity2_dist=dist.HalfNormal(50_000),\n        shape_dist=dist.Gamma(1, 1),\n        logistic_rate_dist=GammaReparametrized(0.01, 0.01),\n        offset_prior=dist.Normal(0, 365 * 2),\n    ):\n\n        self.logistic_capacity_dist = logistic_capacity_dist\n        self.logistic_capacity2_dist = logistic_capacity2_dist\n        self.shape_dist = shape_dist\n        self.logistic_rate_dist = logistic_rate_dist\n        self.offset_prior = offset_prior\n\n        super().__init__()\n\n    def _fit(self, y: pd.DataFrame, X: pd.DataFrame, scale: float = 1):\n        \"\"\"Initialize the effect.\n\n        Set the prior location for the trend.\n\n        Parameters\n        ----------\n        y : pd.DataFrame\n            The timeseries dataframe\n\n        X : pd.DataFrame\n            The DataFrame to initialize the effect.\n\n        scale : float, optional\n            The scale of the timeseries. For multivariate timeseries, this is\n            a dataframe. For univariate, it is a simple float.\n        \"\"\"\n        t = convert_index_to_days_since_epoch(y.index)\n        self.t_min_ = t.min()\n        t = t - self.t_min_\n\n    def _transform(self, X: pd.DataFrame, fh: pd.PeriodIndex) -&gt; dict:\n        \"\"\"\n        Prepare the input data for the piecewise trend model.\n\n        Parameters\n        ----------\n        X: pd.DataFrame\n            The exogenous variables DataFrame.\n        fh: pd.PeriodIndex\n            The forecasting horizon as a pandas Index.\n\n        Returns\n        -------\n        jnp.ndarray\n            An array containing the prepared input data.\n        \"\"\"\n        t = convert_index_to_days_since_epoch(fh)\n        t = t - self.t_min_\n        self.offset_prior_loc = len(fh)\n        return t\n\n    def _predict(self, data, previous_effects: Dict[str, jnp.ndarray], params):\n        \"\"\"\n        Compute the trend based on the Generalized Logistic function.\n\n        Parameters\n        ----------\n        data: jnp.ndarray\n            The changepoint matrix.\n        predicted_effects: Dict[str, jnp.ndarray]\n            Dictionary of previously computed effects. For the trend, it is an empty\n            dict.\n\n        Returns\n        -------\n        jnp.ndarray\n            The computed trend.\n        \"\"\"\n        # Alias for clarity\n        time = data\n\n        logistic_rate = numpyro.sample(\"logistic_rate\", self.logistic_rate_dist)\n\n        logistic_capacity1 = numpyro.sample(\n            \"logistic_capacity\",\n            self.logistic_capacity_dist,\n        )\n\n        logistic_capacity2 = numpyro.sample(\n            \"logistic_capacity2\",\n            self.logistic_capacity2_dist,\n        )\n\n        shape = numpyro.sample(\"logistic_shape\", self.shape_dist)\n\n        offset = numpyro.sample(\"offset\", self.offset_prior)\n\n        trend = dgeneralized_logistic(\n            time,\n            K1=logistic_capacity1,\n            K2=logistic_capacity2,\n            A=logistic_rate,\n            v=shape,\n            M=offset,\n        )\n\n        numpyro.deterministic(\"__trend\", trend)\n\n        numpyro.deterministic(\n            \"capacity\", logistic_capacity1 * (time - offset) + logistic_capacity2\n        )\n\n        return trend.reshape((-1, 1))",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#fit-the-model-and-make-predictions",
    "href": "howto/custom_trend.html#fit-the-model-and-make-predictions",
    "title": "Custom Trend in Prophetverse",
    "section": "Fit the model and make predictions",
    "text": "Fit the model and make predictions\n\nimport numpyro\nfrom sktime.transformations.series.fourier import FourierFeatures\n\nfrom prophetverse.effects import LinearFourierSeasonality\nfrom prophetverse.effects.linear import LinearEffect\nfrom prophetverse.engine import MCMCInferenceEngine\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import no_input_columns, starts_with\n\nnumpyro.enable_x64()\n\nmodel = Prophetverse(\n    likelihood=\"negbinomial\",\n    trend=GenLogisticTrend(),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                sp_list=[7, 365.25],\n                fourier_terms_list=[3, 8],\n                freq=\"D\",\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n    ],\n    inference_engine=MCMCInferenceEngine(\n        num_samples=500,\n        num_warmup=1000,\n    ),\n    # Avoid normalization of the timeseries by setting\n    # scale=1\n    scale=1,\n    noise_scale=10,\n)\n\nnumpyro.enable_x64()\n\nmodel.fit(y_train)\n\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:150: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:150: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:150: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:244: UserWarning: No columns match the regex ^$\n  self._fit_effects(X, y)\n\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              8],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_samples=500,\n                                                  num_warmup=1000),\n             likelihood='negbinomial', noise_scale=10, scale=1,\n             trend=GenLogisticTrend())Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              8],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_samples=500,\n                                                  num_warmup=1000),\n             likelihood='negbinomial', noise_scale=10, scale=1,\n             trend=GenLogisticTrend())effectsGenLogisticTrendGenLogisticTrend()LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 8], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMCMCInferenceEngineMCMCInferenceEngine(num_samples=500, num_warmup=1000)\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfh = pd.period_range(y.index.min(), \"2026-01-01\")\npreds = model.predict(fh=fh)\ninterval = model.predict_interval(\n    fh=fh,\n    coverage=0.9,\n)\ndisplay(preds.head())\n\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:150: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:150: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n\n\n\n\n\n\n\n\n\nday-stars\n\n\n\n\n2015-11-27\n53.069934\n\n\n2015-11-28\n38.653177\n\n\n2015-11-29\n38.342499\n\n\n2015-11-30\n52.187450\n\n\n2015-12-01\n54.210998\n\n\n\n\n\n\n\n\ninterval.columns = interval.columns.droplevel([0, 1])\ninterval.head()\n\n\n\n\n\n\n\n\nlower\nupper\n\n\n\n\n2015-11-27\n31.00\n84.0\n\n\n2015-11-28\n20.95\n61.0\n\n\n2015-11-29\n21.00\n57.0\n\n\n2015-11-30\n31.95\n81.0\n\n\n2015-12-01\n30.00\n82.0",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#plotting-the-results",
    "href": "howto/custom_trend.html#plotting-the-results",
    "title": "Custom Trend in Prophetverse",
    "section": "Plotting the results",
    "text": "Plotting the results\n\n# Just the scatter of y, without lines\nfig, ax = plt.subplots(figsize=(12, 5))\nax = (\n    y[\"day-stars\"]\n    .rename(\"Observed\")\n    .plot.line(\n        marker=\"o\", linestyle=\"None\", legend=False, markersize=1, color=\"black\", ax=ax\n    )\n)\nax.axvline(y_train.index.max(), color=\"black\", zorder=-1, alpha=0.4, linewidth=1)\nax.spines[\"left\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n# Superior\nax.spines[\"top\"].set_visible(False)\n\npreds[\"day-stars\"].rename(\"Forecast\").plot.line(\n    ax=ax, alpha=1, linewidth=0.5, legend=False\n)\n\nax.fill_between(\n    fh.to_timestamp(),\n    interval[\"lower\"],\n    interval[\"upper\"],\n    color=\"blue\",\n    alpha=0.2,\n    zorder=-1,\n    label=\"90% Credible Interval\",\n)\nfig.legend()\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n\n\n\n\n# Forecast samples\nyhat_samples = model.predict_samples(fh=fh)\n# Samples of all sites (capacity, for example, that we had set as deterministic with numpyro.deterministic)\nsite_samples = model.predict_component_samples(fh=fh)\n\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:150: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:150: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n\n\n\nimport pandas as pd\n# Set number of columns to display to 4 temporarily\npd.set_option(\"display.max_columns\", 4)\nyhat_samples.head()\n\n\n\n\n\n\n\n\n0\n1\n...\n498\n499\n\n\n\n\n2015-11-27\n69\n69\n...\n57\n56\n\n\n2015-11-28\n25\n55\n...\n45\n50\n\n\n2015-11-29\n22\n33\n...\n34\n42\n\n\n2015-11-30\n44\n25\n...\n46\n47\n\n\n2015-12-01\n28\n60\n...\n55\n46\n\n\n\n\n5 rows × 500 columns\n\n\n\n\n# Get quantile 0.25, 0.75 and mean\nimport functools\nimport numpy as np\n\n\ndef q25(x):\n    return np.quantile(x, q=0.25)\n\n\ndef q75(x):\n    return np.quantile(x, q=0.75)\n\n\nsite_quantiles = site_samples.groupby(level=[-1]).agg(\n    [\n        np.mean,\n        q25,\n        q75,\n    ]\n)\nsite_quantiles.head()\n\n/tmp/ipykernel_3373/2904010725.py:14: FutureWarning: The provided callable &lt;function mean at 0x7f23541922a0&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  site_quantiles = site_samples.groupby(level=[-1]).agg(\n\n\n\n\n\n\n\n\n\nmean\n...\ntrend/capacity\n\n\n\nmean\nq25\n...\nq25\nq75\n\n\n\n\n2015-11-27\n53.069934\n51.485973\n...\n45313.495166\n80236.635899\n\n\n2015-11-28\n38.653177\n37.322672\n...\n45354.507746\n80263.527407\n\n\n2015-11-29\n38.342499\n37.046701\n...\n45395.520325\n80290.418916\n\n\n2015-11-30\n52.187450\n50.596605\n...\n45436.532905\n80317.310424\n\n\n2015-12-01\n54.210998\n52.695983\n...\n45477.545484\n80344.201932\n\n\n\n\n5 rows × 18 columns\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(12, 5))\n\n# Plot true value\nax.plot(y.index.to_timestamp(), y.cumsum(), label=\"Observed\")\n\n# Train test split\nax.axvline(y_train.index.max(), color=\"black\", alpha=0.8, zorder=-1, linewidth=1)\n\n\n# Capacity asymptotic\nax.fill_between(\n    fh.to_timestamp(),\n    site_quantiles.loc[:, (\"trend/capacity\", \"q25\")],\n    site_quantiles.loc[:, (\"trend/capacity\", \"q75\")],\n    color=\"red\",\n    alpha=0.1,\n    zorder=-1,\n    label=\"Asymptotic capacity\",\n)\nax.plot(\n    fh.to_timestamp(),\n    site_quantiles.loc[:, (\"trend/capacity\", \"mean\")],\n    color=\"red\",\n    alpha=0.2,\n    linestyle=\"--\",\n    zorder=-1,\n    linewidth=0.9,\n)\n\n\n# Plot some random samples\nidxs = np.random.choice(yhat_samples.columns, 10)\n\nfor i, idx in enumerate(idxs):\n    kwargs = {}\n    if i == 0:\n        kwargs[\"label\"] = \"MCMC Samples\"\n    ax.plot(\n        fh.to_timestamp(),\n        yhat_samples.cumsum().loc[:, idx],\n        color=\"black\",\n        alpha=0.1,\n        linewidth=1,\n        **kwargs,\n    )\n\nalpha = 0.1\nupper_and_lower_cumsum = (\n    yhat_samples.cumsum().quantile([alpha / 2, 1 - alpha / 2], axis=1).T\n)\n\n\nax.fill_between(\n    upper_and_lower_cumsum.index.to_timestamp(),\n    upper_and_lower_cumsum.iloc[:, 0],\n    upper_and_lower_cumsum.iloc[:, 1],\n    alpha=0.5,\n)\nax.grid(alpha=0.2)\nax.spines[\"left\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n# Superior\nax.spines[\"top\"].set_visible(False)\n\nax.set_xlim(fh.to_timestamp().min(), fh.to_timestamp().max())\n\n# Add samples to legend\n\nfig.legend()\nax.set_title(\"Total number of stars (forecast)\")\nfig.show()",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_effects.html",
    "href": "howto/custom_effects.html",
    "title": "Customizing Exogenous Effects in Prophetverse",
    "section": "",
    "text": "The exogenous effect API allows you to create custom exogenous components for the Prophetverse model. This is useful when we want to model specific patterns or relationships between the exogenous variables and the target variable. For example, enforcing a positive effect of a variable on the mean, or modeling a non-linear relationship.\nIf you have read the theory section, by effect we mean each function \\(f_i\\). You can implement those custom functions by subclassing the BaseEffect class, and then use them in the Prophetverse model. Some effects are already implemented in the library, and you can find them in the prophetverse.effects module.\nWhen creating a model instance, effects can be specified through exogenous_effects parameter of the Prophetverse model. This parameter is a list of tuples of three values: the name, the effect object, and a regex to filter columns related to that effect. The regex is what defines \\(x_i\\) in the previous section. The prophetverse.utils.regex module provides some useful functions to create regex patterns for common use cases, include starts_with, ends_with, contains, and no_input_columns.\nConsider the example below, where we create a model with a linear seasonality effect and a custom effect that uses the feature channel1_investment as input and transforms it with a hill curve, which is a common curve for capturing diminishing returns.\n\nfrom prophetverse.effects import HillEffect, LinearFourierSeasonality\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import exact, no_input_columns, starts_with\n\nexogenous_effects = [\n    (\n        \"seasonality\",\n        LinearFourierSeasonality(\n            freq=\"D\",\n            sp_list=[7, 365.25],\n            fourier_terms_list=[3, 10],\n            prior_scale=0.1,\n            effect_mode=\"multiplicative\",\n        ),\n        no_input_columns,\n    ),\n    (\n        \"channel1_investment_incremental\",\n        HillEffect(effect_mode=\"additive\"),\n        exact(\"channel1_investment\"),\n    ),\n]\n\nmodel = Prophetverse(exogenous_effects=exogenous_effects)\n\n/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nCreating such models in Prophetverse is like creating buildings from lego blocks. You define how you model should work, and then you can leverage all the interface to carry out the forecasting and inference tasks.",
    "crumbs": [
      "Custom exogenous effect"
    ]
  },
  {
    "objectID": "howto/custom_effects.html#the-effects-api",
    "href": "howto/custom_effects.html#the-effects-api",
    "title": "Customizing Exogenous Effects in Prophetverse",
    "section": "",
    "text": "The exogenous effect API allows you to create custom exogenous components for the Prophetverse model. This is useful when we want to model specific patterns or relationships between the exogenous variables and the target variable. For example, enforcing a positive effect of a variable on the mean, or modeling a non-linear relationship.\nIf you have read the theory section, by effect we mean each function \\(f_i\\). You can implement those custom functions by subclassing the BaseEffect class, and then use them in the Prophetverse model. Some effects are already implemented in the library, and you can find them in the prophetverse.effects module.\nWhen creating a model instance, effects can be specified through exogenous_effects parameter of the Prophetverse model. This parameter is a list of tuples of three values: the name, the effect object, and a regex to filter columns related to that effect. The regex is what defines \\(x_i\\) in the previous section. The prophetverse.utils.regex module provides some useful functions to create regex patterns for common use cases, include starts_with, ends_with, contains, and no_input_columns.\nConsider the example below, where we create a model with a linear seasonality effect and a custom effect that uses the feature channel1_investment as input and transforms it with a hill curve, which is a common curve for capturing diminishing returns.\n\nfrom prophetverse.effects import HillEffect, LinearFourierSeasonality\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import exact, no_input_columns, starts_with\n\nexogenous_effects = [\n    (\n        \"seasonality\",\n        LinearFourierSeasonality(\n            freq=\"D\",\n            sp_list=[7, 365.25],\n            fourier_terms_list=[3, 10],\n            prior_scale=0.1,\n            effect_mode=\"multiplicative\",\n        ),\n        no_input_columns,\n    ),\n    (\n        \"channel1_investment_incremental\",\n        HillEffect(effect_mode=\"additive\"),\n        exact(\"channel1_investment\"),\n    ),\n]\n\nmodel = Prophetverse(exogenous_effects=exogenous_effects)\n\n/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nCreating such models in Prophetverse is like creating buildings from lego blocks. You define how you model should work, and then you can leverage all the interface to carry out the forecasting and inference tasks.",
    "crumbs": [
      "Custom exogenous effect"
    ]
  },
  {
    "objectID": "howto/custom_effects.html#creating-a-custom-effect",
    "href": "howto/custom_effects.html#creating-a-custom-effect",
    "title": "Customizing Exogenous Effects in Prophetverse",
    "section": "Creating a Custom Effect",
    "text": "Creating a Custom Effect\nThe effects can be any object that implements the BaseEffect interface, and you can create your own effects by subclassing BaseEffect and implementing _fit, _transform and _predict methods.\n\n_fit (optional): This method is called during fit() of the forecasting and should be used to initialize any necessary parameters or data structures. It receives the exogenous variables dataframe X, the series y, and the scale factor scale that was used to scale the timeseries.\n_transform (optional): This method receives the exogenous variables dataframe, and should return an object containing the data needed for the effect. This object will be passed to the predict method as data. By default the columns of the dataframe that match the regex pattern are selected, and the result is converted to a jnp.ndarray.\n_predict (mandatory): This method receives the output of _transform and all previously computed effects. It should return the effect values as a jnp.ndarray\n\nIn many cases, the _fit and _transform steps are not needed to be implemented, since the default behaviour may be the desired one. In the example below, we implement a really simple SquaredEffect class, which leverages the default behaviour of the BaseEffect class.\n\nSquared Effect Class\nThe SquaredEffect class receives two hyperparameters: the prior distribution for the scale parameter, and the prior distribution for the offset parameter. If no prior is provided, it uses a Gamma(1, 1) for the scale and a Normal(0, 1) for the offset. Note that here we already see an interesting feature of Prophetverse: by adopting a Gamma Prior, we force the effect to be positive. Any other prior with positive support would work as well. If no such constraint is needed, we can use a Normal(0, 1) prior or any other distribution with support in the real line.\n\nfrom typing import Dict, Optional\n\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro.distributions import Distribution\n\nfrom prophetverse.effects.base import BaseEffect\n\n\nclass SquaredEffect(BaseEffect):\n    \"\"\"Represents a squared effect as effect = scale * (data - offset)^2.\"\"\"\n\n    def __init__(\n        self,\n        scale_prior: Optional[Distribution] = None,\n        offset_prior: Optional[Distribution] = None,\n    ):\n        self.scale_prior = scale_prior or dist.Gamma(1, 1)\n        self.offset_prior = offset_prior or dist.Normal(0, 1)\n        super().__init__()\n\n    def _predict(\n        self,\n        data: jnp.ndarray,\n        predicted_effects: Optional[Dict[str, jnp.ndarray]],\n        params: Dict,\n    ) -&gt; jnp.ndarray:\n        scale = numpyro.sample(\"log_scale\", self.scale_prior)\n        offset = numpyro.sample(\"offset\", self.offset_prior)\n        effect = scale * (data - offset) ** 2\n        return effect\n\nThe _fit and _transform methods are not implemented, and the default behaviour is preserved (the columns of the dataframe that match the regex pattern are selected, and the result is converted to a jnp.ndarray with key “data”).",
    "crumbs": [
      "Custom exogenous effect"
    ]
  },
  {
    "objectID": "howto/custom_effects.html#practical-example",
    "href": "howto/custom_effects.html#practical-example",
    "title": "Customizing Exogenous Effects in Prophetverse",
    "section": "Practical Example",
    "text": "Practical Example\nThe example below is, of course, a toy example, but I hope it illustrates the process of creating a custom effect. We load a synthetic dataset with a squared relationship between the exogenous variable and the target variable, and then we fit a model with the SquaredEffect. The true relationship is 2 * (x - 5) ** 2, and we will see if the model is able to recover it.\n\nLoading the Series\n\nimport matplotlib.pyplot as plt\nfrom sktime.split import temporal_train_test_split\nfrom sktime.utils.plotting import plot_series\n\nfrom prophetverse.datasets import load_synthetic_squared_exogenous\n\ny, X = load_synthetic_squared_exogenous()\ny_train, y_test, X_train, X_test = temporal_train_test_split(\n    y,\n    X,\n    test_size=0.2,\n)\n\ndisplay(y.head())\ndisplay(X.head())\n\nfig, ax = plot_series(\n    y_train, y_test, labels=[\"Train\", \"Test\"], title=\"Target variable\"\n)\nfig.show()\nfig, ax = plot_series(\n    X_train, X_test, labels=[\"Train\", \"Test\"], title=\"Exogenous variable\"\n)\nfig.show()\n\n\n\n\n\n\n\n\ntarget\n\n\ntime\n\n\n\n\n\n2010-01-01\n14.956419\n\n\n2010-01-02\n1.694310\n\n\n2010-01-03\n28.520329\n\n\n2010-01-04\n15.180486\n\n\n2010-01-05\n20.784949\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexogenous\n\n\ntime\n\n\n\n\n\n2010-01-01\n7.965604\n\n\n2010-01-02\n4.949906\n\n\n2010-01-03\n8.727381\n\n\n2010-01-04\n7.276312\n\n\n2010-01-05\n1.847596\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating the Model\n\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import exact\n\nmodel = (\n    Prophetverse()\n    &gt;&gt; PiecewiseLinearTrend(\n        changepoint_interval=100,\n        changepoint_prior_scale=0.1,\n        changepoint_range=-100,\n    )\n    &gt;&gt; MAPInferenceEngine()\n) &gt;&gt; (\n    \"exog_effect\",\n    SquaredEffect(\n        scale_prior=dist.Normal(0, 10),\n        offset_prior=dist.Normal(0, 10),\n    ),\n    exact(\"exogenous\"),\n)\nmodel\n\nProphetverse(exogenous_effects=[('exog_effect',\n                                 SquaredEffect(offset_prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f7ad409a850 with batch shape () and event shape ()&gt;,\n                                               scale_prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f7ad41ac610 with batch shape () and event shape ()&gt;),\n                                 '^exogenous$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=100,\n                                        changepoint_prior_scale=0.1,\n                                        changepoint_range=-100))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('exog_effect',\n                                 SquaredEffect(offset_prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f7ad409a850 with batch shape () and event shape ()&gt;,\n                                               scale_prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f7ad41ac610 with batch shape () and event shape ()&gt;),\n                                 '^exogenous$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=100,\n                                        changepoint_prior_scale=0.1,\n                                        changepoint_range=-100))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=100, changepoint_prior_scale=0.1,\n                     changepoint_range=-100)SquaredEffectSquaredEffect(offset_prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f7ad409a850 with batch shape () and event shape ()&gt;,\n              scale_prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f7ad41ac610 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\nTo fit and plot, we use always the same interface, from sktime library.\n\nmodel.fit(y=y_train, X=X_train)\ny_pred = model.predict(fh=y_test.index, X=X)\ny_pred.head()\n\n\n\n\n\n\n\n\ntarget\n\n\n\n\n2011-07-15\n31.400940\n\n\n2011-07-16\n16.796745\n\n\n2011-07-17\n36.026505\n\n\n2011-07-18\n16.221329\n\n\n2011-07-19\n16.970356\n\n\n\n\n\n\n\n\nplot_series(y, y_pred, labels=[\"True\", \"Predicted\"], title=\"True vs Predicted\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRecovering the Predicted Effect and Components\nThis library adds extra methods to the sktime interface, such as predict_components, which behaves similarly to predict, but returns the components of the forecast as components of the output.\nThe name of the effect in the output dataframe is equal to the one we have passed as first item in the tuple when creating the model. In this case, the name is “exog_effect”.\n\ncomponents = model.predict_components(fh=y.index, X=X)\ncomponents.head()\n\n\n\n\n\n\n\n\nexog_effect\nmean\nobs\ntrend\n\n\n\n\n2010-01-01\n17.593287\n17.405809\n17.539135\n-0.187464\n\n\n2010-01-02\n0.006434\n-0.164753\n-0.247736\n-0.171187\n\n\n2010-01-03\n27.817465\n27.662580\n27.589167\n-0.154911\n\n\n2010-01-04\n10.351579\n10.212944\n10.137814\n-0.138634\n\n\n2010-01-05\n20.049173\n19.926819\n19.955585\n-0.122358\n\n\n\n\n\n\n\nNow, let’s compare it with the true effect. We will plot the true effect and the predicted effect in the same plot.\n\nfig, ax = plt.subplots()\nax.scatter(\n    X[\"exogenous\"], 2 * (X[\"exogenous\"] - 5) ** 2, color=\"black\", label=\"True effect\"\n)\nax.scatter(\n    X[\"exogenous\"],\n    components[\"exog_effect\"],\n    marker=\"x\",\n    color=\"red\",\n    s=10,\n    label=\"Predicted effect\",\n)\nax.set(\n    xlabel=\"Exogenous variable\",\n    ylabel=\"Effect\",\n    title=\"True effect vs Predicted effect\",\n)\nax.legend()\nfig.show()",
    "crumbs": [
      "Custom exogenous effect"
    ]
  },
  {
    "objectID": "prompts/custom_effects.html",
    "href": "prompts/custom_effects.html",
    "title": "Prompt for Custom Effects",
    "section": "",
    "text": "Paste the prompt below just before your request to the LLM. This will help the model understand what you want to achieve.\n\"\"\"\nExtension template for creating custom effects in Prophetverse.\n\nThis template provides comprehensive examples and guidance for creating your own effects.\nEffects are the building blocks of Prophetverse models, allowing you to incorporate\nvarious components like trend, seasonality, and exogenous regressors.\n\nKey Methods to Understand:\n--------------------------\n1. `_fit(self, y, X, scale)`: (Optional) Called once during the forecaster's `fit`.\n   Use this to perform pre-computations or fit objects needed later.\n\n2. `_transform(self, X, fh)`: (Optional) Called during both `fit` and `predict`.\n   Converts pandas DataFrame to a format suitable for `_predict` (typically JAX arrays).\n   The default implementation converts selected columns to JAX arrays. The\n   shape for array data is (n_series, n_timepoints, n_features).\n\n3. `_predict(self, data, predicted_effects, **kwargs)`: (Mandatory) Core effect logic.\n   Receives data from `_transform` and returns the computed effect as a JAX array.\n   Use `numpyro.sample` here to define prior distributions for parameters. The\n   shape for array data is (n_series, n_timepoints, n_features) if capability\n   if false for both `capability:panel` and `capability:multivariate_input`.,\n   (n_timepoints, n_features) if  'capability:multivariate_input' is true, and\n   (n_series, n_timepoints, 1) if `capability:panel` is true. The output should\n   be a jax array of shapre (n_series, n_timepoints, 1) if `capability:panel` is true,\n   and (n_timepoints, 1) otherwise.\n\n\nEffect Tags (Control Behavior):\n-------------------------------\n- `capability:panel`: Can handle multiple time series at once.\n- `capability:multivariate_input`: Can process multiple columns simultaneously.\n- `requires_X`: Skip if no matching columns found in X.\n- `applies_to`: Whether effect uses 'X' (exogenous) or 'y' (target) data.\n- `filter_indexes_with_forecating_horizon_at_transform`: Pre-filter data to forecast horizon.\n- `requires_fit_before_transform`: Require fit() before transform().\n- `feature:panel_hyperpriors`: Uses hyperpriors for hierarchical modeling.\n\nHow Tags Modify Behavior:\n-------------------------\nTags are metadata that control how Prophetverse handles your effect. Here's a\ndeeper dive into how they influence the `_fit`, `_transform`, and `_predict` methods:\n\n- `capability:panel` (bool):\n  - If `True`, your effect is expected to handle panel data (multiple time series) directly. `_fit` and `_transform` receive the full DataFrame with a MultiIndex.\n  - If `False` (default), and panel data is provided, Prophetverse automatically broadcasts the effect, applying it to each time series individually.\n\n- `capability:multivariate_input` (bool):\n  - If `True`, your effect is expected to handle a DataFrame with multiple columns as input. `_fit` and `_transform` receive all matching columns at once.\n  - If `False` (default), and a multi-column DataFrame is provided, Prophetverse broadcasts the effect, applying it to each column individually.\n\n- `requires_X` (bool):\n  - If `True` (default), the effect depends on exogenous variables (`X`).\n  - If no columns in `X` match the effect's `regex`, the entire effect is\n    skipped (`_transform` and `_predict` are not called).\n  - If `False`, the effect runs even if `X` is empty or `None`.\n\n- `applies_to` (str: 'X' or 'y'):\n  - Determines the input data for your effect.\n  - If `'X'` (default), the `data` passed to `_fit` and `_transform` is the\n    exogenous variables DataFrame.\n  - If `'y'`, the `data` is the target variable DataFrame.\n\n- `filter_indexes_with_forecating_horizon_at_transform` (bool):\n  - If `True` (default), during `predict`, the DataFrame passed to `_transform`\n    is automatically filtered to only contain dates in the forecasting horizon (`fh`).\n  - This is a convenience to avoid manual filtering inside `_transform`.\n  - Set to `False` if your effect needs to see data outside the forecast horizon\n    during prediction (e.g., for calculating lags).\n\n- `requires_fit_before_transform` (bool):\n  - If `True`, Prophetverse ensures that `_fit` has been called before `_transform`.\n  - This is crucial if `_transform` relies on state computed in `_fit` (e.g.,\n    means, scaling factors).\n  - If `False` (default), `_transform` can be called without `_fit`, which is\n    typical for stateless effects.\n\n- `feature:panel_hyperpriors` (bool):\n  - If `True`, signals that your effect defines hyperpriors for hierarchical\n    models, allowing parameters to be shared and learned across different\n    time series in a panel dataset. This is an advanced feature for\n    implementing hierarchical Bayesian models.\n\"\"\"\n\nfrom typing import Any, Dict, Optional\n\nimport pandas as pd\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\n\nfrom prophetverse.effects.base import BaseEffect\nfrom prophetverse.utils.frame_to_array import series_to_tensor_or_array\n\n\nclass MySimpleEffect(BaseEffect):\n    \"\"\"\n    A simple custom effect example that only overrides `_predict`.\n\n    This template is suitable when no fitting or parameter sampling is required,\n    and the effect is a direct transformation of the input data.\n\n    Parameters\n    ----------\n    scale_factor : float\n        A scaling factor applied to the input data.\n    bias : float\n        A constant bias added to the scaled input.\n    \"\"\"\n\n    _tags = {\n        \"capability:panel\": False,\n        \"capability:multivariate_input\": False,\n        \"requires_X\": True,\n        \"applies_to\": \"X\",\n        \"filter_indexes_with_forecating_horizon_at_transform\": True,\n        \"requires_fit_before_transform\": False,\n    }\n\n    def __init__(self, scale_factor: float = 1.0, bias: float = 0.0):\n        # Init hyperparameters before BaseEffect init\n        # Do not change them!\n        self.scale_factor = scale_factor\n        self.bias = bias\n        super().__init__()\n\n        # Now, do parameter handling\n\n    def _predict(\n        self,\n        data: jnp.ndarray,\n        predicted_effects: Dict[str, jnp.ndarray],\n        *args,\n        **kwargs,\n    ) -&gt; jnp.ndarray:\n        \"\"\"\n        Compute the custom effect by scaling `data` and adding a bias.\n\n        Parameters\n        ----------\n        data : jnp.ndarray\n            Transformed exogenous data from the base `_transform` method.\n        predicted_effects : dict\n            A dictionary of already computed effects in the model (unused here).\n\n        Returns\n        -------\n        jnp.ndarray\n            The computed effect, a JAX array.\n        \"\"\"\n        variable = numpyro.sample(\"variable\", dist.Normal(0.0, 1.0))\n        return data * self.scale_factor + self.bias + variable\n\n\nclass MyCustomEffect(BaseEffect):\n    \"\"\"\n    A full-featured custom effect example.\n\n    This demonstrates how to implement `_fit`, `_transform`, and `_predict`,\n    and how to use tags to control the effect's behavior.\n\n    Steps to implement a new effect:\n      1. Override `_fit` (optional) to compute static quantities from `y` and `X`.\n      2. Override `_transform` (optional) to prepare `X` as JAX arrays.\n      3. Within `_predict`, sample any parameters via `numpyro.sample`.\n      4. Implement `_predict` (required) using `data`, `predicted_effects`, and samples.\n\n    Parameters\n    ----------\n    multiplier : float\n        A multiplier applied in `_predict`.\n    prior_scale : float\n        Scale of the Normal prior for sampling a coefficient.\n    \"\"\"\n\n    _tags = {\n        \"capability:panel\": False,\n        \"capability:multivariate_input\": False,\n        \"requires_X\": True,\n        \"applies_to\": \"X\",\n        \"filter_indexes_with_forecating_horizon_at_transform\": True,\n        \"requires_fit_before_transform\": True,  # We need fit to learn the mean\n    }\n\n    def __init__(self, multiplier: float = 1.0, prior=None):\n        # Init hyperparameters before BaseEffect init\n        # Do not change them!\n        self.multiplier = multiplier\n        self.prior = prior\n        super().__init__()\n        # It's good practice to define priors in __init__\n        self._prior = prior if prior is not None else dist.Normal(0.0, 1.0)\n\n    def _fit(self, y: pd.DataFrame, X: Optional[pd.DataFrame], scale: float = 1.0):\n        \"\"\"\n        (Optional) Fit phase: called once during forecaster.fit().\n\n        This example learns the column means from the training data `X` for centering.\n        \"\"\"\n        if X is not None:\n            # Compute and store column means of X for centering in _transform\n            self._X_mean = X.mean()\n        else:\n            self._X_mean = 0.0\n        super()._fit(y, X, scale)\n\n    def _transform(self, X: pd.DataFrame, fh: pd.Index) -&gt; Any:\n        \"\"\"\n        (Optional) Transform phase: prepares data for `_predict`.\n\n        This example implementation subtracts the stored mean and then converts\n        the data to a JAX array.\n\n        The `_transform` method can return one of the following structures:\n        - A single `jnp.ndarray`: The simplest and most common case.\n        - A `tuple`: Useful for passing multiple arrays or mixed data types.\n          The first element is typically the main data array.\n        - A `dict`: Flexible for passing named arrays and other metadata.\n          Must contain a 'data' key holding the main `jnp.ndarray`.\n        \"\"\"\n        # Center the data using the mean learned in _fit\n        X_proc = X - self._X_mean\n        # Convert to JAX tensor/array\n        return series_to_tensor_or_array(X_proc)\n\n    def _predict(\n        self,\n        data: Any,\n        predicted_effects: Dict[str, jnp.ndarray],\n        *args,\n        **kwargs,\n    ) -&gt; jnp.ndarray:\n        \"\"\"\n        (Mandatory) Prediction phase: core effect computation.\n\n        This is where the main effect logic happens. Use numpyro.sample()\n        to define Bayesian priors for parameters.\n\n        The `data` argument receives the output of `_transform`. Your implementation\n        should handle the structure you defined:\n        - If `_transform` returns a `jnp.ndarray`, `data` will be that array.\n        - If `_transform` returns a `tuple`, `data` will be that tuple.\n        - If `_transform` returns a `dict`, `data` will be that dictionary.\n        \"\"\"\n        # Sample a coefficient from the prior defined in __init__\n        coef = numpyro.sample(\"my_custom_coef\", self._prior)\n\n        # The effect's computation.\n        # This example creates a linear effect with the centered data.\n        effect = data * coef * self.multiplier\n\n        return effect\n\n    @classmethod\n    def get_test_params(cls, parameter_set: str = \"default\"):\n        \"\"\"\n        (Optional) Provide test parameters for Prophetverse's testing framework.\n        \"\"\"\n        return [{\"multiplier\": 2.0, \"prior_scale\": 1.0}]\n\n\n# --- Advanced: Additive vs Multiplicative Effects ---\n#\n# For effects that can switch between additive and multiplicative modes,\n# inherit from BaseAdditiveOrMultiplicativeEffect instead of BaseEffect.\n#\n# Example:\n# from prophetverse.effects.base import BaseAdditiveOrMultiplicativeEffect\n#\n# class AdstockEffect(BaseAdditiveOrMultiplicativeEffect):\n#     def __init__(self, effect_mode=\"multiplicative\", **kwargs):\n#         super().__init__(effect_mode=effect_mode, **kwargs)\n#\n#     def _predict(self, data, predicted_effects, **kwargs):\n#         # Your core logic here\n#         adstock_rate = numpyro.sample(\"adstock\", dist.Beta(1, 1))\n#         # The base class handles additive vs multiplicative application\n#         return apply_adstock(data, adstock_rate)\n\n# --- Tips for Creating Custom Effects ---\n#\n# 1. Start with MySimpleEffect template for basic transformations.\n# 2. Use MyCustomEffect template when you need parameter fitting.\n# 3. Check existing effects in prophetverse.effects for inspiration:\n#    - LinearEffect: Simple linear regression\n#    - HillEffect: Hill saturation transformation\n#    - GeometricAdstockEffect: Media adstock modeling\n# 4. Use descriptive parameter names in numpyro.sample().\n# 5. Test your effect with get_test_params() method.\n# 6. Consider panel data capabilities if working with multiple time series."
  },
  {
    "objectID": "reference/MaximizeROI.html",
    "href": "reference/MaximizeROI.html",
    "title": "MaximizeROI",
    "section": "",
    "text": "MaximizeROI\nbudget_optimization.objectives.MaximizeROI(self)\nMaximize return on investment (ROI) objective function.",
    "crumbs": [
      "Objective Functions",
      "MaximizeROI"
    ]
  },
  {
    "objectID": "reference/HillEffect.html",
    "href": "reference/HillEffect.html",
    "title": "HillEffect",
    "section": "",
    "text": "effects.HillEffect(\n    self,\n    effect_mode='multiplicative',\n    half_max_prior=None,\n    slope_prior=None,\n    max_effect_prior=None,\n    offset_slope=0.0,\n    input_scale=1.0,\n    base_effect_name='trend',\n)\nRepresents a Hill effect in a time series model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nhalf_max_prior\nDistribution\nPrior distribution for the half-maximum parameter\nNone\n\n\nslope_prior\nDistribution\nPrior distribution for the slope parameter\nNone\n\n\nmax_effect_prior\nDistribution\nPrior distribution for the maximum effect parameter\nNone\n\n\neffect_mode\neffects_application\nMode of the effect (either “additive” or “multiplicative”)\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "HillEffect"
    ]
  },
  {
    "objectID": "reference/HillEffect.html#parameters",
    "href": "reference/HillEffect.html#parameters",
    "title": "HillEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nhalf_max_prior\nDistribution\nPrior distribution for the half-maximum parameter\nNone\n\n\nslope_prior\nDistribution\nPrior distribution for the slope parameter\nNone\n\n\nmax_effect_prior\nDistribution\nPrior distribution for the maximum effect parameter\nNone\n\n\neffect_mode\neffects_application\nMode of the effect (either “additive” or “multiplicative”)\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "HillEffect"
    ]
  },
  {
    "objectID": "reference/GeometricAdstockEffect.html",
    "href": "reference/GeometricAdstockEffect.html",
    "title": "GeometricAdstockEffect",
    "section": "",
    "text": "effects.GeometricAdstockEffect(\n    self,\n    decay_prior=None,\n    raise_error_if_fh_changes=False,\n)\nRepresents a Geometric Adstock effect in a time series model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndecay_prior\nDistribution\nPrior distribution for the decay parameter (controls the rate of decay).\nNone\n\n\nrase_error_if_fh_changes\nbool\nWhether to raise an error if the forecasting horizon changes during predict\nrequired",
    "crumbs": [
      "Exogenous effects",
      "GeometricAdstockEffect"
    ]
  },
  {
    "objectID": "reference/GeometricAdstockEffect.html#parameters",
    "href": "reference/GeometricAdstockEffect.html#parameters",
    "title": "GeometricAdstockEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndecay_prior\nDistribution\nPrior distribution for the decay parameter (controls the rate of decay).\nNone\n\n\nrase_error_if_fh_changes\nbool\nWhether to raise an error if the forecasting horizon changes during predict\nrequired",
    "crumbs": [
      "Exogenous effects",
      "GeometricAdstockEffect"
    ]
  },
  {
    "objectID": "reference/Prophetverse.html",
    "href": "reference/Prophetverse.html",
    "title": "Prophetverse",
    "section": "",
    "text": "sktime.Prophetverse(\n    self,\n    trend='linear',\n    exogenous_effects=None,\n    default_effect=None,\n    feature_transformer=None,\n    noise_scale=None,\n    likelihood='normal',\n    scale=None,\n    rng_key=None,\n    inference_engine=None,\n    broadcast_mode='estimator',\n)\nUnivariate Prophetverse forecaster with multiple likelihood options.\nThis forecaster implements a univariate model with support for different likelihoods. It differs from Facebook’s Prophet in several ways: - Logistic trend is parametrized differently, inferring capacity from data. - Arbitrary sktime transformers can be used (e.g., FourierFeatures or HolidayFeatures). - No default weekly or yearly seasonality; these must be provided via the feature_transformer. - Uses ‘changepoint_interval’ instead of ‘n_changepoints’ for selecting changepoints. - Allows for configuring distinct functions for each exogenous variable effect.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[str, BaseEffect]\nType of trend to use. Either “linear” (default) or “logistic”, or a custom effect object.\n'linear'\n\n\nexogenous_effects\nOptional[List[BaseEffect]]\nList of effect objects defining the exogenous effects.\nNone\n\n\ndefault_effect\nOptional[BaseEffect]\nThe default effect for variables without a specified effect.\nNone\n\n\nfeature_transformer\nsktime transformer\nTransformer object to generate additional features (e.g., Fourier terms).\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the observation noise. Must be greater than 0. (default: 0.05)\nNone\n\n\nlikelihood\nstr\nThe likelihood model to use. One of “normal”, “gamma”, or “negbinomial”. (default: “normal”)\n'normal'\n\n\nscale\noptional\nScaling value inferred from the data.\nNone\n\n\nrng_key\noptional\nA jax.random.PRNGKey instance, or None.\nNone\n\n\ninference_engine\noptional\nAn inference engine for running the model.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf noise_scale is not greater than 0 or an unsupported likelihood is provided.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_test_params\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\nsktime.Prophetverse.get_test_params(parameter_set='default')\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set name (currently ignored).\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing test parameters.",
    "crumbs": [
      "Sktime",
      "Prophetverse"
    ]
  },
  {
    "objectID": "reference/Prophetverse.html#parameters",
    "href": "reference/Prophetverse.html#parameters",
    "title": "Prophetverse",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[str, BaseEffect]\nType of trend to use. Either “linear” (default) or “logistic”, or a custom effect object.\n'linear'\n\n\nexogenous_effects\nOptional[List[BaseEffect]]\nList of effect objects defining the exogenous effects.\nNone\n\n\ndefault_effect\nOptional[BaseEffect]\nThe default effect for variables without a specified effect.\nNone\n\n\nfeature_transformer\nsktime transformer\nTransformer object to generate additional features (e.g., Fourier terms).\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the observation noise. Must be greater than 0. (default: 0.05)\nNone\n\n\nlikelihood\nstr\nThe likelihood model to use. One of “normal”, “gamma”, or “negbinomial”. (default: “normal”)\n'normal'\n\n\nscale\noptional\nScaling value inferred from the data.\nNone\n\n\nrng_key\noptional\nA jax.random.PRNGKey instance, or None.\nNone\n\n\ninference_engine\noptional\nAn inference engine for running the model.\nNone",
    "crumbs": [
      "Sktime",
      "Prophetverse"
    ]
  },
  {
    "objectID": "reference/Prophetverse.html#raises",
    "href": "reference/Prophetverse.html#raises",
    "title": "Prophetverse",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf noise_scale is not greater than 0 or an unsupported likelihood is provided.",
    "crumbs": [
      "Sktime",
      "Prophetverse"
    ]
  },
  {
    "objectID": "reference/Prophetverse.html#methods",
    "href": "reference/Prophetverse.html#methods",
    "title": "Prophetverse",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_test_params\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\nsktime.Prophetverse.get_test_params(parameter_set='default')\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set name (currently ignored).\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing test parameters.",
    "crumbs": [
      "Sktime",
      "Prophetverse"
    ]
  },
  {
    "objectID": "reference/MinimizeBudget.html",
    "href": "reference/MinimizeBudget.html",
    "title": "MinimizeBudget",
    "section": "",
    "text": "MinimizeBudget\nbudget_optimization.objectives.MinimizeBudget(self, scale=1)\nMinimize budget constraint objective function.",
    "crumbs": [
      "Objective Functions",
      "MinimizeBudget"
    ]
  },
  {
    "objectID": "reference/TotalBudgetConstraint.html",
    "href": "reference/TotalBudgetConstraint.html",
    "title": "TotalBudgetConstraint",
    "section": "",
    "text": "budget_optimization.constraints.TotalBudgetConstraint(\n    self,\n    channels=None,\n    total=None,\n)\nShared budget constraint.\nThis constraint ensures that the sum of the budgets for the specified channels is equal to the total budget.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nchannels\nlist\nList of channels to be constrained. If None, all channels are used.\nNone\n\n\ntotal\nfloat\nTotal budget. If None, the total budget is computed from the input data.\nNone",
    "crumbs": [
      "Budget Constraints",
      "TotalBudgetConstraint"
    ]
  },
  {
    "objectID": "reference/TotalBudgetConstraint.html#parameters",
    "href": "reference/TotalBudgetConstraint.html#parameters",
    "title": "TotalBudgetConstraint",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nchannels\nlist\nList of channels to be constrained. If None, all channels are used.\nNone\n\n\ntotal\nfloat\nTotal budget. If None, the total budget is computed from the input data.\nNone",
    "crumbs": [
      "Budget Constraints",
      "TotalBudgetConstraint"
    ]
  },
  {
    "objectID": "reference/LiftExperimentLikelihood.html",
    "href": "reference/LiftExperimentLikelihood.html",
    "title": "LiftExperimentLikelihood",
    "section": "",
    "text": "effects.LiftExperimentLikelihood(\n    self,\n    effect,\n    lift_test_results,\n    prior_scale,\n    likelihood_scale=1,\n)\nWrap an effect and applies a normal likelihood to its output.\nThis class uses an input as a reference for the effect, and applies a normal likelihood to the output of the effect.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\neffect\nBaseEffect\nThe effect to wrap.\nrequired\n\n\nlift_test_results\npd.DataFrame\nA dataframe with the lift test results. Should be in sktime format, and must have the same index as the input data.\nrequired\n\n\nprior_scale\nfloat\nThe scale of the prior distribution for the likelihood.\nrequired",
    "crumbs": [
      "MMM Likelihoods",
      "LiftExperimentLikelihood"
    ]
  },
  {
    "objectID": "reference/LiftExperimentLikelihood.html#parameters",
    "href": "reference/LiftExperimentLikelihood.html#parameters",
    "title": "LiftExperimentLikelihood",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\neffect\nBaseEffect\nThe effect to wrap.\nrequired\n\n\nlift_test_results\npd.DataFrame\nA dataframe with the lift test results. Should be in sktime format, and must have the same index as the input data.\nrequired\n\n\nprior_scale\nfloat\nThe scale of the prior distribution for the likelihood.\nrequired",
    "crumbs": [
      "MMM Likelihoods",
      "LiftExperimentLikelihood"
    ]
  },
  {
    "objectID": "reference/PiecewiseLinearTrend.html",
    "href": "reference/PiecewiseLinearTrend.html",
    "title": "PiecewiseLinearTrend",
    "section": "",
    "text": "effects.PiecewiseLinearTrend(\n    self,\n    changepoint_interval=25,\n    changepoint_range=0.8,\n    changepoint_prior_scale=0.001,\n    offset_prior_scale=0.1,\n    squeeze_if_single_series=True,\n    remove_seasonality_before_suggesting_initial_vals=True,\n    global_rate_prior_loc=None,\n    offset_prior_loc=None,\n)\nPiecewise Linear Trend model.\nThis model assumes that the trend is piecewise linear, with changepoints at regular intervals. The number of changepoints is determined by the changepoint_interval and changepoint_range parameters. The changepoint_interval parameter specifies the interval between changepoints, while the changepoint_range parameter specifies the range of the changepoints.\nThis implementation is based on the Prophet_ library. The initial values (global rate and global offset) are suggested using the maximum and minimum values of the time series data.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nchangepoint_interval\nint\nThe interval between changepoints.\n25\n\n\nchangepoint_range\nint\nThe range of the changepoints.\n0.8\n\n\nchangepoint_prior_scale\ndist.Distribution\nThe prior scale for the changepoints.\n0.001\n\n\noffset_prior_scale\nfloat\nThe prior scale for the offset. Default is 0.1.\n0.1\n\n\nsqueeze_if_single_series\nbool\nIf True, squeeze the output if there is only one series. Default is True.\nTrue\n\n\nremove_seasonality_before_suggesting_initial_vals\nbool\nIf True, remove seasonality before suggesting initial values, using sktime’s detrender. Default is True.\nTrue\n\n\nglobal_rate_prior_loc\nfloat\nThe prior location for the global rate. Default is suggested empirically from data.\nNone\n\n\noffset_prior_loc\nfloat\nThe prior location for the offset. Default is suggested empirically from data.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nn_changepoint_per_series\nGet the number of changepoints per series.\n\n\nn_changepoints\nGet the total number of changepoints.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_changepoint_matrix\nReturn the changepoint matrix for the given index.\n\n\n\n\n\neffects.PiecewiseLinearTrend.get_changepoint_matrix(idx)\nReturn the changepoint matrix for the given index.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.PeriodIndex\nThe index for which to compute the changepoint matrix.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\njnp.ndarray: The changepoint matrix.",
    "crumbs": [
      "Trends",
      "PiecewiseLinearTrend"
    ]
  },
  {
    "objectID": "reference/PiecewiseLinearTrend.html#parameters",
    "href": "reference/PiecewiseLinearTrend.html#parameters",
    "title": "PiecewiseLinearTrend",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nchangepoint_interval\nint\nThe interval between changepoints.\n25\n\n\nchangepoint_range\nint\nThe range of the changepoints.\n0.8\n\n\nchangepoint_prior_scale\ndist.Distribution\nThe prior scale for the changepoints.\n0.001\n\n\noffset_prior_scale\nfloat\nThe prior scale for the offset. Default is 0.1.\n0.1\n\n\nsqueeze_if_single_series\nbool\nIf True, squeeze the output if there is only one series. Default is True.\nTrue\n\n\nremove_seasonality_before_suggesting_initial_vals\nbool\nIf True, remove seasonality before suggesting initial values, using sktime’s detrender. Default is True.\nTrue\n\n\nglobal_rate_prior_loc\nfloat\nThe prior location for the global rate. Default is suggested empirically from data.\nNone\n\n\noffset_prior_loc\nfloat\nThe prior location for the offset. Default is suggested empirically from data.\nNone",
    "crumbs": [
      "Trends",
      "PiecewiseLinearTrend"
    ]
  },
  {
    "objectID": "reference/PiecewiseLinearTrend.html#attributes",
    "href": "reference/PiecewiseLinearTrend.html#attributes",
    "title": "PiecewiseLinearTrend",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nn_changepoint_per_series\nGet the number of changepoints per series.\n\n\nn_changepoints\nGet the total number of changepoints.",
    "crumbs": [
      "Trends",
      "PiecewiseLinearTrend"
    ]
  },
  {
    "objectID": "reference/PiecewiseLinearTrend.html#methods",
    "href": "reference/PiecewiseLinearTrend.html#methods",
    "title": "PiecewiseLinearTrend",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_changepoint_matrix\nReturn the changepoint matrix for the given index.\n\n\n\n\n\neffects.PiecewiseLinearTrend.get_changepoint_matrix(idx)\nReturn the changepoint matrix for the given index.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.PeriodIndex\nThe index for which to compute the changepoint matrix.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\njnp.ndarray: The changepoint matrix.",
    "crumbs": [
      "Trends",
      "PiecewiseLinearTrend"
    ]
  },
  {
    "objectID": "reference/LinearEffect.html",
    "href": "reference/LinearEffect.html",
    "title": "LinearEffect",
    "section": "",
    "text": "effects.LinearEffect(\n    self,\n    effect_mode='multiplicative',\n    prior=None,\n    broadcast=False,\n)\nRepresents a linear effect in a hierarchical prophet model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprior\nDistribution\nA numpyro distribution to use as prior. Defaults to dist.Normal(0, 1)\nNone\n\n\neffect_mode\neffects_application\nEither “multiplicative” or “additive” by default “multiplicative”.\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "LinearEffect"
    ]
  },
  {
    "objectID": "reference/LinearEffect.html#parameters",
    "href": "reference/LinearEffect.html#parameters",
    "title": "LinearEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nprior\nDistribution\nA numpyro distribution to use as prior. Defaults to dist.Normal(0, 1)\nNone\n\n\neffect_mode\neffects_application\nEither “multiplicative” or “additive” by default “multiplicative”.\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "LinearEffect"
    ]
  },
  {
    "objectID": "reference/MichaelisMentenEffect.html",
    "href": "reference/MichaelisMentenEffect.html",
    "title": "MichaelisMentenEffect",
    "section": "",
    "text": "effects.MichaelisMentenEffect(\n    self,\n    effect_mode='multiplicative',\n    max_effect_prior=None,\n    half_saturation_prior=None,\n    base_effect_name='trend',\n)\nRepresents a Michaelis-Menten effect in a time series model.\nThe Michaelis-Menten equation is commonly used in biochemistry to describe enzyme kinetics, but it’s also useful for modeling saturation effects in time series analysis. The effect follows the equation:\neffect = (max_effect * data) / (half_saturation + data)\nWhere: - max_effect is the maximum effect value (Vmax in biochemistry) - half_saturation is the value at which effect = max_effect/2 (Km in biochemistry) - data is the input variable (substrate concentration in biochemistry)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmax_effect_prior\nOptional[Distribution]\nPrior distribution for the maximum effect parameter\nNone\n\n\nhalf_saturation_prior\nOptional[Distribution]\nPrior distribution for the half-saturation parameter\nNone\n\n\neffect_mode\neffects_application\nEither “additive” or “multiplicative”, by default “multiplicative”\n'multiplicative'\n\n\nbase_effect_name\nstr\nName of the base effect to multiply with (if multiplicative)\n'trend'"
  },
  {
    "objectID": "reference/MichaelisMentenEffect.html#parameters",
    "href": "reference/MichaelisMentenEffect.html#parameters",
    "title": "MichaelisMentenEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmax_effect_prior\nOptional[Distribution]\nPrior distribution for the maximum effect parameter\nNone\n\n\nhalf_saturation_prior\nOptional[Distribution]\nPrior distribution for the half-saturation parameter\nNone\n\n\neffect_mode\neffects_application\nEither “additive” or “multiplicative”, by default “multiplicative”\n'multiplicative'\n\n\nbase_effect_name\nstr\nName of the base effect to multiply with (if multiplicative)\n'trend'"
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Sktime models\n\n\n\nProphetverse\nUnivariate Prophetverse forecaster with multiple likelihood options.\n\n\nHierarchicalProphet\nA Bayesian hierarchical time series forecasting model based on Meta’s Prophet.\n\n\n\n\n\n\nExogenous effects\n\n\n\nLinearEffect\nRepresents a linear effect in a hierarchical prophet model.\n\n\nLinearFourierSeasonality\nLinear Fourier Seasonality effect.\n\n\nLogEffect\nRepresents a log effect as effect = scale * log(rate * data + 1).\n\n\nHillEffect\nRepresents a Hill effect in a time series model.\n\n\nMichaelisMentenEffect\nRepresents a Michaelis-Menten effect for modeling saturation.\n\n\nChainedEffects\nChains multiple effects sequentially, applying them one after the other.\n\n\nGeometricAdstockEffect\nRepresents a Geometric Adstock effect in a time series model.\n\n\n\n\n\n\nMMM Likelihoods\n\n\n\nLiftExperimentLikelihood\nWrap an effect and applies a normal likelihood to its output.\n\n\nExactLikelihood\nWrap an effect and applies a normal likelihood to its output.\n\n\n\n\n\n\nTrends\n\n\n\nPiecewiseLinearTrend\nPiecewise Linear Trend model.\n\n\nPiecewiseLogisticTrend\nPiecewise logistic trend model.\n\n\nFlatTrend\nFlat trend model.\n\n\n\n\n\n\nLikelihoods for the target variable\n\n\n\nMultivariateNormal\nBase class for effects.\n\n\nNormalTargetLikelihood\n\n\n\nGammaTargetLikelihood\n\n\n\nNegativeBinomialTargetLikelihood\n\n\n\n\n\n\n\nBudget Optimization\n\n\n\nBudgetOptimizer\nBudget optimizer using scipy.optimize.minimize.\n\n\n\n\n\n\nBudget Constraints\n\n\n\nTotalBudgetConstraint\nShared budget constraint.\n\n\nMinimumTargetResponse\nMinimum target response constraint.\n\n\n\n\n\n\nObjective Functions\n\n\n\nMinimizeBudget\nMinimize budget constraint objective function.\n\n\nMaximizeKPI\nMaximize the KPI objective function.\n\n\nMaximizeROI\nMaximize return on investment (ROI) objective function.\n\n\n\n\n\n\nBudget Parametrizations",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#sktime",
    "href": "reference/index.html#sktime",
    "title": "Function reference",
    "section": "",
    "text": "Sktime models\n\n\n\nProphetverse\nUnivariate Prophetverse forecaster with multiple likelihood options.\n\n\nHierarchicalProphet\nA Bayesian hierarchical time series forecasting model based on Meta’s Prophet.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#exogenous-effects",
    "href": "reference/index.html#exogenous-effects",
    "title": "Function reference",
    "section": "",
    "text": "Exogenous effects\n\n\n\nLinearEffect\nRepresents a linear effect in a hierarchical prophet model.\n\n\nLinearFourierSeasonality\nLinear Fourier Seasonality effect.\n\n\nLogEffect\nRepresents a log effect as effect = scale * log(rate * data + 1).\n\n\nHillEffect\nRepresents a Hill effect in a time series model.\n\n\nMichaelisMentenEffect\nRepresents a Michaelis-Menten effect for modeling saturation.\n\n\nChainedEffects\nChains multiple effects sequentially, applying them one after the other.\n\n\nGeometricAdstockEffect\nRepresents a Geometric Adstock effect in a time series model.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#mmm-likelihoods",
    "href": "reference/index.html#mmm-likelihoods",
    "title": "Function reference",
    "section": "",
    "text": "MMM Likelihoods\n\n\n\nLiftExperimentLikelihood\nWrap an effect and applies a normal likelihood to its output.\n\n\nExactLikelihood\nWrap an effect and applies a normal likelihood to its output.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#trends",
    "href": "reference/index.html#trends",
    "title": "Function reference",
    "section": "",
    "text": "Trends\n\n\n\nPiecewiseLinearTrend\nPiecewise Linear Trend model.\n\n\nPiecewiseLogisticTrend\nPiecewise logistic trend model.\n\n\nFlatTrend\nFlat trend model.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#target-likelihoods",
    "href": "reference/index.html#target-likelihoods",
    "title": "Function reference",
    "section": "",
    "text": "Likelihoods for the target variable\n\n\n\nMultivariateNormal\nBase class for effects.\n\n\nNormalTargetLikelihood\n\n\n\nGammaTargetLikelihood\n\n\n\nNegativeBinomialTargetLikelihood",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#budget-optimization",
    "href": "reference/index.html#budget-optimization",
    "title": "Function reference",
    "section": "",
    "text": "Budget Optimization\n\n\n\nBudgetOptimizer\nBudget optimizer using scipy.optimize.minimize.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#budget-constraints",
    "href": "reference/index.html#budget-constraints",
    "title": "Function reference",
    "section": "",
    "text": "Budget Constraints\n\n\n\nTotalBudgetConstraint\nShared budget constraint.\n\n\nMinimumTargetResponse\nMinimum target response constraint.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#objective-functions",
    "href": "reference/index.html#objective-functions",
    "title": "Function reference",
    "section": "",
    "text": "Objective Functions\n\n\n\nMinimizeBudget\nMinimize budget constraint objective function.\n\n\nMaximizeKPI\nMaximize the KPI objective function.\n\n\nMaximizeROI\nMaximize return on investment (ROI) objective function.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#budget-parametrizations",
    "href": "reference/index.html#budget-parametrizations",
    "title": "Function reference",
    "section": "",
    "text": "Budget Parametrizations",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/PiecewiseLogisticTrend.html",
    "href": "reference/PiecewiseLogisticTrend.html",
    "title": "PiecewiseLogisticTrend",
    "section": "",
    "text": "effects.PiecewiseLogisticTrend(\n    self,\n    changepoint_interval=25,\n    changepoint_range=0.8,\n    changepoint_prior_scale=0.001,\n    offset_prior_scale=10,\n    capacity_prior=None,\n    squeeze_if_single_series=True,\n    remove_seasonality_before_suggesting_initial_vals=True,\n    global_rate_prior_loc=None,\n    offset_prior_loc=None,\n)\nPiecewise logistic trend model.\nThis logistic trend differs from the original Prophet logistic trend in that it considers a capacity prior distribution. The capacity prior distribution is used to estimate the maximum value that the time series trend can reach.\nIt uses internally the piecewise linear trend model, and then applies a logistic function to the output of the linear trend model.\nThe initial values (global rate and global offset) are suggested using the maximum and minimum values of the time series data.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nchangepoint_interval\nint\nThe interval between changepoints.\n25\n\n\nchangepoint_range\nint\nThe range of the changepoints.\n0.8\n\n\nchangepoint_prior_scale\ndist.Distribution\nThe prior scale for the changepoints.\n0.001\n\n\noffset_prior_scale\nfloat\nThe prior scale for the offset. Default is 0.1.\n10\n\n\nsqueeze_if_single_series\nbool\nIf True, squeeze the output if there is only one series. Default is True.\nTrue\n\n\nremove_seasonality_before_suggesting_initial_vals\nbool\nIf True, remove seasonality before suggesting initial values, using sktime’s detrender. Default is True.\nTrue\n\n\ncapacity_prior\ndist.Distribution\nThe prior distribution for the capacity. Default is a HalfNormal distribution with loc=1.05 and scale=1.\nNone",
    "crumbs": [
      "Trends",
      "PiecewiseLogisticTrend"
    ]
  },
  {
    "objectID": "reference/PiecewiseLogisticTrend.html#parameters",
    "href": "reference/PiecewiseLogisticTrend.html#parameters",
    "title": "PiecewiseLogisticTrend",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nchangepoint_interval\nint\nThe interval between changepoints.\n25\n\n\nchangepoint_range\nint\nThe range of the changepoints.\n0.8\n\n\nchangepoint_prior_scale\ndist.Distribution\nThe prior scale for the changepoints.\n0.001\n\n\noffset_prior_scale\nfloat\nThe prior scale for the offset. Default is 0.1.\n10\n\n\nsqueeze_if_single_series\nbool\nIf True, squeeze the output if there is only one series. Default is True.\nTrue\n\n\nremove_seasonality_before_suggesting_initial_vals\nbool\nIf True, remove seasonality before suggesting initial values, using sktime’s detrender. Default is True.\nTrue\n\n\ncapacity_prior\ndist.Distribution\nThe prior distribution for the capacity. Default is a HalfNormal distribution with loc=1.05 and scale=1.\nNone",
    "crumbs": [
      "Trends",
      "PiecewiseLogisticTrend"
    ]
  },
  {
    "objectID": "reference/ExactLikelihood.html",
    "href": "reference/ExactLikelihood.html",
    "title": "ExactLikelihood",
    "section": "",
    "text": "effects.ExactLikelihood(self, effect_name, reference_df, prior_scale)\nWrap an effect and applies a normal likelihood to its output.\nThis class uses an input as a reference for the effect, and applies a normal likelihood to the output of the effect.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\neffect_name\nstr\nThe effect to use in the likelihood.\nrequired\n\n\nreference_df\npd.DataFrame\nA dataframe with the reference values. Should be in sktime format, and must have the same index as the input data.\nrequired\n\n\nprior_scale\nfloat\nThe scale of the prior distribution for the likelihood.\nrequired",
    "crumbs": [
      "MMM Likelihoods",
      "ExactLikelihood"
    ]
  },
  {
    "objectID": "reference/ExactLikelihood.html#parameters",
    "href": "reference/ExactLikelihood.html#parameters",
    "title": "ExactLikelihood",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\neffect_name\nstr\nThe effect to use in the likelihood.\nrequired\n\n\nreference_df\npd.DataFrame\nA dataframe with the reference values. Should be in sktime format, and must have the same index as the input data.\nrequired\n\n\nprior_scale\nfloat\nThe scale of the prior distribution for the likelihood.\nrequired",
    "crumbs": [
      "MMM Likelihoods",
      "ExactLikelihood"
    ]
  },
  {
    "objectID": "development.html",
    "href": "development.html",
    "title": "Contributing to Prophetverse",
    "section": "",
    "text": "If you are brand new to Prophetverse or open-source development, we recommend searching the GitHub “issues” tab to find issues that interest you. Unassigned issues labeled Docs and good first issue are typically good for newer contributors.\nOnce you’ve found an interesting issue, it’s a good idea to assign the issue to yourself, so nobody else duplicates the work on it. On the Github issue, a comment with the exact text take to automatically assign you the issue (this will take seconds and may require refreshing the page to see it).\nIf for whatever reason you are not able to continue working with the issue, please unassign it, so other people know it’s available again. You can check the list of assigned issues, since people may not be working in them anymore. If you want to work on one that is assigned, feel free to kindly ask the current assignee if you can take it (please allow at least a week of inactivity before considering work in the issue discontinued). To submit your contribution make a pull request"
  },
  {
    "objectID": "development.html#finding-an-issue-to-contribute-to",
    "href": "development.html#finding-an-issue-to-contribute-to",
    "title": "Contributing to Prophetverse",
    "section": "",
    "text": "If you are brand new to Prophetverse or open-source development, we recommend searching the GitHub “issues” tab to find issues that interest you. Unassigned issues labeled Docs and good first issue are typically good for newer contributors.\nOnce you’ve found an interesting issue, it’s a good idea to assign the issue to yourself, so nobody else duplicates the work on it. On the Github issue, a comment with the exact text take to automatically assign you the issue (this will take seconds and may require refreshing the page to see it).\nIf for whatever reason you are not able to continue working with the issue, please unassign it, so other people know it’s available again. You can check the list of assigned issues, since people may not be working in them anymore. If you want to work on one that is assigned, feel free to kindly ask the current assignee if you can take it (please allow at least a week of inactivity before considering work in the issue discontinued). To submit your contribution make a pull request"
  },
  {
    "objectID": "development.html#tips-for-a-successful-pull-request",
    "href": "development.html#tips-for-a-successful-pull-request",
    "title": "Contributing to Prophetverse",
    "section": "Tips for a successful pull request",
    "text": "Tips for a successful pull request\nIf you have made it to the Making a pull request phase, one of the core contributors may take a look."
  },
  {
    "objectID": "development.html#what-is-a-good-pull-request",
    "href": "development.html#what-is-a-good-pull-request",
    "title": "Contributing to Prophetverse",
    "section": "What is a good pull request?",
    "text": "What is a good pull request?\n\nReference an open issue for non-trivial changes to clarify the PR’s purpose\nEnsure you have appropriate tests. These should be the first part of any PR\nKeep your pull requests as simple as possible. Larger PRs take longer to review\nEnsure that CI is in a green state. Reviewers may not even look otherwise\nKeep Updating your pull request, either by request or every few days"
  },
  {
    "objectID": "development.html#creating-a-development-environment",
    "href": "development.html#creating-a-development-environment",
    "title": "Contributing to Prophetverse",
    "section": "Creating a development environment",
    "text": "Creating a development environment\n\nStep 1: install python\nStep 1: install poetry\nStep 2: install dev dependencies  poetry install --extras dev"
  },
  {
    "objectID": "development.html#contributing-to-the-documentation",
    "href": "development.html#contributing-to-the-documentation",
    "title": "Contributing to Prophetverse",
    "section": "Contributing to the documentation",
    "text": "Contributing to the documentation\nContributing to the documentation benefits everyone who uses Prophetverse. We encourage you to help us improve the documentation, and you don’t have to be an expert on Prophetverse to do so! In fact, there are sections of the docs that are worse off after being written by experts. If something in the docs doesn’t make sense to you, updating the relevant section after you figure it out is a great way to ensure it will help the next person."
  },
  {
    "objectID": "development.html#about-the-documentation",
    "href": "development.html#about-the-documentation",
    "title": "Contributing to Prophetverse",
    "section": "About the documentation:",
    "text": "About the documentation:\nThe documentation is written in mkdocs, you can learn more at mkdocs getting started guide."
  },
  {
    "objectID": "development.html#contributor-community",
    "href": "development.html#contributor-community",
    "title": "Contributing to Prophetverse",
    "section": "Contributor community :",
    "text": "Contributor community :\nCommunity slack: None yet."
  },
  {
    "objectID": "development.html#contributing-to-the-code-base",
    "href": "development.html#contributing-to-the-code-base",
    "title": "Contributing to Prophetverse",
    "section": "Contributing to the code base",
    "text": "Contributing to the code base\n\nCode standards\nWriting good code is not just about what you write. It is also about how you write it. During Continuous Integration testing, several tools will be run to check your code for stylistic errors. Generating any warnings will cause the test to fail. Thus, good style is a requirement for submitting code to Prophetverse.There are of tools in Prophetverse to help contributors verify their changes before contributing to the project\n\nPytest\nYou can test your code with pytest integration with the poetry command  poetry run pytest\nThe CI tests are computationally intensive, so if you want to do a faster test you can run a smoke test with the command  poetry run pytest -m \"not ci\"\nIf you also wanna run the tests even faster feel free to parallel processing the tests with pytest-xdist.\n\n\nPre-commit\nAdditionally, Continuous Integration will run code formatting checks like black, isort, and mypy and more using pre-commit hooks. Any warnings from these checks will cause the Continuous Integration to fail; therefore, it is helpful to run the check yourself before submitting code. This can be done by installing pre-commit (which should already have happened if you followed the instructions in Setting up your development environment) and then running:\n\npre-commit install\nfrom the root of the Prophetverse repository. Now all of the styling checks will be run each time you commit changes without your needing to run each one manually. In addition, using pre-commit will also allow you to more easily remain up-to-date with our code checks as they change.\n\n\npre-commit usage\nNote that if needed, you can skip these checks with git commit –no-verify.\nIf you don’t want to use pre-commit as part of your workflow, you can still use it to run its checks with one of the following:\n pre-commit run --files &lt;files you have modified&gt;   pre-commit run --from-ref=upstream/main --to-ref=HEAD --all-files \nwithout needing to have done pre-commit install beforehand.\nFinally, we also have some slow pre-commit checks, which don’t run on each commit but which do run during continuous integration. You can trigger them manually with:\n pre-commit run --hook-stage manual --all-files"
  },
  {
    "objectID": "index.html#the-flexible-bayesian-forecasting-and-marketing-mix-modeling-package",
    "href": "index.html#the-flexible-bayesian-forecasting-and-marketing-mix-modeling-package",
    "title": "Prophetverse",
    "section": "The flexible bayesian forecasting and Marketing Mix Modeling package",
    "text": "The flexible bayesian forecasting and Marketing Mix Modeling package\nProphetverse leverages the theory behind the Prophet model for time series forecasting and expands it into a more general framework, enabling custom priors, non-linear effects for exogenous variables and other likelihoods. Built on top of sktime and numpyro, Prophetverse aims to provide a flexible and easy-to-use library for time series forecasting with a focus on interpretability and customizability. It is particularly useful for Marketing Mix Modeling, where understanding the effect of different marketing channels on sales is crucial."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Prophetverse",
    "section": "Getting started",
    "text": "Getting started\n\nInstallation\nTo install with pip:\npip install prophetverse\nOr with poetry:\npoetry add prophetverse\n\n\nForecasting with default values\nThe Prophetverse model provides an interface compatible with sktime. Here’s an example of how to use it:\nfrom prophetverse.sktime import Prophetverse\n\n# Create the model\nmodel = Prophetverse()\n\n# Fit the model\nmodel.fit(y=y, X=X)\n\n# Forecast in sample\ny_pred = model.predict(X=X, fh=y.index)"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Prophetverse",
    "section": "Features",
    "text": "Features\nProphetverse is similar to the original Prophet model in many aspects, but it has some differences and new features. The following table summarizes the main features of Prophetverse and compares them with the original Prophet model:\n\n\n\n\n\n\n\n\n\nFeature\nProphetverse\nOriginal Prophet\nMotivation\n\n\n\n\nLogistic trend\nCapacity as a random variable\nCapacity as a hyperparameter, user input required\nThe capacity is usually unknown by the users. Having it as a variable is useful for Total Addressable Market inference\n\n\nCustom trend\nCustomizable trend functions\nNot available\nUsers can create custom trends and leverage their knowledge about the timeseries to enhance long-term accuracy\n\n\nLikelihoods\nGaussian, Gamma and Negative Binomial\nGaussian only\nGaussian likelihood fails to provide good forecasts to positive-only and count data (sales, for example)\n\n\nCustom priors\nSupports custom priors for model parameters and exogenous variables\nNot supported\nForcing positive coefficients, using prior knowledge to model the timeseries\n\n\nCustom exogenous effects\nNon-linear and customizable effects for exogenous variables, shared coefficients between time series\nNot available\nUsers can create any kind of relationship between exogenous variables and the timeseries, which can be useful for Marketing Mix Modeling and other applications.\n\n\nChangepoints\nUses changepoint interval\nUses changepoint number\nThe changepoint number is not stable in the sense that, when the size of timeseries increases, its impact on forecast changes. Think about setting a changepoint number when timeseries has 6 months, and forecasting in future with 2 years of data (4x time original size). Re-tuning would be required. Prophetverse is expected to be more stable\n\n\nScaling\nTime series scaled internally, exogenous variables scaled by the user\nTime series scaled internally\nScaling y is needed to enhance user experience with hyperparameters. On the other hand, not scaling the exogenous variables provide more control to the user and they can leverage sktime’s transformers to handle that.\n\n\nSeasonality\nFourier terms for seasonality passed as exogenous variables\nBuilt-in seasonality handling\nSetting up seasonality requires almost zero effort by using LinearFourierSeasonality in Prophetverse. The idea is to allow the user to create custom seasonalities easily, without hardcoding it in the code.\n\n\nMultivariate model\nHierarchical model with multivariate normal likelihood and LKJ prior, bottom-up forecast\nNot available\nHaving shared coefficients, using global information to enhance individual forecast.\n\n\nImplementation\nNumpyro\nStan"
  },
  {
    "objectID": "mmm/fitting_and_calibration.html",
    "href": "mmm/fitting_and_calibration.html",
    "title": "Forecasting, Calibration, and Unified Marketing Measurement",
    "section": "",
    "text": "In this tutorial, we walk through the lifecycle of a modern Marketing Mix Model (MMM), from time-series forecasting to incorporating causal evidence like lift tests and attribution.\nYou will learn:\n👉 Why this matters: MMMs are foundational for budget allocation. But good predictions are not enough — we need credible effect estimates to make real-world decisions.\nLet’s get started!\nSetting up some libraries, float64 precision, and plot style:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpyro\nimport numpyro.distributions as dist\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nnumpyro.enable_x64()\nfrom prophetverse.datasets._mmm.dataset1 import get_dataset\n\ny, X, lift_tests, true_components, _ = get_dataset()\nlift_test_search, lift_test_social = lift_tests\n\nprint(f\"y shape: {y.shape}, X shape: {X.shape}\")\nX.head()\n\ny shape: (1828,), X shape: (1828, 2)\n\n\n\n\n\n\n\n\n\nad_spend_search\nad_spend_social_media\n\n\n\n\n2000-01-01\n89076.191178\n98587.488958\n\n\n2000-01-02\n88891.993106\n99066.321168\n\n\n2000-01-03\n89784.955064\n97334.106903\n\n\n2000-01-04\n89931.220681\n101747.300585\n\n\n2000-01-05\n89184.319596\n93825.221809",
    "crumbs": [
      "Fitting, calibrating and Unified Marketing Measurement"
    ]
  },
  {
    "objectID": "mmm/fitting_and_calibration.html#part-1-forecasting-with-adstock-saturation-effects",
    "href": "mmm/fitting_and_calibration.html#part-1-forecasting-with-adstock-saturation-effects",
    "title": "Forecasting, Calibration, and Unified Marketing Measurement",
    "section": "Part 1: Forecasting with Adstock & Saturation Effects",
    "text": "Part 1: Forecasting with Adstock & Saturation Effects\nHere we’ll build a time-series forecasting model that includes:\n\nTrend and seasonality\n\nLagged media effects (Adstock)\n\nDiminishing returns (Saturation / Hill curves)\n\n🔎 Why this matters:\nRaw spend is not immediately effective, and it doesn’t convert linearly.\nCapturing these dynamics is essential to make ROI estimates realistic.\n\nfrom prophetverse.effects import (\n    PiecewiseLinearTrend,\n    LinearFourierSeasonality,\n    ChainedEffects,\n    GeometricAdstockEffect,\n    HillEffect,\n)\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.engine.optimizer import LBFGSSolver\n\nyearly = (\n    \"yearly_seasonality\",\n    LinearFourierSeasonality(freq=\"D\", sp_list=[365.25], fourier_terms_list=[5], prior_scale=0.1, effect_mode=\"multiplicative\"),\n    None,\n)\n\nweekly = (\n    \"weekly_seasonality\",\n    LinearFourierSeasonality(freq=\"D\", sp_list=[7], fourier_terms_list=[3], prior_scale=0.05, effect_mode=\"multiplicative\"),\n    None,\n)\n\nhill = HillEffect(\n    half_max_prior=dist.HalfNormal(1),\n    slope_prior=dist.InverseGamma(2, 1),\n    max_effect_prior=dist.HalfNormal(1),\n    effect_mode=\"additive\",\n    input_scale=1e6,\n)\n\nchained_search = (\n    \"ad_spend_search\",\n    ChainedEffects([(\"adstock\", GeometricAdstockEffect()), (\"saturation\", hill)]),\n    \"ad_spend_search\",\n)\nchained_social = (\n    \"ad_spend_social_media\",\n    ChainedEffects([(\"adstock\", GeometricAdstockEffect()), (\"saturation\", hill)]),\n    \"ad_spend_social_media\",\n)\n\n\nbaseline_model = Prophetverse(\n    trend=PiecewiseLinearTrend(changepoint_interval=100),\n    exogenous_effects=[yearly, weekly, chained_search, chained_social],\n    inference_engine=MAPInferenceEngine(\n        num_steps=5000,\n        optimizer=LBFGSSolver(memory_size=300, max_linesearch_steps=300),\n    ),\n)\nbaseline_model.fit(y=y, X=X)\n\nProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 Chained...\n                                                                   max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n                                                                   slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;))]),\n                                 'ad_spend_social_media')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 Chained...\n                                                                   max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n                                                                   slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;))]),\n                                 'ad_spend_social_media')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=100)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=0.1, sp_list=[365.25])LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[3],\n                         freq='D', prior_scale=0.05, sp_list=[7])ad_spend_searchChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;)ad_spend_social_mediaChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine(num_steps=5000,\n                   optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                         memory_size=300))\n\n\n\ny_pred = baseline_model.predict(X=X, fh=X.index)\n\nplt.figure(figsize=(8, 4))\ny.plot(label=\"Observed\")\ny_pred.plot(label=\"Predicted\")\nplt.title(\"In-Sample Forecast: Observed vs Predicted\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n1.1 Component-Level Diagnostics\nWith predict_components, we can obtain the model’s components.\n\ny_pred_components = baseline_model.predict_components(X=X, fh=X.index)\ny_pred_components.head()\n\n\n\n\n\n\n\n\nad_spend_search\nad_spend_social_media\nmean\nobs\ntrend\nweekly_seasonality\nyearly_seasonality\n\n\n\n\n2000-01-01\n3.248404e-142\n9.938527e+06\n1.013302e+07\n1.011236e+07\n209456.628526\n9018.345677\n-23987.280944\n\n\n2000-01-02\n3.268777e-142\n1.076268e+07\n1.095673e+07\n1.091248e+07\n218277.857343\n353.912482\n-24581.649816\n\n\n2000-01-03\n3.276154e-142\n1.099999e+07\n1.120483e+07\n1.126308e+07\n227099.086160\n2826.317316\n-25091.939669\n\n\n2000-01-04\n3.279368e-142\n1.113403e+07\n1.133325e+07\n1.129282e+07\n235920.314977\n-11179.413043\n-25513.010868\n\n\n2000-01-05\n3.280680e-142\n1.113046e+07\n1.134788e+07\n1.129927e+07\n244741.543794\n-1478.499893\n-25839.938436\n\n\n\n\n\n\n\nIn a real use-casee, you would not have access to the ground truth of the components. We use them here to show how the model behaves, and how incorporing extra information can improve it.\n\nfig, axs = plt.subplots(4, 1, figsize=(8, 12), sharex=True)\nfor i, name in enumerate(\n    [\"trend\", \"yearly_seasonality\", \"ad_spend_search\", \"ad_spend_social_media\"]\n):\n    true_components[name].plot(ax=axs[i], label=\"True\", color=\"black\")\n    y_pred_components[name].plot(ax=axs[i], label=\"Estimated\")\n    axs[i].set_title(name)\n    axs[i].legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.2 Backtesting with Cross-Validation\nWe use rolling-window CV to assess out-of-sample accuracy using MAPE.\n🧠 Caution: Low error ≠ correct attribution. But high error often indicates a bad model.\n\nfrom sktime.split import ExpandingWindowSplitter\nfrom sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\nfrom sktime.forecasting.model_evaluation import evaluate\n\nmetric = MeanAbsolutePercentageError()\ncv = ExpandingWindowSplitter(\n    initial_window=365 * 3, step_length=180, fh=list(range(1, 180))\n)\ncv_results = evaluate(\n    forecaster=baseline_model, y=y, X=X, cv=cv, scoring=metric, return_data=True\n)\ncv_results\n\n\n\n\n\n\n\n\ntest_MeanAbsolutePercentageError\nfit_time\npred_time\nlen_train_window\ncutoff\ny_train\ny_test\ny_pred\n\n\n\n\n0\n0.066920\n18.554837\n0.673911\n1095\n2002-12-30\n2000-01-01 1.081551e+07 2000-01-02 1.112...\n2002-12-31 1.306520e+07 2003-01-01 1.429...\n2002-12-31 1.116013e+07 2003-01-01 1.167...\n\n\n1\n0.058951\n46.089615\n0.612454\n1275\n2003-06-28\n2000-01-01 1.081551e+07 2000-01-02 1.112...\n2003-06-29 1.725815e+07 2003-06-30 1.696...\n2003-06-29 1.618745e+07 2003-06-30 1.652...\n\n\n2\n0.052211\n20.876224\n0.618459\n1455\n2003-12-25\n2000-01-01 1.081551e+07 2000-01-02 1.112...\n2003-12-26 2.511414e+07 2003-12-27 2.496...\n2003-12-26 2.437842e+07 2003-12-27 2.476...\n\n\n3\n0.057693\n21.975533\n0.638152\n1635\n2004-06-22\n2000-01-01 1.081551e+07 2000-01-02 1.112...\n2004-06-23 2.913561e+07 2004-06-24 2.878...\n2004-06-23 3.049679e+07 2004-06-24 3.017...\n\n\n\n\n\n\n\nThe average error across folds is:\n\ncv_results[\"test_MeanAbsolutePercentageError\"].mean()\n\nnp.float64(0.058943899162743875)\n\n\nWe can visualize them by iterating the dataframe:\n\nfor idx, row in cv_results.iterrows():\n    plt.figure(figsize=(8, 2))\n    observed = pd.concat([row[\"y_train\"].iloc[-100:], row[\"y_test\"]])\n    observed.plot(label=\"Observed\", color=\"black\")\n    row[\"y_pred\"].plot(label=\"Prediction\")\n    plt.title(f\"Fold {idx + 1} – MAPE: {row['test_MeanAbsolutePercentageError']:.2%}\")\n    plt.legend()\n    plt.show()\n    if idx &gt; 3:\n        break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.3 Saturation Curves\nThese curves show diminishing marginal effect as spend increases.\n🔍 Insight: This shape helps guide budget allocation decisions (e.g. where additional spend will have little return).\nNote how the model captures a saturation effect, but it is still far from the correct shape.\nThis is why, in many situations, you will need calibration to correct the model’s behavior. This is what we will do in the next section.\n\nfig, axs = plt.subplots(figsize=(8, 6), nrows=1, ncols=2)\n\nfor ax, channel in zip(axs, [\"ad_spend_search\", \"ad_spend_social_media\"]):\n    ax.scatter(\n        X[channel],\n        y_pred_components[channel],\n        alpha=0.6,\n        label=channel,\n    )\n    ax.scatter(\n        X[channel],\n        true_components[channel],\n        color=\"black\",\n        label=\"True Effect\",\n    )\n    ax.set(\n        xlabel=\"Daily Spend\",\n        ylabel=\"Incremental Effect\",\n        title=f\"{channel} - Saturation Curve\",\n    )\n    ax.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Fitting, calibrating and Unified Marketing Measurement"
    ]
  },
  {
    "objectID": "mmm/fitting_and_calibration.html#part-2-calibration-with-causal-evidence",
    "href": "mmm/fitting_and_calibration.html#part-2-calibration-with-causal-evidence",
    "title": "Forecasting, Calibration, and Unified Marketing Measurement",
    "section": "Part 2: Calibration with Causal Evidence",
    "text": "Part 2: Calibration with Causal Evidence\nTime-series alone cannot disentangle correlated channels.\nWe integrate lift tests (local experiments) and attribution models (high-resolution signal) to correct this.\n\nlift_test_search\n\n\n\n\n\n\n\n\nlift\nx_start\nx_end\n\n\n\n\n2000-09-04\n0.986209\n88991.849098\n88616.674279\n\n\n2003-07-17\n1.602655\n25147.476284\n30137.721832\n\n\n2004-04-11\n0.762057\n58323.761863\n46723.500000\n\n\n2003-01-06\n0.758514\n23857.995845\n21913.505457\n\n\n2003-03-07\n1.332419\n29036.114641\n31676.191662\n\n\n2001-01-17\n1.142551\n71184.337297\n80915.848594\n\n\n2003-04-12\n1.654931\n36588.754410\n46777.071912\n\n\n2002-02-16\n0.825554\n67892.914951\n55030.729840\n\n\n2001-10-05\n0.929293\n69504.770296\n65128.356512\n\n\n2000-10-02\n0.994120\n85176.526599\n88936.541885\n\n\n2003-06-05\n0.760863\n27988.449143\n24654.281482\n\n\n2000-07-07\n0.897929\n97458.154049\n79856.919150\n\n\n2003-09-28\n1.570214\n42328.076114\n54497.175586\n\n\n2002-08-24\n0.863911\n35824.789963\n33256.068138\n\n\n2000-11-28\n1.242113\n69593.024187\n92943.044427\n\n\n2001-12-28\n0.881569\n71500.973097\n69504.085790\n\n\n2001-09-02\n1.197270\n69966.800254\n93695.953636\n\n\n2002-12-16\n1.660649\n7810.413156\n9322.237192\n\n\n2004-08-30\n1.286178\n47599.593625\n56735.271957\n\n\n2001-02-18\n1.254430\n64837.275444\n81784.074961\n\n\n2000-05-15\n1.044025\n100464.794301\n121779.610581\n\n\n2004-08-13\n0.967493\n57864.200257\n56080.324427\n\n\n2001-02-02\n1.212479\n67857.642170\n88071.282712\n\n\n2001-05-21\n1.178636\n71337.927880\n93445.016948\n\n\n2003-05-06\n0.644814\n25609.287364\n21843.082411\n\n\n2000-07-31\n1.012828\n92617.293678\n85766.681515\n\n\n2002-07-03\n1.267595\n57443.596207\n66595.562035\n\n\n2004-09-27\n1.369890\n41546.443755\n50176.823146\n\n\n2000-10-25\n0.985062\n72757.343228\n70195.986606\n\n\n2002-11-02\n0.834577\n19043.272758\n18373.397899\n\n\n\n\n\n\n\n\n2.1 Visualizing Lift Tests\nEach experiment records: pre-spend (x_start), post-spend (x_end), and measured lift. These give us causal “ground truth” deltas.\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Scatter plot for pre-spend and observed lift\nax.scatter(lift_test_search[\"x_start\"], [1] * len(lift_test_search), label=\"Pre-Spend\", alpha=0.6)\nax.scatter(lift_test_search[\"x_end\"], lift_test_search[\"lift\"], label=\"Observed Lift\", alpha=0.6)\n\n# Annotate with arrows to show lift effect\nfor _, row in lift_test_search.iterrows():\n    ax.annotate(\n        \"\",\n        xy=(row[\"x_end\"], row[\"lift\"]),\n        xytext=(row[\"x_start\"], 1),\n        arrowprops=dict(arrowstyle=\"-&gt;\", alpha=0.5),\n    )\n\n# Add horizontal line and labels\nax.axhline(1, linestyle=\"--\", color=\"gray\", alpha=0.7)\nax.set(\n    title=\"Search Ads Lift Tests\",\n    xlabel=\"Spend\",\n    ylabel=\"Revenue Ratio\",\n)\n\n# Add legend and finalize layout\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.2 Improve Estimates via LiftExperimentLikelihood\nThis adds a new likelihood term that makes the model match lift observations.\n🔁 Still Bayesian: It incorporates test variance and model uncertainty.\nSince we use sktime interface, we have access to get_params() and set_params(**kwargs) methods. This allows us to easily swap effects and likelihoods. When we define our model, the effect’s name become a key in the model’s get_params() dictionary. We can use this to set the effect’s parameters directly.\n\nfrom prophetverse.effects.lift_likelihood import LiftExperimentLikelihood\n\nmodel_lift = baseline_model.clone()\nmodel_lift.set_params(\n    ad_spend_search=LiftExperimentLikelihood(\n        effect=baseline_model.get_params()[\"ad_spend_search\"],\n        lift_test_results=lift_test_search,\n        prior_scale=0.05,\n    ),\n    ad_spend_social_media=LiftExperimentLikelihood(\n        effect=baseline_model.get_params()[\"ad_spend_social_media\"],\n        lift_test_results=lift_test_social,\n        prior_scale=0.05,\n    ),\n)\n\nmodel_lift.fit(y=y, X=X)\n\nProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 LiftExp...\n2002-07-03  1.053316   25537.869750   38025.254409\n2004-09-27  1.010456    8523.749415   11037.466255\n2000-10-25  0.980559   50964.957119   65038.733878\n2002-11-02  1.355164    1195.765141    1603.253926,\n                                                          prior_scale=0.05),\n                                 'ad_spend_social_media')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 LiftExp...\n2002-07-03  1.053316   25537.869750   38025.254409\n2004-09-27  1.010456    8523.749415   11037.466255\n2000-10-25  0.980559   50964.957119   65038.733878\n2002-11-02  1.355164    1195.765141    1603.253926,\n                                                          prior_scale=0.05),\n                                 'ad_spend_social_media')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=100)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=0.1, sp_list=[365.25])LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[3],\n                         freq='D', prior_scale=0.05, sp_list=[7])ad_spend_searchLiftExperimentLikelihood(effect=ChainedEffects(steps=[('adstock',\n                                                       GeometricAdstockEffect()),\n                                                      ('saturation',\n                                                       HillEffect(effect_mode='additive',\n                                                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n                                                                  input_scale=1000000.0,\n                                                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f08...\n2001-02-02  1.212479   67857.642170   88071.282712\n2001-05-21  1.178636   71337.927880   93445.016948\n2003-05-06  0.644814   25609.287364   21843.082411\n2000-07-31  1.012828   92617.293678   85766.681515\n2002-07-03  1.267595   57443.596207   66595.562035\n2004-09-27  1.369890   41546.443755   50176.823146\n2000-10-25  0.985062   72757.343228   70195.986606\n2002-11-02  0.834577   19043.272758   18373.397899,\n                         prior_scale=0.05)effect: ChainedEffectsChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;)ad_spend_social_mediaLiftExperimentLikelihood(effect=ChainedEffects(steps=[('adstock',\n                                                       GeometricAdstockEffect()),\n                                                      ('saturation',\n                                                       HillEffect(effect_mode='additive',\n                                                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n                                                                  input_scale=1000000.0,\n                                                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f08...\n2001-02-02  1.085461   38129.336619   47626.107330\n2001-05-21  0.961702   50010.574434   73721.957893\n2003-05-06  1.099837    2893.044038    2702.425237\n2000-07-31  1.028477  105196.357631  125200.174092\n2002-07-03  1.053316   25537.869750   38025.254409\n2004-09-27  1.010456    8523.749415   11037.466255\n2000-10-25  0.980559   50964.957119   65038.733878\n2002-11-02  1.355164    1195.765141    1603.253926,\n                         prior_scale=0.05)effect: ChainedEffectsChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine(num_steps=5000,\n                   optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                         memory_size=300))\n\n\n\ncomponents_lift = model_lift.predict_components(X=X, fh=X.index)\ncomponents_lift.head()\n\n\n\n\n\n\n\n\nad_spend_search\nad_spend_social_media\nmean\nobs\ntrend\nweekly_seasonality\nyearly_seasonality\n\n\n\n\n2000-01-01\n6.895926e+06\n3.175476e+06\n1.048024e+07\n1.045962e+07\n440291.367469\n18676.814590\n-50131.863307\n\n\n2000-01-02\n7.567479e+06\n3.223350e+06\n1.119076e+07\n1.114657e+07\n449462.659740\n793.114837\n-50328.557617\n\n\n2000-01-03\n7.681069e+06\n3.232674e+06\n1.132772e+07\n1.138588e+07\n458633.952011\n5733.194775\n-50386.367115\n\n\n2000-01-04\n7.702442e+06\n3.238811e+06\n1.133666e+07\n1.129629e+07\n467805.244281\n-22094.875301\n-50300.495120\n\n\n2000-01-05\n7.675578e+06\n3.235107e+06\n1.133469e+07\n1.128614e+07\n476976.536552\n-2907.825424\n-50066.486125\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(figsize=(8, 6), ncols=2)\n\nfor ax, channel in zip(axs, [\"ad_spend_search\", \"ad_spend_social_media\"]):\n    ax.scatter(\n        X[channel],\n        y_pred_components[channel],\n        label=\"Baseline\",\n        alpha=0.6,\n        s=50,\n    )\n    ax.scatter(\n        X[channel],\n        components_lift[channel],\n        label=\"With Lift Test\",\n        alpha=0.6,\n        s=50,\n    )\n    ax.plot(\n        X[channel],\n        true_components[channel],\n        label=\"True\",\n        color=\"black\",\n        linewidth=2,\n    )\n    ax.set(\n        title=f\"{channel} Predicted Effects\",\n        xlabel=\"Daily Spend\",\n        ylabel=\"Incremental Effect\",\n    )\n    ax.axhline(0, linestyle=\"--\", color=\"gray\", alpha=0.7)\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nMuch better, right? And it was implemented with a really modular and flexible code. You could wrap any effect with LiftExperimentLikelihood to add lift test data to guide its behaviour. Nevertheless, this is not the end of the story.\n\n\n2.3 Add Attribution Signals with ExactLikelihood\nAttribution models can provide daily signals. If available, you can incorporate them by adding another likelihood term via ExactLikelihood.\nWe create a synthetic attribution signal by multiplying the true effect with a random noise factor.\n\nfrom prophetverse.effects import ExactLikelihood\n\nrng = np.random.default_rng(42)\n\n# Generate attribution signals for search and social media channels\nattr_search = true_components[[\"ad_spend_search\"]] * rng.normal(\n    1, 0.1, size=(len(y), 1)\n)\nattr_social = true_components[[\"ad_spend_social_media\"]] * rng.normal(\n    1, 0.1, size=(len(y), 1)\n)\n\n# Display the first few rows of the social media attribution signal\nattr_social.head()\n\n\n\n\n\n\n\n\nad_spend_social_media\n\n\n\n\n2000-01-01\n2.171846e+06\n\n\n2000-01-02\n2.625088e+06\n\n\n2000-01-03\n2.983027e+06\n\n\n2000-01-04\n2.836756e+06\n\n\n2000-01-05\n2.520331e+06\n\n\n\n\n\n\n\n\nmodel_umm = model_lift.clone()\nmodel_umm.set_params(\n    exogenous_effects=model_lift.get_params()[\"exogenous_effects\"]\n    + [\n        (\n            \"attribution_search\",\n            ExactLikelihood(\"ad_spend_search\", attr_search, 0.01),\n            None,\n        ),\n        (\n            \"attribution_social_media\",\n            ExactLikelihood(\"ad_spend_social_media\", attr_social, 0.01),\n            None,\n        ),\n    ]\n)\nmodel_umm.fit(y=y, X=X)\n\nProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 LiftExp...\n2000-01-04           2.836756e+06\n2000-01-05           2.520331e+06\n...                           ...\n2004-12-28           1.012305e+06\n2004-12-29           1.108765e+06\n2004-12-30           1.070854e+06\n2004-12-31           1.027756e+06\n2005-01-01           1.012141e+06\n\n[1828 rows x 1 columns]),\n                                 None)],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 LiftExp...\n2000-01-04           2.836756e+06\n2000-01-05           2.520331e+06\n...                           ...\n2004-12-28           1.012305e+06\n2004-12-29           1.108765e+06\n2004-12-30           1.070854e+06\n2004-12-31           1.027756e+06\n2005-01-01           1.012141e+06\n\n[1828 rows x 1 columns]),\n                                 None)],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=100)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=0.1, sp_list=[365.25])LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[3],\n                         freq='D', prior_scale=0.05, sp_list=[7])ad_spend_searchLiftExperimentLikelihood(effect=ChainedEffects(steps=[('adstock',\n                                                       GeometricAdstockEffect()),\n                                                      ('saturation',\n                                                       HillEffect(effect_mode='additive',\n                                                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n                                                                  input_scale=1000000.0,\n                                                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f08...\n2001-02-02  1.212479   67857.642170   88071.282712\n2001-05-21  1.178636   71337.927880   93445.016948\n2003-05-06  0.644814   25609.287364   21843.082411\n2000-07-31  1.012828   92617.293678   85766.681515\n2002-07-03  1.267595   57443.596207   66595.562035\n2004-09-27  1.369890   41546.443755   50176.823146\n2000-10-25  0.985062   72757.343228   70195.986606\n2002-11-02  0.834577   19043.272758   18373.397899,\n                         prior_scale=0.05)effect: ChainedEffectsChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;)ad_spend_social_mediaLiftExperimentLikelihood(effect=ChainedEffects(steps=[('adstock',\n                                                       GeometricAdstockEffect()),\n                                                      ('saturation',\n                                                       HillEffect(effect_mode='additive',\n                                                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n                                                                  input_scale=1000000.0,\n                                                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f08...\n2001-02-02  1.085461   38129.336619   47626.107330\n2001-05-21  0.961702   50010.574434   73721.957893\n2003-05-06  1.099837    2893.044038    2702.425237\n2000-07-31  1.028477  105196.357631  125200.174092\n2002-07-03  1.053316   25537.869750   38025.254409\n2004-09-27  1.010456    8523.749415   11037.466255\n2000-10-25  0.980559   50964.957119   65038.733878\n2002-11-02  1.355164    1195.765141    1603.253926,\n                         prior_scale=0.05)effect: ChainedEffectsChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c2c69d0 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f086c343050 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f0864712a90 with batch shape () and event shape ()&gt;)ExactLikelihoodExactLikelihood(effect_name='ad_spend_search', prior_scale=0.01,\n                reference_df=            ad_spend_search\n2000-01-01     8.682649e+06\n2000-01-02     7.542228e+06\n2000-01-03     9.091821e+06\n2000-01-04     9.259557e+06\n2000-01-05     6.785853e+06\n...                     ...\n2004-12-28     3.547301e+06\n2004-12-29     2.771639e+06\n2004-12-30     2.972279e+06\n2004-12-31     3.226681e+06\n2005-01-01     3.605501e+06\n\n[1828 rows x 1 columns])ExactLikelihoodExactLikelihood(effect_name='ad_spend_social_media', prior_scale=0.01,\n                reference_df=            ad_spend_social_media\n2000-01-01           2.171846e+06\n2000-01-02           2.625088e+06\n2000-01-03           2.983027e+06\n2000-01-04           2.836756e+06\n2000-01-05           2.520331e+06\n...                           ...\n2004-12-28           1.012305e+06\n2004-12-29           1.108765e+06\n2004-12-30           1.070854e+06\n2004-12-31           1.027756e+06\n2005-01-01           1.012141e+06\n\n[1828 rows x 1 columns])inference_engineMAPInferenceEngineMAPInferenceEngine(num_steps=5000,\n                   optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                         memory_size=300))\n\n\n\ncomponents_umm = model_umm.predict_components(X=X, fh=X.index)\n\nfig, axs = plt.subplots(2, 1, figsize=(8, 10), sharex=True)\nfor ax, channel in zip(axs, [\"ad_spend_search\", \"ad_spend_social_media\"]):\n    ax.scatter(X[channel], y_pred_components[channel], label=\"Baseline\", alpha=0.4)\n    ax.scatter(X[channel], components_lift[channel], label=\"With Lift Test\", alpha=0.4)\n    ax.scatter(X[channel], components_umm[channel], label=\"With Attribution\", alpha=0.4)\n    ax.plot(X[channel], true_components[channel], label=\"True Effect\", color=\"black\")\n    ax.set_title(channel)\n    ax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nEven better! And, due to sktime-like interface, wrapping and adding new effects is easy.",
    "crumbs": [
      "Fitting, calibrating and Unified Marketing Measurement"
    ]
  },
  {
    "objectID": "mmm/fitting_and_calibration.html#final-thoughts-toward-unified-marketing-measurement",
    "href": "mmm/fitting_and_calibration.html#final-thoughts-toward-unified-marketing-measurement",
    "title": "Forecasting, Calibration, and Unified Marketing Measurement",
    "section": "Final Thoughts: Toward Unified Marketing Measurement",
    "text": "Final Thoughts: Toward Unified Marketing Measurement\n✅ What we learned:\n1. Adstock + saturation are essential to capture media dynamics.\n2. Good predictions ≠ good attribution.\n3. Causal data like lift tests can correct misattribution.\n4. Attribution signals add further constraints.\n🛠️ Use this when:\n* Channels are correlated → Use lift tests.\n* You have granular model output → Add attribution likelihoods.\n🧪 Model selection tip:\nAlways validate causal logic, not just fit quality.\nWith Prophetverse, you can combine observational, experimental, and model-based signals into one coherent MMM+UMM pipeline.",
    "crumbs": [
      "Fitting, calibrating and Unified Marketing Measurement"
    ]
  }
]