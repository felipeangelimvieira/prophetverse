[
  {
    "objectID": "tutorials/univariate.html",
    "href": "tutorials/univariate.html",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "",
    "text": "This example shows how to use Prophetverse to perform univariate forecasting with a time series dataset, using sktime-style interface.\nBecause of this compatibility, you can benefit from all the features of sktime, such as hierarchical reconciliation, ensemble models, pipelines, etc. There are two main methods to use Prophetverse with sktime:\nLater in this example, we will also show additional methods to make predictions, such as predict_quantiles and predict_components.\nimport warnings\nwarnings.simplefilter(action=\"ignore\")\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom numpyro import distributions as dist\nimport numpyro\n\nnumpyro.enable_x64()  # To avoid computational issues",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#import-dataset",
    "href": "tutorials/univariate.html#import-dataset",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Import dataset",
    "text": "Import dataset\nWe import a dataset from Prophet’s original repository. We then put it into sktime-friendly format, where the index is a pd.PeriodIndex and the colums are the time series.\n\nfrom prophetverse.datasets.loaders import load_peyton_manning\n\ny = load_peyton_manning()\ndisplay(y.head())\n\n\n\n\n\n\n\n\ny\n\n\nds\n\n\n\n\n\n2007-12-10\n9.590761\n\n\n2007-12-11\n8.519590\n\n\n2007-12-12\n8.183677\n\n\n2007-12-13\n8.072467\n\n\n2007-12-14\n7.893572\n\n\n\n\n\n\n\nThe full dataset looks like this:\n\ny.plot.line(figsize=(8, 6))\nplt.show()",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#fit-model",
    "href": "tutorials/univariate.html#fit-model",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Fit model",
    "text": "Fit model\nHere, we will show how you can fit a simple model with Prophetverse. We first fit a model without seasonal components, and then fit a full model. We also show how easy it is to switch between Maximum A Posteriori (MAP) inference and Markov Chain Monte Carlo (MCMC).\n\nNo seasonality\n\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils import no_input_columns\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-250,\n    ),\n    inference_engine=MAPInferenceEngine(),\n)\nmodel.fit(y=y)\n\nProphetverse(inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(8, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)\n\n\n\n\n\n\n\n\n\n\nWith seasonality\nHere, we fit the univariate Prophet and pass an exogenous effect as hyperparameter. The exogenous_effects parameter let us add new components to the model and control the relationship between exogenous variables and the target variable.\nIn this case, the LinearFourierSeasonality effect creates sinusoidal and cosine terms to model the seasonality of the time series, which are then multiplied by linear coefficients and added to the model.\nThis argument is a list of tuples of the form (effect_name, effect, regex_to_filter_relevant_columns), where effect_name is a string and effect is an instance of a subclass of prophetverse.effects.BaseEffect. The regex is used to filter the columns of X that are relevant for the effect, but can also be None (or its alias prophetverse.utils.no_input_columns) if no input in X is needed for the effect.\n\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.utils import no_input_columns\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-500,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                freq=\"D\",\n                sp_list=[7, 365.25],\n                fourier_terms_list=[3, 10],\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n    ],\n    inference_engine=MAPInferenceEngine(),\n)\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(8, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#probabilistic-forecasting",
    "href": "tutorials/univariate.html#probabilistic-forecasting",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Probabilistic forecasting",
    "text": "Probabilistic forecasting\nWe can also make probabilistic forecasts with Prophetverse, in sktime fashion. The predict_quantiles method returns the quantiles of the predictive distribution in a pd.DataFrame\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.1, 0.9])\nquantiles.head()\n\n\n\n\n\n\n\n\ny\n\n\n\n0.1\n0.9\n\n\n\n\n2007-01-01\n8.047412\n9.356146\n\n\n2007-01-02\n7.836059\n9.125930\n\n\n2007-01-03\n7.785226\n9.044309\n\n\n2007-01-04\n7.715282\n9.009478\n\n\n2007-01-05\n7.754124\n9.043622\n\n\n\n\n\n\n\nThe plot below shows the (0.1, 0.9) quantiles of the predictive distribution\n\nfig, ax = plt.subplots(figsize=(8, 5))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=1)",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#timeseries-decomposition",
    "href": "tutorials/univariate.html#timeseries-decomposition",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Timeseries decomposition",
    "text": "Timeseries decomposition\nWe can easily extract the components of the time series with the predict_components method. This method, in particular, is not implemented in sktime’s BaseForecaster, but it is a method of prophetverse.Prophetverse class.\n\nsites = model.predict_components(fh=forecast_horizon)\nsites.head()\n\n\n\n\n\n\n\n\nmean\nobs\nseasonality\ntrend\n\n\n\n\n2007-01-01\n8.716897\n8.707676\n0.916926\n7.799971\n\n\n2007-01-02\n8.520570\n8.500814\n0.720600\n7.799971\n\n\n2007-01-03\n8.366813\n8.392820\n0.566843\n7.799971\n\n\n2007-01-04\n8.387161\n8.369109\n0.587191\n7.799971\n\n\n2007-01-05\n8.415625\n8.393918\n0.615654\n7.799971\n\n\n\n\n\n\n\n\nfor column in sites.columns:\n    fig, ax = plt.subplots(figsize=(8, 2))\n    ax.plot(sites.index.to_timestamp(), sites[column], label=column)\n    ax.set_title(column)\n    fig.show()",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#fitting-with-mcmc",
    "href": "tutorials/univariate.html#fitting-with-mcmc",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Fitting with MCMC",
    "text": "Fitting with MCMC\nIn the previous examples, we used MAP inference to fit the model. However, we can also use Markov Chain Monte Carlo (MCMC) to fit the model. To do this, we just need to change the inference_engine parameter to MCMCInferenceEngine. The rest of the code remains the same.\nThe set_params method is used to set the parameters of the model, in sklearn fashion.\n\nfrom prophetverse.engine import MCMCInferenceEngine\n\nmodel.set_params(inference_engine=MCMCInferenceEngine(num_warmup=1000))\n\n\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=1000),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=1000),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMCMCInferenceEngineMCMCInferenceEngine(num_warmup=1000)\n\n\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.75, 0.25])\nfig, ax = plt.subplots(figsize=(8, 5))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=1)\n\n\n\n\n\n\n\n\nOne interesting feature of MCMC is that it allows us to obtain samples from the posterior distribution of the parameters. In other words, we can also obtain probabilistic forecasts for the TS components.\n\nsamples = model.predict_component_samples(fh=forecast_horizon)\nsamples\n\n\n\n\n\n\n\n\n\nmean\nobs\nseasonality\ntrend\n\n\nsample\n\n\n\n\n\n\n\n\n\n0\n2007-01-01\n8.766647\n8.993755\n0.911963\n7.854683\n\n\n2007-01-02\n8.603121\n8.961214\n0.748438\n7.854683\n\n\n2007-01-03\n8.367542\n7.589113\n0.512859\n7.854683\n\n\n2007-01-04\n8.440357\n9.144632\n0.585673\n7.854683\n\n\n2007-01-05\n8.514072\n8.527602\n0.659388\n7.854683\n\n\n...\n...\n...\n...\n...\n...\n\n\n999\n2017-12-28\n7.408649\n6.519647\n0.346344\n7.062305\n\n\n2017-12-29\n7.396831\n6.769823\n0.335339\n7.061492\n\n\n2017-12-30\n7.247843\n7.795866\n0.187163\n7.060679\n\n\n2017-12-31\n7.603085\n7.073530\n0.543218\n7.059867\n\n\n2018-01-01\n7.883352\n8.442698\n0.824297\n7.059054\n\n\n\n\n4019000 rows × 4 columns",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#fitting-with-variational-inference",
    "href": "tutorials/univariate.html#fitting-with-variational-inference",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Fitting with Variational Inference",
    "text": "Fitting with Variational Inference\nAnother inference method available in Prophetverse is Variational Inference (VI). This method is generally faster than MCMC and can be used for larger datasets, while still providing probabilistic samples for the parameters.\nTo use VI, we just need to change the inference_engine parameter to VIInferenceEngine. The rest of the code remains the same.\n\n\n\n\n\n\nTip\n\n\n\nChoose carefully the number of steps and the optimizer. The default optimizer might not work well for all use-cases\n\n\n\nfrom prophetverse.engine import VIInferenceEngine\nfrom prophetverse import LBFGSSolver, CosineScheduleAdamOptimizer\n\n\noptimizer = CosineScheduleAdamOptimizer()\nmodel.set_params(\n    inference_engine=VIInferenceEngine(\n        progress_bar=True, num_steps=100_000, optimizer=optimizer\n    )\n)\n\n\nmodel.fit(y=y)\n\n  0%|          | 0/100000 [00:00&lt;?, ?it/s]  0%|          | 1/100000 [00:00&lt;21:16:55,  1.31it/s]  0%|          | 354/100000 [00:00&lt;02:59, 555.73it/s]  1%|          | 716/100000 [00:00&lt;01:28, 1117.04it/s]  1%|          | 1126/100000 [00:01&lt;00:56, 1735.04it/s]  1%|▏         | 1484/100000 [00:01&lt;00:45, 2156.17it/s]  2%|▏         | 1857/100000 [00:01&lt;00:38, 2542.02it/s]  2%|▏         | 2218/100000 [00:01&lt;00:34, 2818.04it/s]  3%|▎         | 2628/100000 [00:01&lt;00:30, 3163.25it/s]  3%|▎         | 3042/100000 [00:01&lt;00:28, 3434.06it/s]  3%|▎         | 3458/100000 [00:01&lt;00:26, 3639.24it/s]  4%|▍         | 3853/100000 [00:01&lt;00:26, 3640.12it/s]  4%|▍         | 4242/100000 [00:01&lt;00:25, 3710.66it/s]  5%|▍         | 4629/100000 [00:01&lt;00:25, 3739.50it/s]  5%|▌         | 5023/100000 [00:02&lt;00:25, 3796.39it/s, init loss: 22917835223.3965, avg. loss [1-5000]: 8902531163.0927]  5%|▌         | 5411/100000 [00:02&lt;00:25, 3765.47it/s, init loss: 22917835223.3965, avg. loss [1-5000]: 8902531163.0927]  6%|▌         | 5807/100000 [00:02&lt;00:24, 3821.44it/s, init loss: 22917835223.3965, avg. loss [1-5000]: 8902531163.0927]  6%|▌         | 6193/100000 [00:02&lt;00:24, 3774.50it/s, init loss: 22917835223.3965, avg. loss [1-5000]: 8902531163.0927]  7%|▋         | 6574/100000 [00:02&lt;00:25, 3730.28it/s, init loss: 22917835223.3965, avg. loss [1-5000]: 8902531163.0927]  7%|▋         | 6963/100000 [00:02&lt;00:24, 3774.68it/s, init loss: 22917835223.3965, avg. loss [1-5000]: 8902531163.0927]  7%|▋         | 7342/100000 [00:02&lt;00:24, 3729.38it/s, init loss: 22917835223.3965, avg. loss [1-5000]: 8902531163.0927]  8%|▊         | 7720/100000 [00:02&lt;00:24, 3741.65it/s, init loss: 22917835223.3965, avg. loss [1-5000]: 8902531163.0927]  8%|▊         | 8113/100000 [00:02&lt;00:24, 3795.01it/s, init loss: 22917835223.3965, avg. loss [1-5000]: 8902531163.0927]  8%|▊         | 8494/100000 [00:03&lt;00:24, 3782.95it/s, init loss: 22917835223.3965, avg. loss [1-5000]: 8902531163.0927]  9%|▉         | 8873/100000 [00:03&lt;00:24, 3695.65it/s, init loss: 22917835223.3965, avg. loss [1-5000]: 8902531163.0927]  9%|▉         | 9268/100000 [00:03&lt;00:24, 3768.82it/s, init loss: 22917835223.3965, avg. loss [1-5000]: 8902531163.0927] 10%|▉         | 9652/100000 [00:03&lt;00:23, 3789.56it/s, init loss: 22917835223.3965, avg. loss [1-5000]: 8902531163.0927] 10%|█         | 10032/100000 [00:03&lt;00:23, 3749.56it/s, init loss: 22917835223.3965, avg. loss [5001-10000]: 365463518.6272] 10%|█         | 10408/100000 [00:03&lt;00:24, 3705.53it/s, init loss: 22917835223.3965, avg. loss [5001-10000]: 365463518.6272] 11%|█         | 10782/100000 [00:03&lt;00:24, 3713.85it/s, init loss: 22917835223.3965, avg. loss [5001-10000]: 365463518.6272] 11%|█         | 11177/100000 [00:03&lt;00:23, 3781.62it/s, init loss: 22917835223.3965, avg. loss [5001-10000]: 365463518.6272] 12%|█▏        | 11587/100000 [00:03&lt;00:22, 3874.93it/s, init loss: 22917835223.3965, avg. loss [5001-10000]: 365463518.6272] 12%|█▏        | 11977/100000 [00:03&lt;00:22, 3881.31it/s, init loss: 22917835223.3965, avg. loss [5001-10000]: 365463518.6272] 12%|█▏        | 12366/100000 [00:04&lt;00:23, 3798.20it/s, init loss: 22917835223.3965, avg. loss [5001-10000]: 365463518.6272] 13%|█▎        | 12772/100000 [00:04&lt;00:22, 3872.38it/s, init loss: 22917835223.3965, avg. loss [5001-10000]: 365463518.6272] 13%|█▎        | 13182/100000 [00:04&lt;00:22, 3939.34it/s, init loss: 22917835223.3965, avg. loss [5001-10000]: 365463518.6272] 14%|█▎        | 13577/100000 [00:04&lt;00:22, 3879.88it/s, init loss: 22917835223.3965, avg. loss [5001-10000]: 365463518.6272] 14%|█▍        | 13966/100000 [00:04&lt;00:22, 3841.92it/s, init loss: 22917835223.3965, avg. loss [5001-10000]: 365463518.6272] 14%|█▍        | 14351/100000 [00:04&lt;00:22, 3823.13it/s, init loss: 22917835223.3965, avg. loss [5001-10000]: 365463518.6272] 15%|█▍        | 14744/100000 [00:04&lt;00:22, 3853.13it/s, init loss: 22917835223.3965, avg. loss [5001-10000]: 365463518.6272] 15%|█▌        | 15130/100000 [00:04&lt;00:22, 3801.92it/s, init loss: 22917835223.3965, avg. loss [10001-15000]: 32986751.0654] 16%|█▌        | 15541/100000 [00:04&lt;00:21, 3889.91it/s, init loss: 22917835223.3965, avg. loss [10001-15000]: 32986751.0654] 16%|█▌        | 15931/100000 [00:04&lt;00:22, 3806.49it/s, init loss: 22917835223.3965, avg. loss [10001-15000]: 32986751.0654] 16%|█▋        | 16328/100000 [00:05&lt;00:21, 3853.68it/s, init loss: 22917835223.3965, avg. loss [10001-15000]: 32986751.0654] 17%|█▋        | 16714/100000 [00:05&lt;00:21, 3808.55it/s, init loss: 22917835223.3965, avg. loss [10001-15000]: 32986751.0654] 17%|█▋        | 17096/100000 [00:05&lt;00:21, 3787.53it/s, init loss: 22917835223.3965, avg. loss [10001-15000]: 32986751.0654] 17%|█▋        | 17476/100000 [00:05&lt;00:21, 3780.13it/s, init loss: 22917835223.3965, avg. loss [10001-15000]: 32986751.0654] 18%|█▊        | 17884/100000 [00:05&lt;00:21, 3865.48it/s, init loss: 22917835223.3965, avg. loss [10001-15000]: 32986751.0654] 18%|█▊        | 18286/100000 [00:05&lt;00:20, 3910.40it/s, init loss: 22917835223.3965, avg. loss [10001-15000]: 32986751.0654] 19%|█▊        | 18678/100000 [00:05&lt;00:21, 3823.25it/s, init loss: 22917835223.3965, avg. loss [10001-15000]: 32986751.0654] 19%|█▉        | 19076/100000 [00:05&lt;00:20, 3868.71it/s, init loss: 22917835223.3965, avg. loss [10001-15000]: 32986751.0654] 19%|█▉        | 19483/100000 [00:05&lt;00:20, 3926.86it/s, init loss: 22917835223.3965, avg. loss [10001-15000]: 32986751.0654] 20%|█▉        | 19877/100000 [00:05&lt;00:20, 3893.62it/s, init loss: 22917835223.3965, avg. loss [10001-15000]: 32986751.0654] 20%|██        | 20272/100000 [00:06&lt;00:20, 3909.85it/s, init loss: 22917835223.3965, avg. loss [15001-20000]: 3276970.1186]  21%|██        | 20664/100000 [00:06&lt;00:20, 3841.62it/s, init loss: 22917835223.3965, avg. loss [15001-20000]: 3276970.1186] 21%|██        | 21049/100000 [00:06&lt;00:20, 3834.97it/s, init loss: 22917835223.3965, avg. loss [15001-20000]: 3276970.1186] 21%|██▏       | 21450/100000 [00:06&lt;00:20, 3884.72it/s, init loss: 22917835223.3965, avg. loss [15001-20000]: 3276970.1186] 22%|██▏       | 21839/100000 [00:06&lt;00:20, 3877.89it/s, init loss: 22917835223.3965, avg. loss [15001-20000]: 3276970.1186] 22%|██▏       | 22227/100000 [00:06&lt;00:20, 3767.96it/s, init loss: 22917835223.3965, avg. loss [15001-20000]: 3276970.1186] 23%|██▎       | 22633/100000 [00:06&lt;00:20, 3851.42it/s, init loss: 22917835223.3965, avg. loss [15001-20000]: 3276970.1186] 23%|██▎       | 23019/100000 [00:06&lt;00:20, 3789.32it/s, init loss: 22917835223.3965, avg. loss [15001-20000]: 3276970.1186] 23%|██▎       | 23399/100000 [00:06&lt;00:20, 3731.80it/s, init loss: 22917835223.3965, avg. loss [15001-20000]: 3276970.1186] 24%|██▍       | 23794/100000 [00:06&lt;00:20, 3793.27it/s, init loss: 22917835223.3965, avg. loss [15001-20000]: 3276970.1186] 24%|██▍       | 24206/100000 [00:07&lt;00:19, 3886.99it/s, init loss: 22917835223.3965, avg. loss [15001-20000]: 3276970.1186] 25%|██▍       | 24615/100000 [00:07&lt;00:19, 3944.99it/s, init loss: 22917835223.3965, avg. loss [15001-20000]: 3276970.1186] 25%|██▌       | 25010/100000 [00:07&lt;00:19, 3825.41it/s, init loss: 22917835223.3965, avg. loss [20001-25000]: 338664.2764]  25%|██▌       | 25394/100000 [00:07&lt;00:19, 3747.91it/s, init loss: 22917835223.3965, avg. loss [20001-25000]: 338664.2764] 26%|██▌       | 25784/100000 [00:07&lt;00:19, 3791.63it/s, init loss: 22917835223.3965, avg. loss [20001-25000]: 338664.2764] 26%|██▌       | 26194/100000 [00:07&lt;00:19, 3880.41it/s, init loss: 22917835223.3965, avg. loss [20001-25000]: 338664.2764] 27%|██▋       | 26605/100000 [00:07&lt;00:18, 3946.48it/s, init loss: 22917835223.3965, avg. loss [20001-25000]: 338664.2764] 27%|██▋       | 27001/100000 [00:07&lt;00:18, 3843.28it/s, init loss: 22917835223.3965, avg. loss [20001-25000]: 338664.2764] 27%|██▋       | 27387/100000 [00:07&lt;00:18, 3844.32it/s, init loss: 22917835223.3965, avg. loss [20001-25000]: 338664.2764] 28%|██▊       | 27794/100000 [00:08&lt;00:18, 3909.26it/s, init loss: 22917835223.3965, avg. loss [20001-25000]: 338664.2764] 28%|██▊       | 28186/100000 [00:08&lt;00:18, 3808.75it/s, init loss: 22917835223.3965, avg. loss [20001-25000]: 338664.2764] 29%|██▊       | 28581/100000 [00:08&lt;00:18, 3847.94it/s, init loss: 22917835223.3965, avg. loss [20001-25000]: 338664.2764] 29%|██▉       | 28987/100000 [00:08&lt;00:18, 3909.33it/s, init loss: 22917835223.3965, avg. loss [20001-25000]: 338664.2764] 29%|██▉       | 29402/100000 [00:08&lt;00:17, 3977.83it/s, init loss: 22917835223.3965, avg. loss [20001-25000]: 338664.2764] 30%|██▉       | 29801/100000 [00:08&lt;00:17, 3928.33it/s, init loss: 22917835223.3965, avg. loss [20001-25000]: 338664.2764] 30%|███       | 30195/100000 [00:08&lt;00:18, 3787.90it/s, init loss: 22917835223.3965, avg. loss [25001-30000]: 47149.1611]  31%|███       | 30576/100000 [00:08&lt;00:18, 3754.83it/s, init loss: 22917835223.3965, avg. loss [25001-30000]: 47149.1611] 31%|███       | 30968/100000 [00:08&lt;00:18, 3802.06it/s, init loss: 22917835223.3965, avg. loss [25001-30000]: 47149.1611] 31%|███▏      | 31349/100000 [00:08&lt;00:18, 3723.86it/s, init loss: 22917835223.3965, avg. loss [25001-30000]: 47149.1611] 32%|███▏      | 31723/100000 [00:09&lt;00:18, 3698.70it/s, init loss: 22917835223.3965, avg. loss [25001-30000]: 47149.1611] 32%|███▏      | 32110/100000 [00:09&lt;00:18, 3746.84it/s, init loss: 22917835223.3965, avg. loss [25001-30000]: 47149.1611] 32%|███▏      | 32486/100000 [00:09&lt;00:18, 3676.39it/s, init loss: 22917835223.3965, avg. loss [25001-30000]: 47149.1611] 33%|███▎      | 32855/100000 [00:09&lt;00:18, 3670.90it/s, init loss: 22917835223.3965, avg. loss [25001-30000]: 47149.1611] 33%|███▎      | 33243/100000 [00:09&lt;00:17, 3731.90it/s, init loss: 22917835223.3965, avg. loss [25001-30000]: 47149.1611] 34%|███▎      | 33617/100000 [00:09&lt;00:17, 3693.54it/s, init loss: 22917835223.3965, avg. loss [25001-30000]: 47149.1611] 34%|███▍      | 34014/100000 [00:09&lt;00:17, 3774.06it/s, init loss: 22917835223.3965, avg. loss [25001-30000]: 47149.1611] 34%|███▍      | 34400/100000 [00:09&lt;00:17, 3798.66it/s, init loss: 22917835223.3965, avg. loss [25001-30000]: 47149.1611] 35%|███▍      | 34781/100000 [00:09&lt;00:17, 3734.31it/s, init loss: 22917835223.3965, avg. loss [25001-30000]: 47149.1611] 35%|███▌      | 35155/100000 [00:09&lt;00:17, 3712.90it/s, init loss: 22917835223.3965, avg. loss [30001-35000]: 16997.5104] 36%|███▌      | 35559/100000 [00:10&lt;00:16, 3808.19it/s, init loss: 22917835223.3965, avg. loss [30001-35000]: 16997.5104] 36%|███▌      | 35941/100000 [00:10&lt;00:16, 3788.62it/s, init loss: 22917835223.3965, avg. loss [30001-35000]: 16997.5104] 36%|███▋      | 36321/100000 [00:10&lt;00:17, 3737.45it/s, init loss: 22917835223.3965, avg. loss [30001-35000]: 16997.5104] 37%|███▋      | 36696/100000 [00:10&lt;00:17, 3722.28it/s, init loss: 22917835223.3965, avg. loss [30001-35000]: 16997.5104] 37%|███▋      | 37069/100000 [00:10&lt;00:16, 3720.98it/s, init loss: 22917835223.3965, avg. loss [30001-35000]: 16997.5104] 37%|███▋      | 37478/100000 [00:10&lt;00:16, 3829.15it/s, init loss: 22917835223.3965, avg. loss [30001-35000]: 16997.5104] 38%|███▊      | 37868/100000 [00:10&lt;00:16, 3849.15it/s, init loss: 22917835223.3965, avg. loss [30001-35000]: 16997.5104] 38%|███▊      | 38273/100000 [00:10&lt;00:15, 3907.90it/s, init loss: 22917835223.3965, avg. loss [30001-35000]: 16997.5104] 39%|███▊      | 38664/100000 [00:10&lt;00:15, 3853.36it/s, init loss: 22917835223.3965, avg. loss [30001-35000]: 16997.5104] 39%|███▉      | 39050/100000 [00:11&lt;00:16, 3787.75it/s, init loss: 22917835223.3965, avg. loss [30001-35000]: 16997.5104] 39%|███▉      | 39455/100000 [00:11&lt;00:15, 3863.51it/s, init loss: 22917835223.3965, avg. loss [30001-35000]: 16997.5104] 40%|███▉      | 39842/100000 [00:11&lt;00:15, 3808.60it/s, init loss: 22917835223.3965, avg. loss [30001-35000]: 16997.5104] 40%|████      | 40224/100000 [00:11&lt;00:16, 3708.06it/s, init loss: 22917835223.3965, avg. loss [35001-40000]: 10272.4040] 41%|████      | 40606/100000 [00:11&lt;00:15, 3739.80it/s, init loss: 22917835223.3965, avg. loss [35001-40000]: 10272.4040] 41%|████      | 40981/100000 [00:11&lt;00:15, 3709.00it/s, init loss: 22917835223.3965, avg. loss [35001-40000]: 10272.4040] 41%|████▏     | 41391/100000 [00:11&lt;00:15, 3821.95it/s, init loss: 22917835223.3965, avg. loss [35001-40000]: 10272.4040] 42%|████▏     | 41774/100000 [00:11&lt;00:15, 3779.24it/s, init loss: 22917835223.3965, avg. loss [35001-40000]: 10272.4040] 42%|████▏     | 42167/100000 [00:11&lt;00:15, 3821.85it/s, init loss: 22917835223.3965, avg. loss [35001-40000]: 10272.4040] 43%|████▎     | 42550/100000 [00:11&lt;00:15, 3796.38it/s, init loss: 22917835223.3965, avg. loss [35001-40000]: 10272.4040] 43%|████▎     | 42934/100000 [00:12&lt;00:14, 3806.43it/s, init loss: 22917835223.3965, avg. loss [35001-40000]: 10272.4040] 43%|████▎     | 43315/100000 [00:12&lt;00:14, 3804.39it/s, init loss: 22917835223.3965, avg. loss [35001-40000]: 10272.4040] 44%|████▎     | 43696/100000 [00:12&lt;00:15, 3733.37it/s, init loss: 22917835223.3965, avg. loss [35001-40000]: 10272.4040] 44%|████▍     | 44097/100000 [00:12&lt;00:14, 3811.80it/s, init loss: 22917835223.3965, avg. loss [35001-40000]: 10272.4040] 44%|████▍     | 44479/100000 [00:12&lt;00:14, 3751.33it/s, init loss: 22917835223.3965, avg. loss [35001-40000]: 10272.4040] 45%|████▍     | 44856/100000 [00:12&lt;00:14, 3754.53it/s, init loss: 22917835223.3965, avg. loss [35001-40000]: 10272.4040] 45%|████▌     | 45232/100000 [00:12&lt;00:14, 3727.11it/s, init loss: 22917835223.3965, avg. loss [40001-45000]: 6127.6165]  46%|████▌     | 45615/100000 [00:12&lt;00:14, 3755.46it/s, init loss: 22917835223.3965, avg. loss [40001-45000]: 6127.6165] 46%|████▌     | 45997/100000 [00:12&lt;00:14, 3774.25it/s, init loss: 22917835223.3965, avg. loss [40001-45000]: 6127.6165] 46%|████▋     | 46375/100000 [00:12&lt;00:14, 3733.43it/s, init loss: 22917835223.3965, avg. loss [40001-45000]: 6127.6165] 47%|████▋     | 46778/100000 [00:13&lt;00:13, 3819.68it/s, init loss: 22917835223.3965, avg. loss [40001-45000]: 6127.6165] 47%|████▋     | 47161/100000 [00:13&lt;00:14, 3758.83it/s, init loss: 22917835223.3965, avg. loss [40001-45000]: 6127.6165] 48%|████▊     | 47543/100000 [00:13&lt;00:13, 3774.66it/s, init loss: 22917835223.3965, avg. loss [40001-45000]: 6127.6165] 48%|████▊     | 47941/100000 [00:13&lt;00:13, 3834.42it/s, init loss: 22917835223.3965, avg. loss [40001-45000]: 6127.6165] 48%|████▊     | 48351/100000 [00:13&lt;00:13, 3911.45it/s, init loss: 22917835223.3965, avg. loss [40001-45000]: 6127.6165] 49%|████▉     | 48761/100000 [00:13&lt;00:12, 3967.39it/s, init loss: 22917835223.3965, avg. loss [40001-45000]: 6127.6165] 49%|████▉     | 49176/100000 [00:13&lt;00:12, 4019.68it/s, init loss: 22917835223.3965, avg. loss [40001-45000]: 6127.6165] 50%|████▉     | 49579/100000 [00:13&lt;00:12, 3963.40it/s, init loss: 22917835223.3965, avg. loss [40001-45000]: 6127.6165] 50%|████▉     | 49976/100000 [00:13&lt;00:12, 3865.09it/s, init loss: 22917835223.3965, avg. loss [40001-45000]: 6127.6165] 50%|█████     | 50364/100000 [00:13&lt;00:13, 3804.95it/s, init loss: 22917835223.3965, avg. loss [45001-50000]: 3776.0849] 51%|█████     | 50746/100000 [00:14&lt;00:13, 3762.82it/s, init loss: 22917835223.3965, avg. loss [45001-50000]: 3776.0849] 51%|█████     | 51132/100000 [00:14&lt;00:12, 3788.13it/s, init loss: 22917835223.3965, avg. loss [45001-50000]: 3776.0849] 52%|█████▏    | 51512/100000 [00:14&lt;00:13, 3729.70it/s, init loss: 22917835223.3965, avg. loss [45001-50000]: 3776.0849] 52%|█████▏    | 51899/100000 [00:14&lt;00:12, 3768.84it/s, init loss: 22917835223.3965, avg. loss [45001-50000]: 3776.0849] 52%|█████▏    | 52311/100000 [00:14&lt;00:12, 3871.71it/s, init loss: 22917835223.3965, avg. loss [45001-50000]: 3776.0849] 53%|█████▎    | 52724/100000 [00:14&lt;00:11, 3945.30it/s, init loss: 22917835223.3965, avg. loss [45001-50000]: 3776.0849] 53%|█████▎    | 53126/100000 [00:14&lt;00:11, 3967.04it/s, init loss: 22917835223.3965, avg. loss [45001-50000]: 3776.0849] 54%|█████▎    | 53523/100000 [00:14&lt;00:11, 3897.27it/s, init loss: 22917835223.3965, avg. loss [45001-50000]: 3776.0849] 54%|█████▍    | 53926/100000 [00:14&lt;00:11, 3933.53it/s, init loss: 22917835223.3965, avg. loss [45001-50000]: 3776.0849] 54%|█████▍    | 54320/100000 [00:15&lt;00:11, 3836.00it/s, init loss: 22917835223.3965, avg. loss [45001-50000]: 3776.0849] 55%|█████▍    | 54705/100000 [00:15&lt;00:11, 3775.75it/s, init loss: 22917835223.3965, avg. loss [45001-50000]: 3776.0849] 55%|█████▌    | 55084/100000 [00:15&lt;00:11, 3774.51it/s, init loss: 22917835223.3965, avg. loss [50001-55000]: 2335.9004] 55%|█████▌    | 55472/100000 [00:15&lt;00:11, 3802.62it/s, init loss: 22917835223.3965, avg. loss [50001-55000]: 2335.9004] 56%|█████▌    | 55853/100000 [00:15&lt;00:11, 3760.06it/s, init loss: 22917835223.3965, avg. loss [50001-55000]: 2335.9004] 56%|█████▌    | 56237/100000 [00:15&lt;00:11, 3781.50it/s, init loss: 22917835223.3965, avg. loss [50001-55000]: 2335.9004] 57%|█████▋    | 56647/100000 [00:15&lt;00:11, 3874.34it/s, init loss: 22917835223.3965, avg. loss [50001-55000]: 2335.9004] 57%|█████▋    | 57044/100000 [00:15&lt;00:11, 3902.63it/s, init loss: 22917835223.3965, avg. loss [50001-55000]: 2335.9004] 57%|█████▋    | 57435/100000 [00:15&lt;00:10, 3904.53it/s, init loss: 22917835223.3965, avg. loss [50001-55000]: 2335.9004] 58%|█████▊    | 57826/100000 [00:15&lt;00:10, 3894.54it/s, init loss: 22917835223.3965, avg. loss [50001-55000]: 2335.9004] 58%|█████▊    | 58216/100000 [00:16&lt;00:10, 3864.28it/s, init loss: 22917835223.3965, avg. loss [50001-55000]: 2335.9004] 59%|█████▊    | 58603/100000 [00:16&lt;00:10, 3797.28it/s, init loss: 22917835223.3965, avg. loss [50001-55000]: 2335.9004] 59%|█████▉    | 58984/100000 [00:16&lt;00:11, 3717.27it/s, init loss: 22917835223.3965, avg. loss [50001-55000]: 2335.9004] 59%|█████▉    | 59357/100000 [00:16&lt;00:10, 3697.36it/s, init loss: 22917835223.3965, avg. loss [50001-55000]: 2335.9004] 60%|█████▉    | 59728/100000 [00:16&lt;00:10, 3662.16it/s, init loss: 22917835223.3965, avg. loss [50001-55000]: 2335.9004] 60%|██████    | 60100/100000 [00:16&lt;00:10, 3676.46it/s, init loss: 22917835223.3965, avg. loss [55001-60000]: 1136.8837] 61%|██████    | 60508/100000 [00:16&lt;00:10, 3794.96it/s, init loss: 22917835223.3965, avg. loss [55001-60000]: 1136.8837] 61%|██████    | 60888/100000 [00:16&lt;00:10, 3721.93it/s, init loss: 22917835223.3965, avg. loss [55001-60000]: 1136.8837] 61%|██████▏   | 61283/100000 [00:16&lt;00:10, 3786.50it/s, init loss: 22917835223.3965, avg. loss [55001-60000]: 1136.8837] 62%|██████▏   | 61663/100000 [00:16&lt;00:10, 3700.00it/s, init loss: 22917835223.3965, avg. loss [55001-60000]: 1136.8837] 62%|██████▏   | 62034/100000 [00:17&lt;00:10, 3690.87it/s, init loss: 22917835223.3965, avg. loss [55001-60000]: 1136.8837] 62%|██████▏   | 62426/100000 [00:17&lt;00:10, 3755.52it/s, init loss: 22917835223.3965, avg. loss [55001-60000]: 1136.8837] 63%|██████▎   | 62832/100000 [00:17&lt;00:09, 3844.16it/s, init loss: 22917835223.3965, avg. loss [55001-60000]: 1136.8837] 63%|██████▎   | 63223/100000 [00:17&lt;00:09, 3863.63it/s, init loss: 22917835223.3965, avg. loss [55001-60000]: 1136.8837] 64%|██████▎   | 63615/100000 [00:17&lt;00:09, 3878.94it/s, init loss: 22917835223.3965, avg. loss [55001-60000]: 1136.8837] 64%|██████▍   | 64004/100000 [00:17&lt;00:09, 3808.45it/s, init loss: 22917835223.3965, avg. loss [55001-60000]: 1136.8837] 64%|██████▍   | 64406/100000 [00:17&lt;00:09, 3869.41it/s, init loss: 22917835223.3965, avg. loss [55001-60000]: 1136.8837] 65%|██████▍   | 64818/100000 [00:17&lt;00:08, 3941.41it/s, init loss: 22917835223.3965, avg. loss [55001-60000]: 1136.8837] 65%|██████▌   | 65213/100000 [00:17&lt;00:08, 3883.98it/s, init loss: 22917835223.3965, avg. loss [60001-65000]: 158.2418]  66%|██████▌   | 65606/100000 [00:17&lt;00:08, 3894.78it/s, init loss: 22917835223.3965, avg. loss [60001-65000]: 158.2418] 66%|██████▌   | 65996/100000 [00:18&lt;00:08, 3863.36it/s, init loss: 22917835223.3965, avg. loss [60001-65000]: 158.2418] 66%|██████▋   | 66383/100000 [00:18&lt;00:08, 3826.35it/s, init loss: 22917835223.3965, avg. loss [60001-65000]: 158.2418] 67%|██████▋   | 66766/100000 [00:18&lt;00:08, 3821.47it/s, init loss: 22917835223.3965, avg. loss [60001-65000]: 158.2418] 67%|██████▋   | 67166/100000 [00:18&lt;00:08, 3874.13it/s, init loss: 22917835223.3965, avg. loss [60001-65000]: 158.2418] 68%|██████▊   | 67554/100000 [00:18&lt;00:08, 3800.72it/s, init loss: 22917835223.3965, avg. loss [60001-65000]: 158.2418] 68%|██████▊   | 67935/100000 [00:18&lt;00:08, 3785.65it/s, init loss: 22917835223.3965, avg. loss [60001-65000]: 158.2418] 68%|██████▊   | 68314/100000 [00:18&lt;00:08, 3748.35it/s, init loss: 22917835223.3965, avg. loss [60001-65000]: 158.2418] 69%|██████▊   | 68698/100000 [00:18&lt;00:08, 3773.14it/s, init loss: 22917835223.3965, avg. loss [60001-65000]: 158.2418] 69%|██████▉   | 69076/100000 [00:18&lt;00:08, 3718.83it/s, init loss: 22917835223.3965, avg. loss [60001-65000]: 158.2418] 69%|██████▉   | 69463/100000 [00:18&lt;00:08, 3761.97it/s, init loss: 22917835223.3965, avg. loss [60001-65000]: 158.2418] 70%|██████▉   | 69840/100000 [00:19&lt;00:08, 3729.42it/s, init loss: 22917835223.3965, avg. loss [60001-65000]: 158.2418] 70%|███████   | 70233/100000 [00:19&lt;00:07, 3788.37it/s, init loss: 22917835223.3965, avg. loss [65001-70000]: -600.7649] 71%|███████   | 70613/100000 [00:19&lt;00:07, 3790.01it/s, init loss: 22917835223.3965, avg. loss [65001-70000]: -600.7649] 71%|███████   | 71008/100000 [00:19&lt;00:07, 3837.47it/s, init loss: 22917835223.3965, avg. loss [65001-70000]: -600.7649] 71%|███████▏  | 71417/100000 [00:19&lt;00:07, 3912.12it/s, init loss: 22917835223.3965, avg. loss [65001-70000]: -600.7649] 72%|███████▏  | 71809/100000 [00:19&lt;00:07, 3911.01it/s, init loss: 22917835223.3965, avg. loss [65001-70000]: -600.7649] 72%|███████▏  | 72201/100000 [00:19&lt;00:07, 3878.38it/s, init loss: 22917835223.3965, avg. loss [65001-70000]: -600.7649] 73%|███████▎  | 72601/100000 [00:19&lt;00:07, 3912.27it/s, init loss: 22917835223.3965, avg. loss [65001-70000]: -600.7649] 73%|███████▎  | 72993/100000 [00:19&lt;00:06, 3899.31it/s, init loss: 22917835223.3965, avg. loss [65001-70000]: -600.7649] 73%|███████▎  | 73384/100000 [00:20&lt;00:06, 3837.22it/s, init loss: 22917835223.3965, avg. loss [65001-70000]: -600.7649] 74%|███████▍  | 73790/100000 [00:20&lt;00:06, 3902.04it/s, init loss: 22917835223.3965, avg. loss [65001-70000]: -600.7649] 74%|███████▍  | 74181/100000 [00:20&lt;00:06, 3853.06it/s, init loss: 22917835223.3965, avg. loss [65001-70000]: -600.7649] 75%|███████▍  | 74567/100000 [00:20&lt;00:06, 3748.66it/s, init loss: 22917835223.3965, avg. loss [65001-70000]: -600.7649] 75%|███████▍  | 74953/100000 [00:20&lt;00:06, 3779.60it/s, init loss: 22917835223.3965, avg. loss [65001-70000]: -600.7649] 75%|███████▌  | 75340/100000 [00:20&lt;00:06, 3805.56it/s, init loss: 22917835223.3965, avg. loss [70001-75000]: -1263.6386] 76%|███████▌  | 75736/100000 [00:20&lt;00:06, 3849.25it/s, init loss: 22917835223.3965, avg. loss [70001-75000]: -1263.6386] 76%|███████▌  | 76127/100000 [00:20&lt;00:06, 3866.95it/s, init loss: 22917835223.3965, avg. loss [70001-75000]: -1263.6386] 77%|███████▋  | 76514/100000 [00:20&lt;00:06, 3840.31it/s, init loss: 22917835223.3965, avg. loss [70001-75000]: -1263.6386] 77%|███████▋  | 76899/100000 [00:20&lt;00:06, 3766.02it/s, init loss: 22917835223.3965, avg. loss [70001-75000]: -1263.6386] 77%|███████▋  | 77277/100000 [00:21&lt;00:06, 3759.01it/s, init loss: 22917835223.3965, avg. loss [70001-75000]: -1263.6386] 78%|███████▊  | 77673/100000 [00:21&lt;00:05, 3816.99it/s, init loss: 22917835223.3965, avg. loss [70001-75000]: -1263.6386] 78%|███████▊  | 78055/100000 [00:21&lt;00:05, 3756.18it/s, init loss: 22917835223.3965, avg. loss [70001-75000]: -1263.6386] 78%|███████▊  | 78461/100000 [00:21&lt;00:05, 3843.42it/s, init loss: 22917835223.3965, avg. loss [70001-75000]: -1263.6386] 79%|███████▉  | 78872/100000 [00:21&lt;00:05, 3919.51it/s, init loss: 22917835223.3965, avg. loss [70001-75000]: -1263.6386] 79%|███████▉  | 79265/100000 [00:21&lt;00:05, 3880.01it/s, init loss: 22917835223.3965, avg. loss [70001-75000]: -1263.6386] 80%|███████▉  | 79654/100000 [00:21&lt;00:05, 3778.34it/s, init loss: 22917835223.3965, avg. loss [70001-75000]: -1263.6386] 80%|████████  | 80033/100000 [00:21&lt;00:05, 3684.49it/s, init loss: 22917835223.3965, avg. loss [75001-80000]: -1886.9540] 80%|████████  | 80439/100000 [00:21&lt;00:05, 3790.41it/s, init loss: 22917835223.3965, avg. loss [75001-80000]: -1886.9540] 81%|████████  | 80819/100000 [00:21&lt;00:05, 3785.98it/s, init loss: 22917835223.3965, avg. loss [75001-80000]: -1886.9540] 81%|████████  | 81214/100000 [00:22&lt;00:04, 3833.71it/s, init loss: 22917835223.3965, avg. loss [75001-80000]: -1886.9540] 82%|████████▏ | 81598/100000 [00:22&lt;00:04, 3807.51it/s, init loss: 22917835223.3965, avg. loss [75001-80000]: -1886.9540] 82%|████████▏ | 81980/100000 [00:22&lt;00:04, 3785.05it/s, init loss: 22917835223.3965, avg. loss [75001-80000]: -1886.9540] 82%|████████▏ | 82359/100000 [00:22&lt;00:04, 3727.71it/s, init loss: 22917835223.3965, avg. loss [75001-80000]: -1886.9540] 83%|████████▎ | 82733/100000 [00:22&lt;00:04, 3701.01it/s, init loss: 22917835223.3965, avg. loss [75001-80000]: -1886.9540] 83%|████████▎ | 83136/100000 [00:22&lt;00:04, 3796.14it/s, init loss: 22917835223.3965, avg. loss [75001-80000]: -1886.9540] 84%|████████▎ | 83524/100000 [00:22&lt;00:04, 3820.38it/s, init loss: 22917835223.3965, avg. loss [75001-80000]: -1886.9540] 84%|████████▍ | 83907/100000 [00:22&lt;00:04, 3749.36it/s, init loss: 22917835223.3965, avg. loss [75001-80000]: -1886.9540] 84%|████████▍ | 84283/100000 [00:22&lt;00:04, 3702.31it/s, init loss: 22917835223.3965, avg. loss [75001-80000]: -1886.9540] 85%|████████▍ | 84654/100000 [00:22&lt;00:04, 3689.99it/s, init loss: 22917835223.3965, avg. loss [75001-80000]: -1886.9540] 85%|████████▌ | 85024/100000 [00:23&lt;00:04, 3640.17it/s, init loss: 22917835223.3965, avg. loss [80001-85000]: -2397.8516] 85%|████████▌ | 85389/100000 [00:23&lt;00:04, 3611.59it/s, init loss: 22917835223.3965, avg. loss [80001-85000]: -2397.8516] 86%|████████▌ | 85782/100000 [00:23&lt;00:03, 3703.75it/s, init loss: 22917835223.3965, avg. loss [80001-85000]: -2397.8516] 86%|████████▌ | 86155/100000 [00:23&lt;00:03, 3709.83it/s, init loss: 22917835223.3965, avg. loss [80001-85000]: -2397.8516] 87%|████████▋ | 86546/100000 [00:23&lt;00:03, 3767.36it/s, init loss: 22917835223.3965, avg. loss [80001-85000]: -2397.8516] 87%|████████▋ | 86956/100000 [00:23&lt;00:03, 3863.95it/s, init loss: 22917835223.3965, avg. loss [80001-85000]: -2397.8516] 87%|████████▋ | 87367/100000 [00:23&lt;00:03, 3935.51it/s, init loss: 22917835223.3965, avg. loss [80001-85000]: -2397.8516] 88%|████████▊ | 87761/100000 [00:23&lt;00:03, 3796.65it/s, init loss: 22917835223.3965, avg. loss [80001-85000]: -2397.8516] 88%|████████▊ | 88162/100000 [00:23&lt;00:03, 3855.99it/s, init loss: 22917835223.3965, avg. loss [80001-85000]: -2397.8516] 89%|████████▊ | 88549/100000 [00:24&lt;00:03, 3760.10it/s, init loss: 22917835223.3965, avg. loss [80001-85000]: -2397.8516] 89%|████████▉ | 88927/100000 [00:24&lt;00:02, 3712.31it/s, init loss: 22917835223.3965, avg. loss [80001-85000]: -2397.8516] 89%|████████▉ | 89299/100000 [00:24&lt;00:02, 3690.97it/s, init loss: 22917835223.3965, avg. loss [80001-85000]: -2397.8516] 90%|████████▉ | 89669/100000 [00:24&lt;00:02, 3660.71it/s, init loss: 22917835223.3965, avg. loss [80001-85000]: -2397.8516] 90%|█████████ | 90036/100000 [00:24&lt;00:02, 3653.23it/s, init loss: 22917835223.3965, avg. loss [85001-90000]: -2841.6718] 90%|█████████ | 90425/100000 [00:24&lt;00:02, 3721.63it/s, init loss: 22917835223.3965, avg. loss [85001-90000]: -2841.6718] 91%|█████████ | 90798/100000 [00:24&lt;00:02, 3709.25it/s, init loss: 22917835223.3965, avg. loss [85001-90000]: -2841.6718] 91%|█████████ | 91170/100000 [00:24&lt;00:02, 3681.07it/s, init loss: 22917835223.3965, avg. loss [85001-90000]: -2841.6718] 92%|█████████▏| 91576/100000 [00:24&lt;00:02, 3792.70it/s, init loss: 22917835223.3965, avg. loss [85001-90000]: -2841.6718] 92%|█████████▏| 91956/100000 [00:24&lt;00:02, 3761.19it/s, init loss: 22917835223.3965, avg. loss [85001-90000]: -2841.6718] 92%|█████████▏| 92333/100000 [00:25&lt;00:02, 3738.34it/s, init loss: 22917835223.3965, avg. loss [85001-90000]: -2841.6718] 93%|█████████▎| 92717/100000 [00:25&lt;00:01, 3767.73it/s, init loss: 22917835223.3965, avg. loss [85001-90000]: -2841.6718] 93%|█████████▎| 93127/100000 [00:25&lt;00:01, 3865.24it/s, init loss: 22917835223.3965, avg. loss [85001-90000]: -2841.6718] 94%|█████████▎| 93514/100000 [00:25&lt;00:01, 3768.25it/s, init loss: 22917835223.3965, avg. loss [85001-90000]: -2841.6718] 94%|█████████▍| 93906/100000 [00:25&lt;00:01, 3812.28it/s, init loss: 22917835223.3965, avg. loss [85001-90000]: -2841.6718] 94%|█████████▍| 94314/100000 [00:25&lt;00:01, 3888.66it/s, init loss: 22917835223.3965, avg. loss [85001-90000]: -2841.6718] 95%|█████████▍| 94704/100000 [00:25&lt;00:01, 3801.17it/s, init loss: 22917835223.3965, avg. loss [85001-90000]: -2841.6718] 95%|█████████▌| 95085/100000 [00:25&lt;00:01, 3787.75it/s, init loss: 22917835223.3965, avg. loss [90001-95000]: -3190.8188] 95%|█████████▌| 95494/100000 [00:25&lt;00:01, 3875.65it/s, init loss: 22917835223.3965, avg. loss [90001-95000]: -3190.8188] 96%|█████████▌| 95883/100000 [00:25&lt;00:01, 3853.78it/s, init loss: 22917835223.3965, avg. loss [90001-95000]: -3190.8188] 96%|█████████▋| 96274/100000 [00:26&lt;00:00, 3869.71it/s, init loss: 22917835223.3965, avg. loss [90001-95000]: -3190.8188] 97%|█████████▋| 96672/100000 [00:26&lt;00:00, 3900.50it/s, init loss: 22917835223.3965, avg. loss [90001-95000]: -3190.8188] 97%|█████████▋| 97063/100000 [00:26&lt;00:00, 3805.26it/s, init loss: 22917835223.3965, avg. loss [90001-95000]: -3190.8188] 97%|█████████▋| 97452/100000 [00:26&lt;00:00, 3828.15it/s, init loss: 22917835223.3965, avg. loss [90001-95000]: -3190.8188] 98%|█████████▊| 97836/100000 [00:26&lt;00:00, 3775.94it/s, init loss: 22917835223.3965, avg. loss [90001-95000]: -3190.8188] 98%|█████████▊| 98247/100000 [00:26&lt;00:00, 3873.51it/s, init loss: 22917835223.3965, avg. loss [90001-95000]: -3190.8188] 99%|█████████▊| 98640/100000 [00:26&lt;00:00, 3889.50it/s, init loss: 22917835223.3965, avg. loss [90001-95000]: -3190.8188] 99%|█████████▉| 99030/100000 [00:26&lt;00:00, 3850.14it/s, init loss: 22917835223.3965, avg. loss [90001-95000]: -3190.8188] 99%|█████████▉| 99418/100000 [00:26&lt;00:00, 3857.17it/s, init loss: 22917835223.3965, avg. loss [90001-95000]: -3190.8188]100%|█████████▉| 99804/100000 [00:26&lt;00:00, 3781.51it/s, init loss: 22917835223.3965, avg. loss [90001-95000]: -3190.8188]100%|██████████| 100000/100000 [00:27&lt;00:00, 3698.59it/s, init loss: 22917835223.3965, avg. loss [95001-100000]: -3381.6672]\n\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=VIInferenceEngine(num_steps=100000,\n                                                optimizer=CosineScheduleAdamOptimizer(),\n                                                progress_bar=True),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=VIInferenceEngine(num_steps=100000,\n                                                optimizer=CosineScheduleAdamOptimizer(),\n                                                progress_bar=True),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineVIInferenceEngineVIInferenceEngine(num_steps=100000, optimizer=CosineScheduleAdamOptimizer(),\n                  progress_bar=True)\n\n\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.75, 0.25])\nfig, ax = plt.subplots(figsize=(8, 5))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=1)\n\n\n\n\n\n\n\n\nWe can then plot the seasonality with credible intervals, using the samples from the posterior distribution of the parameters.\n\nsamples = model.predict_component_samples(fh=forecast_horizon)\n\n\nseasonality_intervals = (\n    samples[\"seasonality\"].unstack(level=0).quantile([0.05, 0.95], axis=1).T\n)\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nax.fill_between(\n    seasonality_intervals.index.to_timestamp(),\n    seasonality_intervals[0.05],\n    seasonality_intervals[0.95],\n    alpha=0.5,\n)\nax.set_title(\"Seasonality with 90% credible interval\")\n\nText(0.5, 1.0, 'Seasonality with 90% credible interval')",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/univariate.html#extra-syntax-sugar",
    "href": "tutorials/univariate.html#extra-syntax-sugar",
    "title": "Basic Univariate Forecasting with Prophetverse",
    "section": "Extra: syntax sugar",
    "text": "Extra: syntax sugar\nIn Prophetverse, we’ve implemented the &gt;&gt; operator, which makes it easier to set trend, exogenous_effects and inference_engine parameters.\n\ntrend = PiecewiseLinearTrend(\n    changepoint_interval=300,\n    changepoint_prior_scale=0.0001,\n    changepoint_range=0.8,\n)\nexogenous_effects = [\n    (\n        \"seasonality\",\n        LinearFourierSeasonality(\n            freq=\"D\",\n            sp_list=[7, 365.25],\n            fourier_terms_list=[3, 10],\n            prior_scale=0.1,\n            effect_mode=\"multiplicative\",\n        ),\n        no_input_columns,\n    ),\n]\ninference_engine = MAPInferenceEngine()\n\nmodel = Prophetverse() &gt;&gt; trend &gt;&gt; exogenous_effects &gt;&gt; inference_engine\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=300,\n                                        changepoint_prior_scale=0.0001))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=300,\n                                        changepoint_prior_scale=0.0001))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=300, changepoint_prior_scale=0.0001)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nforecast_horizon = pd.period_range(\"2007-01-01\", \"2018-01-01\", freq=\"D\")\nfig, ax = plt.subplots(figsize=(8, 5))\npreds = model.predict(fh=forecast_horizon)\npreds.plot.line(ax=ax)\nax.scatter(y.index, y, marker=\"o\", color=\"k\", s=2, alpha=0.5)",
    "crumbs": [
      "Univariate forecasting"
    ]
  },
  {
    "objectID": "tutorials/tuning.html",
    "href": "tutorials/tuning.html",
    "title": "Tuning Prophetverse with sktime",
    "section": "",
    "text": "Prophetverse is compatible with sktime’s tuning framework. You can define a parameter grid for components (such as trend and seasonality) and then use cross-validation tools (e.g., GridSearchCV) to search for the best parameters.",
    "crumbs": [
      "Hyperparameter tuning"
    ]
  },
  {
    "objectID": "tutorials/tuning.html#overview",
    "href": "tutorials/tuning.html#overview",
    "title": "Tuning Prophetverse with sktime",
    "section": "",
    "text": "Prophetverse is compatible with sktime’s tuning framework. You can define a parameter grid for components (such as trend and seasonality) and then use cross-validation tools (e.g., GridSearchCV) to search for the best parameters.",
    "crumbs": [
      "Hyperparameter tuning"
    ]
  },
  {
    "objectID": "tutorials/tuning.html#example-using-gridsearchcv-with-prophetverse",
    "href": "tutorials/tuning.html#example-using-gridsearchcv-with-prophetverse",
    "title": "Tuning Prophetverse with sktime",
    "section": "Example: Using GridSearchCV with Prophetverse",
    "text": "Example: Using GridSearchCV with Prophetverse\n\nImport necessary modules and load your dataset.\nDefine the hyperparameter grid for components (e.g., changepoint_interval and changepoint_prior_scale in the trend).\nCreate a Prophetverse instance with initial settings.\nWrap the model with sktime’s GridSearchCV and run the tuning process.\n\n\nLoading the data\n\nimport pandas as pd\nfrom sktime.forecasting.model_selection import ForecastingGridSearchCV\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.utils import no_input_columns\n\n# Load example dataset (replace with your own data as needed)\nfrom prophetverse.datasets.loaders import load_peyton_manning\n\ny = load_peyton_manning()\ny.head()\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n\n\n\n\n\n\ny\n\n\nds\n\n\n\n\n\n2007-12-10\n9.590761\n\n\n2007-12-11\n8.519590\n\n\n2007-12-12\n8.183677\n\n\n2007-12-13\n8.072467\n\n\n2007-12-14\n7.893572\n\n\n\n\n\n\n\n\n\nSetting the model\nWe create our model instance, before passing it to tuning.\n\n# Create the initial Prophetverse model.\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-250,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                freq=\"D\",\n                sp_list=[7, 365.25],\n                fourier_terms_list=[3, 10],\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n    ],\n    inference_engine=MAPInferenceEngine(),\n)\nmodel\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\n\nDefine the searcher\nIn sktime, the tuner is also an estimator/forecaster, so we can use the same interface as for any other sktime forecaster. We can use GridSearchCV to search for the best parameters in a given parameter grid.\n\n# Set up cv strategy\nfrom sktime.split import ExpandingWindowSplitter\n\ncv = ExpandingWindowSplitter(fh=[1, 2, 3], step_length=1000, initial_window=1000)\n\nparam_grid = {\n    \"trend__changepoint_interval\": [300, 700],\n    \"trend__changepoint_prior_scale\": [0.0001, 0.00001],\n    \"seasonality__prior_scale\": [0.1],\n}\n\n\n# Set up GridSearchCV with 3-fold cross-validation.\ngrid_search = ForecastingGridSearchCV(\n                model,\n                param_grid=param_grid,\n                cv=cv\n            )\ngrid_search\n\nForecastingGridSearchCV(cv=ExpandingWindowSplitter(fh=[1, 2, 3],\n                                                   initial_window=1000,\n                                                   step_length=1000),\n                        forecaster=Prophetverse(exogenous_effects=[('seasonality',\n                                                                    LinearFourierSeasonality(effect_mode='multiplicative',\n                                                                                             fourier_terms_list=[3,\n                                                                                                                 10],\n                                                                                             freq='D',\n                                                                                             prior_scale=0.1,\n                                                                                             sp_list=[7,\n                                                                                                      365.25]),\n                                                                    '^$')],\n                                                inference_engine=MAPInferenceEngine(),\n                                                trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                                                           changepoint_prior_scale=1e-05,\n                                                                           changepoint_range=-250)),\n                        param_grid={'seasonality__prior_scale': [0.1],\n                                    'trend__changepoint_interval': [300, 700],\n                                    'trend__changepoint_prior_scale': [0.0001,\n                                                                       1e-05]})Please rerun this cell to show the HTML repr or trust the notebook.ForecastingGridSearchCV?Documentation for ForecastingGridSearchCVForecastingGridSearchCV(cv=ExpandingWindowSplitter(fh=[1, 2, 3],\n                                                   initial_window=1000,\n                                                   step_length=1000),\n                        forecaster=Prophetverse(exogenous_effects=[('seasonality',\n                                                                    LinearFourierSeasonality(effect_mode='multiplicative',\n                                                                                             fourier_terms_list=[3,\n                                                                                                                 10],\n                                                                                             freq='D',\n                                                                                             prior_scale=0.1,\n                                                                                             sp_list=[7,\n                                                                                                      365.25]),\n                                                                    '^$')],\n                                                inference_engine=MAPInferenceEngine(),\n                                                trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                                                           changepoint_prior_scale=1e-05,\n                                                                           changepoint_range=-250)),\n                        param_grid={'seasonality__prior_scale': [0.1],\n                                    'trend__changepoint_interval': [300, 700],\n                                    'trend__changepoint_prior_scale': [0.0001,\n                                                                       1e-05]})forecaster: ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\nNow, we can call fit.\n\n# Run the grid search.\ngrid_search.fit(y=y, X=None)\n\n# Display the best parameters found.\nprint(\"Best parameters:\", grid_search.best_params_)\n\nBest parameters: {'seasonality__prior_scale': 0.1, 'trend__changepoint_interval': 700, 'trend__changepoint_prior_scale': 1e-05}\n\n\nWe can also see the performance of each parameter combination:\n\ngrid_search.cv_results_\n\n\n\n\n\n\n\n\nmean_test_MeanAbsolutePercentageError\nmean_fit_time\nmean_pred_time\nparams\nrank_test_MeanAbsolutePercentageError\n\n\n\n\n0\n0.053915\n17.953892\n1.331865\n{'seasonality__prior_scale': 0.1, 'trend__chan...\n4.0\n\n\n1\n0.049250\n16.760212\n1.060858\n{'seasonality__prior_scale': 0.1, 'trend__chan...\n3.0\n\n\n2\n0.036162\n15.030222\n1.130503\n{'seasonality__prior_scale': 0.1, 'trend__chan...\n2.0\n\n\n3\n0.033485\n21.806507\n0.753069\n{'seasonality__prior_scale': 0.1, 'trend__chan...\n1.0\n\n\n\n\n\n\n\nOptionally, extract the best model from the grid search results.\n\nbest_model = grid_search.best_forecaster_\nbest_model\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=700,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              10],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=700,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-250))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=700, changepoint_prior_scale=1e-05,\n                     changepoint_range=-250)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 10], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMAPInferenceEngineMAPInferenceEngine()",
    "crumbs": [
      "Hyperparameter tuning"
    ]
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "In this section, you can find a collection of tutorials that will help you understand the basics of Prophetverse for forecasting."
  },
  {
    "objectID": "howto/composite_effects.html",
    "href": "howto/composite_effects.html",
    "title": "Composition of effects",
    "section": "",
    "text": "In previous examples, we saw how to create a simple custom effect, which applies a simple transformation to the input data. However, the effect’s interface allows us to apply more complex transformations, such as using the output of previous components as input for the current component, or creating a composite effect that wraps an effect and applies some sort of transformation. This example will cover these topics.",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#creating-a-custom-effect",
    "href": "howto/composite_effects.html#creating-a-custom-effect",
    "title": "Composition of effects",
    "section": "Creating a custom effect",
    "text": "Creating a custom effect\nThe idea here is to create an effect that uses another predicted component to scale the impact of an exogenous variable.\nOne classic use-case for this would be using seasonality to scale the effect of investment, that might be proportional to it. Marketing investments are a good example of this. We will implement such a composite effect in this section.\n\nExample dataset\nThe dataset we use is synthetic, and the relation between the exogenous variable and the target is known. However, let’s pretend we don’t know this relation, and analize the data to find some insights that motivate the creation of a custom effect. The dataset has a target variable, which is a time series, and an exogenous variable, which is the investment made for each date.\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom matplotlib import pyplot as plt\nfrom sktime.split import temporal_train_test_split\nfrom sktime.utils.plotting import plot_series\n\nfrom prophetverse.datasets.synthetic import load_composite_effect_example\n\nnumpyro.enable_x64()\n\ny, X = load_composite_effect_example()\n\ny_train, y_test, X_train, X_test = temporal_train_test_split(y, X, test_size=365)\n\ndisplay(y_train.head())\ndisplay(X_train.head())\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n\n\n\n\n\n\ntarget\n\n\ntime\n\n\n\n\n\n2010-01-01\n29.375431\n\n\n2010-01-02\n30.268786\n\n\n2010-01-03\n29.128912\n\n\n2010-01-04\n31.014165\n\n\n2010-01-05\n31.890928\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninvestment\n\n\ntime\n\n\n\n\n\n2010-01-01\n0.198274\n\n\n2010-01-02\n0.198274\n\n\n2010-01-03\n0.198274\n\n\n2010-01-04\n0.198274\n\n\n2010-01-05\n0.207695\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8,5))\nplot_series(y_train, y_test, labels=[\"Train\", \"Test\"], title=\"Target series\", ax=ax)\nfig.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_series(X[\"investment\"], labels=[\"investment\"], title=\"Features\", ax=ax)\nfig.show()\n\n\n\n\n\n\n\n\nThe timeseries has a yearly seasonality, and it seems that some oscillations are proportional to the investment. Below, we model the timeseries with a simple linear effect between the investment and the target, and a yearly seasonality based on fourier terms. Then, we will analize the residuals to see if there is any pattern that we can capture with a custom effect.\n\nfrom prophetverse.effects import LinearEffect\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.engine.optimizer import LBFGSSolver\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import exact, no_input_columns\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(\n        changepoint_interval=500,\n        changepoint_prior_scale=0.00001,\n        changepoint_range=-500,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                freq=\"D\",\n                sp_list=[365.25],\n                fourier_terms_list=[5],\n                prior_scale=1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n        (\n            \"investment\",\n            LinearEffect(\"multiplicative\", prior=dist.Normal(0, 1)),\n            exact(\"investment\"),\n        ),\n    ],\n    inference_engine=MAPInferenceEngine(\n        optimizer=LBFGSSolver(memory_size=100, max_linesearch_steps=100),\n        progress_bar=True,\n    ),\n)\n\nmodel.fit(y=y_train, X=X_train)\nmodel\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]100%|██████████| 1/1 [00:07&lt;00:00,  7.76s/it, init loss: -5352.1071, avg. loss [1-1]: -5352.1071]100%|██████████| 1/1 [00:07&lt;00:00,  7.76s/it, init loss: -5352.1071, avg. loss [1-1]: -5352.1071]\n\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=1,\n                                                          sp_list=[365.25]),\n                                 '^$'),\n                                ('investment',\n                                 LinearEffect(prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f233e2b9fd0 with batch shape () and event shape ()&gt;),\n                                 '^investment$')],\n             inference_engine=MAPInferenceEngine(optimizer=LBFGSSolver(max_linesearch_steps=100,\n                                                                       memory_size=100),\n                                                 progress_bar=True),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=1,\n                                                          sp_list=[365.25]),\n                                 '^$'),\n                                ('investment',\n                                 LinearEffect(prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f233e2b9fd0 with batch shape () and event shape ()&gt;),\n                                 '^investment$')],\n             inference_engine=MAPInferenceEngine(optimizer=LBFGSSolver(max_linesearch_steps=100,\n                                                                       memory_size=100),\n                                                 progress_bar=True),\n             trend=PiecewiseLinearTrend(changepoint_interval=500,\n                                        changepoint_prior_scale=1e-05,\n                                        changepoint_range=-500))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=500, changepoint_prior_scale=1e-05,\n                     changepoint_range=-500)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=1, sp_list=[365.25])LinearEffectLinearEffect(prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f233e2b9fd0 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine(optimizer=LBFGSSolver(max_linesearch_steps=100,\n                                         memory_size=100),\n                   progress_bar=True)\n\n\nWe plot the predictions on training set to see if the model performs well.\n\ny_pred = model.predict(X=X_train, fh=y_train.index)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_series(y_train, y_pred, labels=[\"Train\", \"Pred\"], title=\"Target series\", ax=ax)\nfig.show()\n\n\n\n\n\n\n\n\nWe can see that some peaks are not captured by the model. Our hypothesis to explain this phenomenon is that the investment has more impact on the target when it is done during the positive seasonality periods. To test this, we plot the residuals of the model against the investment, and color the points based on the seasonality component. We can see that slopes are different for positive and negative seasonality, which indicates that our hypothesis is possibly correct.\n\ncomponents = model.predict_components(X=X_train, fh=y_train.index)\n\nresidual = y_train[\"target\"] - components[\"mean\"]\n\nfig, ax = plt.subplots()\nax.scatter(\n    X_train[\"investment\"],\n    residual,\n    c=components[\"seasonality\"] &lt; 0,\n    cmap=\"Accent\",\n    alpha=0.9,\n)\n# Create legend manually\ncolors = plt.cm.get_cmap(\"Accent\").colors\nax.scatter([], [], color=colors[0], label=\"Positive seasonality\")\nax.scatter([], [], color=colors[1], label=\"Negative seasonality\")\nax.legend()\nax.set(xlabel=\"Investment\", ylabel=\"Residual\", title=\"Residuals vs Investment\")\nfig.show()\n\n/tmp/ipykernel_3572/182972736.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  colors = plt.cm.get_cmap(\"Accent\").colors",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#creating-the-composite-effect",
    "href": "howto/composite_effects.html#creating-the-composite-effect",
    "title": "Composition of effects",
    "section": "Creating the composite effect",
    "text": "Creating the composite effect\nTo model this behaviour with Prophetverse, we will create a custom effect, that scales a new effect by the output of a previous component. The _fit and _transform methods call the inner effect’s methods, and the predict method multiplies the inner effect’s predictions by the seasonality, which is passed as base_effect_name.\n\nfrom typing import Any, Dict, List\n\nimport jax.numpy as jnp\nimport pandas as pd\n\nfrom prophetverse.effects.base import BaseEffect\n\n\nclass WrapEffectAndScaleByAnother(BaseEffect):\n    \"\"\"Wrap an effect and scale it by another effect.\n\n    Parameters\n    ----------\n    effect : BaseEffect\n        The effect to wrap.\n\n    \"\"\"\n\n    _tags = {\"requires_X\": False, \"hierarchical_prophet_compliant\": False}\n\n    def __init__(\n        self,\n        effect: BaseEffect,\n        base_effect_name: str,\n    ):\n\n        self.effect = effect\n        self.base_effect_name = base_effect_name\n\n        super().__init__()\n\n        self.clone_tags(effect)\n\n    def _fit(self, y: pd.DataFrame, X: pd.DataFrame, scale: float = 1):\n        \"\"\"Initialize the effect.\"\"\"\n        self.effect.fit(X=X, y=y, scale=scale)\n\n    def _transform(self, X: pd.DataFrame, fh: pd.Index) -&gt; Dict[str, Any]:\n        \"\"\"Prepare input data to be passed to numpyro model.\"\"\"\n        return self.effect.transform(X=X, fh=fh)\n\n    def _predict(\n        self, data: Dict, predicted_effects: Dict[str, jnp.ndarray], *args, **kwargs\n    ) -&gt; jnp.ndarray:\n        \"\"\"Apply and return the effect values.\"\"\"\n        out = self.effect.predict(data=data, predicted_effects=predicted_effects)\n\n        base_effect = predicted_effects[self.base_effect_name]\n        return base_effect * out",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#instantiating-the-model-with-the-composite-effect",
    "href": "howto/composite_effects.html#instantiating-the-model-with-the-composite-effect",
    "title": "Composition of effects",
    "section": "Instantiating the model with the composite effect",
    "text": "Instantiating the model with the composite effect\nTo create the model, we use the model instance we have, and the rshift operator to append the composite effect to the model.\n\nimport numpyro.distributions as dist\nfrom prophetverse.engine.optimizer import AdamOptimizer\n\ncomposite_effect_tuple = (\n    \"investment_seasonality\",  # The effect ID, can be what you want\n    WrapEffectAndScaleByAnother(\n        effect=LinearEffect(\"multiplicative\", prior=dist.HalfNormal(1)),\n        base_effect_name=\"seasonality\",\n    ),\n    exact(\"investment\"),\n)\n\n\n# We use the rshift operator to append an effect to the model\nmodel_composite = model &gt;&gt; composite_effect_tuple\n\nmodel_composite.fit(y=y_train, X=X_train)\ny_pred_composite = model_composite.predict(X=X_train, fh=y_train.index)\n\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:252: UserWarning: Columns {'investment'} are already set\n  self._fit_effects(X, y)\n  0%|          | 0/1 [00:00&lt;?, ?it/s]100%|██████████| 1/1 [00:08&lt;00:00,  8.31s/it, init loss: -6054.6937, avg. loss [1-1]: -6054.6937]100%|██████████| 1/1 [00:08&lt;00:00,  8.31s/it, init loss: -6054.6937, avg. loss [1-1]: -6054.6937]\n\n\nWe can see below how these oscilations are captured by the model correctly when adding this joint effect.\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_series(y_train, y_pred_composite, labels=[\"Train\", \"Pred\"], title=\"Target series\",ax=ax)\nfig.show()",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#evaluating-the-model-on-test-set",
    "href": "howto/composite_effects.html#evaluating-the-model-on-test-set",
    "title": "Composition of effects",
    "section": "Evaluating the model on test set",
    "text": "Evaluating the model on test set\nWe compare to the previous model to see if the new effect improved the predictions on test set:\n\ny_pred_composite = model_composite.predict(X=X_test, fh=y_test.index)\ny_pred = model.predict(X=X_test, fh=y_test.index)\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_series(\n    y_test,\n    y_pred,\n    y_pred_composite,\n    labels=[\"Test\", \"Pred\", \"Pred composite\"],\n    title=\"Target series\",\n    ax=ax,\n)\n\nplt.show()",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/composite_effects.html#extracting-the-components",
    "href": "howto/composite_effects.html#extracting-the-components",
    "title": "Composition of effects",
    "section": "Extracting the components",
    "text": "Extracting the components\nThe components can be extracted as usual, with the predict_components method.\n\ncomponents = model_composite.predict_components(fh=y_test.index, X=X_test)\n\nfig, ax = plt.subplots(figsize=(10, 5))\ncomponents.plot.line(ax=ax)\nax.set_title(\"Predicted Components\")\nfig.show()",
    "crumbs": [
      "Creating composite effects"
    ]
  },
  {
    "objectID": "howto/custom_trend.html",
    "href": "howto/custom_trend.html",
    "title": "Custom Trend in Prophetverse",
    "section": "",
    "text": "Diffusion of innovations is a theory that seeks to explain how, why, and at what rate new ideas and technology spread through cultures. This theory was formulated by E.M. Rogers in 1962 and is often used to understand the adoption or spread of new products and technologies among different groups of people.\nAn innovation is something new or significantly improved. This can include products, ideas, or practices that are perceived as new by an individual or other unit of adoption. Diffusion refers to the process by which an innovation is communicated over time among the participants in a social system.\nThe diffusion of innovations theory applies to a variety of new ideas. Here are a few examples:",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#use-case-forecasting-product-adoption",
    "href": "howto/custom_trend.html#use-case-forecasting-product-adoption",
    "title": "Custom Trend in Prophetverse",
    "section": "",
    "text": "Diffusion of innovations is a theory that seeks to explain how, why, and at what rate new ideas and technology spread through cultures. This theory was formulated by E.M. Rogers in 1962 and is often used to understand the adoption or spread of new products and technologies among different groups of people.\nAn innovation is something new or significantly improved. This can include products, ideas, or practices that are perceived as new by an individual or other unit of adoption. Diffusion refers to the process by which an innovation is communicated over time among the participants in a social system.\nThe diffusion of innovations theory applies to a variety of new ideas. Here are a few examples:",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#examples-of-processes-following-diffusion-of-innovations",
    "href": "howto/custom_trend.html#examples-of-processes-following-diffusion-of-innovations",
    "title": "Custom Trend in Prophetverse",
    "section": "Examples of Processes Following Diffusion of Innovations",
    "text": "Examples of Processes Following Diffusion of Innovations\n\nNumber of new unique users: The number of new unique users of a product or service can be modeled using the diffusion of innovations theory. This can help businesses forecast their growth and plan for future expansion.\nTechnology Adoption: Perhaps the most common application of the theory, technology adoption refers to how new gadgets, software, or platforms spread among users. For instance, the adoption of smartphones followed this diffusion process, starting with innovators and tech enthusiasts before reaching the broader public.\nHealthcare Practices: New medical practices, treatments, or health campaigns spread among medical professionals and the public using the diffusion framework. An example could be the adoption of telemedicine, which has seen increased acceptance over recent years.\nSustainable Practices: The adoption of renewable energy sources like solar panels or wind turbines often follows the diffusion of innovations model. Innovators begin by testing and using these technologies, which gradually become more mainstream as their advantages and efficiencies are recognized.\nAgricultural Techniques: New farming technologies or methods, such as hydroponics or genetically modified crops, also spread through agricultural communities by following the principles of diffusion of innovations.",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#the-bell-shaped-curve",
    "href": "howto/custom_trend.html#the-bell-shaped-curve",
    "title": "Custom Trend in Prophetverse",
    "section": "The Bell-Shaped Curve",
    "text": "The Bell-Shaped Curve\nThe diffusion of innovations can be visualized using a bell-shaped curve, often called the “diffusion curve.” This curve is crucial for understanding the rate at which new ideas and technologies are adopted in a society. Here’s how it aligns with the categories of adopters:\n\nInnovators make up the first small section on the left of the curve. These are the first few who adopt the innovation.\nEarly Adopters follow next and represent a slightly larger segment as the curve starts to ascend.\nEarly Majority forms the first large segment of the curve, where it reaches and crosses the mean. Adoption is becoming more common and widespread here.\nLate Majority comes next, at the point where the curve starts to descend. This group adopts just as the new idea or technology begins to feel outdated.\nLaggards are the last segment, where the curve tails off. Adoption within this group occurs very slowly and often only when necessary.\n\nThe bell-shaped curve reflects the cumulative adoption of innovations over time, demonstrating that the speed of adoption typically starts slow, accelerates until it reaches the majority of the potential market, and then slows down as fewer non-adopters remain.\nThis curve is central to strategic decisions in marketing, product development, and policy-making, helping stakeholders identify when and how to best introduce new ideas or technologies to different segments of society.",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#the-dataset",
    "href": "howto/custom_trend.html#the-dataset",
    "title": "Custom Trend in Prophetverse",
    "section": "The dataset",
    "text": "The dataset\nAs a proxy for diffusion of innovations, we will use the number of stars received by Tensorflow Repository over time. Although this is not a perfect measure of adoption, it can give us an idea of how the popularity of the repository has grown since its inception.\nThis repository had an initial explosion of stars during the first ~10 days, which we will ignore since the daily granularity is not enough to capture the initial growth (hourly might work). After that, the number of starts grew by following a bell-shaped curve, which we will try to model. This curve might be related to the popularity of deep learning itself.\n\n\n\n\n\n\nNote\n\n\n\nThis dataset was obtained from https://github.com/emanuelef/daily-stars-explorer\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom prophetverse.datasets.loaders import load_tensorflow_github_stars\n\ny = load_tensorflow_github_stars()\n\nfig, ax = plt.subplots()\n# First 30 days\ny.iloc[:30].plot.line(ax=ax)\ny.iloc[:30].cumsum()[\"day-stars\"].rename(\"Cumulative sum\").plot.line(ax=ax, legend=True)\nax.set_title(\"First 30 days\")\nfig.show()\n\nfig, axs = plt.subplots(nrows=2, sharex=True)\ny.iloc[30:].plot.line(ax=axs[0])\ny.iloc[30:].cumsum()[\"day-stars\"].rename(\"Cumulative sum\").plot.line(ax=axs[1])\n# FIgure title\nfig.suptitle(\"After the first 30 days\")\nfig.show()\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#modeling-the-diffusion-of-innovations",
    "href": "howto/custom_trend.html#modeling-the-diffusion-of-innovations",
    "title": "Custom Trend in Prophetverse",
    "section": "Modeling the Diffusion of Innovations",
    "text": "Modeling the Diffusion of Innovations\nTo model this behaviour with Prophetverse, we will use the custom trend feature.\nWe will define a trend model class that implements the generalized logistic curve, which accepts assymetric curves. We will also add another premise: a varying capacity, which will allow us to model a linear growth of the total addressable market (TAM) over time. Let \\(G(t)\\) be the logistic curve defining the acumulated number of stars at time \\(t\\):\n\\[\n\\begin{align*}\nG(t) &= \\frac{C_1(t-t_0) + C_2}{\\left(1 + \\exp(-\\alpha v (t - t_0))\\right)^{\\frac{1}{v}}} \\\n\\text{where} & \\\\\nC_2 \\in \\mathbb{R}_+ &= \\text{is the constant capacity term} \\\\\nC_1 \\in \\mathbb{R}_+ &= \\text{is the linear increasing rate of the capacity} \\\\\nt_0 \\in \\mathbb{R} &= \\text{is the time offset term} \\\\\nv \\in \\mathbb{R}_+ &= \\text{determines the shape of the curve} \\\\\n\\alpha \\in \\mathbb{R} &= \\text{is the rate}\n\\end{align*}\n\\]\nIts derivative is:\n\\[\n\\begin{align*}\ng(t) &= \\alpha\\left(1 - \\frac{G(T)}{C_1(t-t_0) + C_2}\\right) G(T)  + \\frac{C_1}{C_1(t-t_0) + C_2}G(T)\n\\end{align*}\n\\]\nThat curve can be used as trend to model a diffusion process. Below, we plot it for a combination of parameters\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef g(t, C1, C2, t0, v, alpha):\n    return (C1 * (t - t0) + C2) / ((1 + np.exp(-alpha * v * (t - t0))) ** (1 / v))\n\n\ndef normalized_generalized_logistic(x, A, v, t0):\n    return 1 / (1 + np.exp(-A * v * (x - t0))) ** (1 / v)\n\n\n# Define the generalized logistic function\ndef generalized_logistic(x, C1, C2, alpha, v, t0):\n    return (C1 * x + C2) * normalized_generalized_logistic(x, alpha, v, t0)\n\n\ndef dgeneralized_logistic(x, C1, C2, alpha, v, t0):\n    return alpha * (\n        1 - (generalized_logistic(x, C1, C2, alpha, v, t0) / (C1 * x + C2)) ** v\n    ) * generalized_logistic(\n        x, C1, C2, alpha, v, t0\n    ) + C1 * normalized_generalized_logistic(\n        x, alpha, v, t0\n    )\n\n\nC1 = 1\nC2 = 50\nt0 = -2\nv = 1.2\nalpha = 0.5\nlabel = (f\"C1={C1:.1f}, C2={C2:.1f}, t0={t0:.1f}, v={v:.1f}, alpha={alpha:.1f}\",)\n\nt = np.linspace(-10, 10, 1000)\ngt = dgeneralized_logistic(t, C1=C1, C2=C2, t0=t0, v=v, alpha=alpha)\n\nfig, axs = plt.subplots(figsize=(12, 6), nrows=2, sharex=True)\naxs[0].plot(\n    t,\n    gt,\n    label=label,\n)\naxs[0].set_title(\"Visualization of g(t)\")\naxs[0].set_xlabel(\"t\")\naxs[0].set_ylabel(\"g(t)\")\n\nGt = generalized_logistic(t, C1=C1, C2=C2, t0=t0, v=v, alpha=alpha)\naxs[1].plot(\n    t,\n    Gt,\n    label=label,\n)\naxs[1].set_title(\"Visualization of G(t)\")\naxs[1].set_xlabel(\"t\")\naxs[1].set_ylabel(\"g(t)\")\n\n# axs[1].grid(True, alpha=0.3)\nfig.show()\n\n\n\n\n\n\n\n\nThat curve has the bell-shape and the flexiblity to not be symmetric depending on the parameters. Furthermore, it tends to a constant value (\\(C1\\)) as time goes to infinity, which represent our knowledge that the size of the “market” of tensorflow/neural networks users starts at a value and grows with time.",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#splitting-the-dataset",
    "href": "howto/custom_trend.html#splitting-the-dataset",
    "title": "Custom Trend in Prophetverse",
    "section": "Splitting the dataset",
    "text": "Splitting the dataset\nWe leave 7 years to forecast, and 1.5 year to train. Note that, without the prior information on the nature of the curve, a model could simply forecast a linear growth of the number of stars, which would be a very poor forecast.\n\nsplit_at = -int(365 * 5)\ny = y.iloc[20:]\ny_train, y_test = y.iloc[:split_at], y.iloc[split_at:]\n\n\nfig, axs = plt.subplots(nrows=2, sharex=True, figsize=(12, 6))\nax = axs[0]\nax = y_train[\"day-stars\"].rename(\"Train\").plot.line(legend=True, ax=ax)\ny_test[\"day-stars\"].rename(\"Test\").plot.line(ax=ax, alpha=0.2, legend=True)\nax.axvline(y_train.index[-1], color=\"red\", linestyle=\"--\", alpha=0.5, zorder=-1)\nax.set_title(\"Daily new stars\", loc=\"left\")\nax.set_xlabel(\"\")\nax.spines[\"left\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.spines[\"top\"].set_visible(False)\nax.set_xlim(y.index.min(), y.index.max())\nax.set_ylim(0, 300)\n\nax = axs[1]\nax = y_train[\"day-stars\"].rename(\"Train\").cumsum().plot.line(legend=True)\n(\n    y_test[\"day-stars\"].rename(\"Test\").cumsum()\n    + y_train[\"day-stars\"].rename(\"Train\").cumsum().max()\n).plot.line(ax=ax, alpha=0.2, legend=True)\nax.axvline(y_train.index[-1], color=\"red\", linestyle=\"--\", alpha=0.5, zorder=-1)\nax.set_title(\"Total stars\", loc=\"left\")\nax.set_xlabel(\"\")\nax.spines[\"left\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n# Superior\nax.spines[\"top\"].set_visible(False)\nax.set_xlim(y.index.min(), y.index.max())\n\nfig.suptitle(\"Tensorflow Stars\")\n\nText(0.5, 0.98, 'Tensorflow Stars')",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#creating-the-custom-trend",
    "href": "howto/custom_trend.html#creating-the-custom-trend",
    "title": "Custom Trend in Prophetverse",
    "section": "Creating the custom trend",
    "text": "Creating the custom trend\nTo create a custom trend model for use in the Prophetverse library, users can extend the TrendModel abstract base class and implement the required abstract methods. Here’s a step-by-step guide to create a custom trend model, using the GenLogisticTrend class as an example.\n\nStep 1: Define helper functions\nThe GenLogisticTrend class will use the following helper functions:\n\nimport jax\nimport jax.numpy as jnp\n\n\n@jax.jit\ndef normalized_generalized_logistic(x, A, v, t0):\n    return 1 / (1 + jnp.exp(-A * v * (x - t0))) ** (1 / v)\n\n\n@jax.jit\ndef dnormalized_generalized_logistic(x, A, v, t0):\n    return (\n        A\n        * (1 - normalized_generalized_logistic(x, A, v, t0) ** v)\n        * normalized_generalized_logistic(x, A, v, t0)\n    )\n\n\n# Define the generalized logistic function\n\n\n@jax.jit\ndef dgeneralized_logistic(x, K1, K2, A, v, M):\n    return dnormalized_generalized_logistic(x, A, v, M) * (\n        K1 * x + K2\n    ) + K1 * normalized_generalized_logistic(x, A, v, M)\n\n\n\nStep 2: Define the Custom Trend Model Class\nCreate a new class that extends the TrendModel abstract base class. Implement the abstract methods initialize, prepare_input_data, and compute_trend.\n\nimport numpyro\nfrom numpyro import distributions as dist\nimport pandas as pd\nfrom typing import Dict # Added for type hint Dict[str, jnp.ndarray]\n\nfrom prophetverse.distributions import GammaReparametrized\nfrom prophetverse.effects import BaseEffect\nfrom prophetverse.effects.trend import TrendEffectMixin\nfrom prophetverse.utils.frame_to_array import convert_index_to_days_since_epoch\n\n\nclass GenLogisticTrend(TrendEffectMixin, BaseEffect):\n    \"\"\"\n    Custom trend model based on the Generalized Logistic function.\n    \"\"\"\n\n    def __init__(\n        self,\n        logistic_capacity_dist=dist.HalfNormal(10),\n        logistic_capacity2_dist=dist.HalfNormal(50_000),\n        shape_dist=dist.Gamma(1, 1),\n        logistic_rate_dist=GammaReparametrized(0.01, 0.01),\n        offset_prior=dist.Normal(0, 365 * 2),\n    ):\n\n        self.logistic_capacity_dist = logistic_capacity_dist\n        self.logistic_capacity2_dist = logistic_capacity2_dist\n        self.shape_dist = shape_dist\n        self.logistic_rate_dist = logistic_rate_dist\n        self.offset_prior = offset_prior\n\n        super().__init__()\n\n    def _fit(self, y: pd.DataFrame, X: pd.DataFrame, scale: float = 1):\n        \"\"\"Initialize the effect.\n\n        Set the prior location for the trend.\n\n        Parameters\n        ----------\n        y : pd.DataFrame\n            The timeseries dataframe\n\n        X : pd.DataFrame\n            The DataFrame to initialize the effect.\n\n        scale : float, optional\n            The scale of the timeseries. For multivariate timeseries, this is\n            a dataframe. For univariate, it is a simple float.\n        \"\"\"\n        t = convert_index_to_days_since_epoch(y.index)\n        self.t_min_ = t.min()\n        t = t - self.t_min_\n\n    def _transform(self, X: pd.DataFrame, fh: pd.PeriodIndex) -&gt; dict:\n        \"\"\"\n        Prepare the input data for the piecewise trend model.\n\n        Parameters\n        ----------\n        X: pd.DataFrame\n            The exogenous variables DataFrame.\n        fh: pd.PeriodIndex\n            The forecasting horizon as a pandas Index.\n\n        Returns\n        -------\n        jnp.ndarray\n            An array containing the prepared input data.\n        \"\"\"\n        t = convert_index_to_days_since_epoch(fh)\n        t = t - self.t_min_\n        self.offset_prior_loc = len(fh)\n        return t\n\n    def _predict(self, data, previous_effects: Dict[str, jnp.ndarray], params):\n        \"\"\"\n        Compute the trend based on the Generalized Logistic function.\n\n        Parameters\n        ----------\n        data: jnp.ndarray\n            The changepoint matrix.\n        predicted_effects: Dict[str, jnp.ndarray]\n            Dictionary of previously computed effects. For the trend, it is an empty\n            dict.\n\n        Returns\n        -------\n        jnp.ndarray\n            The computed trend.\n        \"\"\"\n        # Alias for clarity\n        time = data\n\n        logistic_rate = numpyro.sample(\"logistic_rate\", self.logistic_rate_dist)\n\n        logistic_capacity1 = numpyro.sample(\n            \"logistic_capacity\",\n            self.logistic_capacity_dist,\n        )\n\n        logistic_capacity2 = numpyro.sample(\n            \"logistic_capacity2\",\n            self.logistic_capacity2_dist,\n        )\n\n        shape = numpyro.sample(\"logistic_shape\", self.shape_dist)\n\n        offset = numpyro.sample(\"offset\", self.offset_prior)\n\n        trend = dgeneralized_logistic(\n            time,\n            K1=logistic_capacity1,\n            K2=logistic_capacity2,\n            A=logistic_rate,\n            v=shape,\n            M=offset,\n        )\n\n        numpyro.deterministic(\"__trend\", trend)\n\n        numpyro.deterministic(\n            \"capacity\", logistic_capacity1 * (time - offset) + logistic_capacity2\n        )\n\n        return trend.reshape((-1, 1))",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#fit-the-model-and-make-predictions",
    "href": "howto/custom_trend.html#fit-the-model-and-make-predictions",
    "title": "Custom Trend in Prophetverse",
    "section": "Fit the model and make predictions",
    "text": "Fit the model and make predictions\n\nimport numpyro\nfrom sktime.transformations.series.fourier import FourierFeatures\n\nfrom prophetverse.effects import LinearFourierSeasonality\nfrom prophetverse.effects.linear import LinearEffect\nfrom prophetverse.engine import MCMCInferenceEngine\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import no_input_columns, starts_with\n\nnumpyro.enable_x64()\n\nmodel = Prophetverse(\n    likelihood=\"negbinomial\",\n    trend=GenLogisticTrend(),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                sp_list=[7, 365.25],\n                fourier_terms_list=[3, 8],\n                freq=\"D\",\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        ),\n    ],\n    inference_engine=MCMCInferenceEngine(\n        num_samples=500,\n        num_warmup=1000,\n    ),\n    # Avoid normalization of the timeseries by setting\n    # scale=1\n    scale=1,\n    noise_scale=10,\n)\n\nnumpyro.enable_x64()\n\nmodel.fit(y_train)\n\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:157: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:157: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:157: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              8],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_samples=500,\n                                                  num_warmup=1000),\n             likelihood='negbinomial', noise_scale=10, scale=1,\n             trend=GenLogisticTrend())Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3,\n                                                                              8],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[7, 365.25]),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_samples=500,\n                                                  num_warmup=1000),\n             likelihood='negbinomial', noise_scale=10, scale=1,\n             trend=GenLogisticTrend())effectsGenLogisticTrendGenLogisticTrend()LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[3, 8], freq='D', prior_scale=0.1,\n                         sp_list=[7, 365.25])inference_engineMCMCInferenceEngineMCMCInferenceEngine(num_samples=500, num_warmup=1000)\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfh = pd.period_range(y.index.min(), \"2026-01-01\")\npreds = model.predict(fh=fh)\ninterval = model.predict_interval(\n    fh=fh,\n    coverage=0.9,\n)\ndisplay(preds.head())\n\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:157: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:157: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n\n\n\n\n\n\n\n\n\nday-stars\n\n\n\n\n2015-11-27\n53.200160\n\n\n2015-11-28\n38.804817\n\n\n2015-11-29\n38.464001\n\n\n2015-11-30\n52.300194\n\n\n2015-12-01\n54.286530\n\n\n\n\n\n\n\n\ninterval.columns = interval.columns.droplevel([0, 1])\ninterval.head()\n\n\n\n\n\n\n\n\nlower\nupper\n\n\n\n\n2015-11-27\n30.95\n83.00\n\n\n2015-11-28\n20.00\n62.05\n\n\n2015-11-29\n21.00\n58.00\n\n\n2015-11-30\n31.95\n80.05\n\n\n2015-12-01\n31.00\n82.00",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/custom_trend.html#plotting-the-results",
    "href": "howto/custom_trend.html#plotting-the-results",
    "title": "Custom Trend in Prophetverse",
    "section": "Plotting the results",
    "text": "Plotting the results\n\n# Just the scatter of y, without lines\nfig, ax = plt.subplots(figsize=(12, 5))\nax = (\n    y[\"day-stars\"]\n    .rename(\"Observed\")\n    .plot.line(\n        marker=\"o\", linestyle=\"None\", legend=False, markersize=1, color=\"black\", ax=ax\n    )\n)\nax.axvline(y_train.index.max(), color=\"black\", zorder=-1, alpha=0.4, linewidth=1)\nax.spines[\"left\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n# Superior\nax.spines[\"top\"].set_visible(False)\n\npreds[\"day-stars\"].rename(\"Forecast\").plot.line(\n    ax=ax, alpha=1, linewidth=0.5, legend=False\n)\n\nax.fill_between(\n    fh.to_timestamp(),\n    interval[\"lower\"],\n    interval[\"upper\"],\n    color=\"blue\",\n    alpha=0.2,\n    zorder=-1,\n    label=\"90% Credible Interval\",\n)\nfig.legend()\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n\n\n\n\n# Forecast samples\nyhat_samples = model.predict_samples(fh=fh)\n# Samples of all sites (capacity, for example, that we had set as deterministic with numpyro.deterministic)\nsite_samples = model.predict_component_samples(fh=fh)\n\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:157: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n/home/runner/work/prophetverse/prophetverse/src/prophetverse/sktime/univariate.py:157: FutureWarning: Warning: 'noise_scale' is deprecated and will be removed in version 0.7.0. Please update your code to avoid issues. Use the noise_scale parameter in the likelihood instead. You can import the likelihood from prophetverse.effects import NormalTargetLikelihood\n  deprecation_warning(\n\n\n\nimport pandas as pd\n# Set number of columns to display to 4 temporarily\npd.set_option(\"display.max_columns\", 4)\nyhat_samples.head()\n\n\n\n\n\n\n\n\n0\n1\n...\n498\n499\n\n\n\n\n2015-11-27\n63\n75\n...\n54\n54\n\n\n2015-11-28\n33\n50\n...\n42\n49\n\n\n2015-11-29\n26\n34\n...\n33\n38\n\n\n2015-11-30\n48\n31\n...\n41\n47\n\n\n2015-12-01\n36\n47\n...\n50\n48\n\n\n\n\n5 rows × 500 columns\n\n\n\n\n# Get quantile 0.25, 0.75 and mean\nimport functools\nimport numpy as np\n\n\ndef q25(x):\n    return np.quantile(x, q=0.25)\n\n\ndef q75(x):\n    return np.quantile(x, q=0.75)\n\n\nsite_quantiles = site_samples.groupby(level=[-1]).agg(\n    [\n        np.mean,\n        q25,\n        q75,\n    ]\n)\nsite_quantiles.head()\n\n/tmp/ipykernel_3500/2904010725.py:14: FutureWarning: The provided callable &lt;function mean at 0x7ff49ca6b4c0&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  site_quantiles = site_samples.groupby(level=[-1]).agg(\n\n\n\n\n\n\n\n\n\nmean\n...\ntrend/capacity\n\n\n\nmean\nq25\n...\nq25\nq75\n\n\n\n\n2015-11-27\n53.200160\n51.627290\n...\n41566.408975\n81936.346465\n\n\n2015-11-28\n38.804817\n37.469424\n...\n41608.955758\n81962.438429\n\n\n2015-11-29\n38.464001\n37.022487\n...\n41651.502541\n81988.530394\n\n\n2015-11-30\n52.300194\n50.810802\n...\n41694.049324\n82014.622359\n\n\n2015-12-01\n54.286530\n52.898653\n...\n41736.596107\n82040.714323\n\n\n\n\n5 rows × 18 columns\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(12, 5))\n\n# Plot true value\nax.plot(y.index.to_timestamp(), y.cumsum(), label=\"Observed\")\n\n# Train test split\nax.axvline(y_train.index.max(), color=\"black\", alpha=0.8, zorder=-1, linewidth=1)\n\n\n# Capacity asymptotic\nax.fill_between(\n    fh.to_timestamp(),\n    site_quantiles.loc[:, (\"trend/capacity\", \"q25\")],\n    site_quantiles.loc[:, (\"trend/capacity\", \"q75\")],\n    color=\"red\",\n    alpha=0.1,\n    zorder=-1,\n    label=\"Asymptotic capacity\",\n)\nax.plot(\n    fh.to_timestamp(),\n    site_quantiles.loc[:, (\"trend/capacity\", \"mean\")],\n    color=\"red\",\n    alpha=0.2,\n    linestyle=\"--\",\n    zorder=-1,\n    linewidth=0.9,\n)\n\n\n# Plot some random samples\nidxs = np.random.choice(yhat_samples.columns, 10)\n\nfor i, idx in enumerate(idxs):\n    kwargs = {}\n    if i == 0:\n        kwargs[\"label\"] = \"MCMC Samples\"\n    ax.plot(\n        fh.to_timestamp(),\n        yhat_samples.cumsum().loc[:, idx],\n        color=\"black\",\n        alpha=0.1,\n        linewidth=1,\n        **kwargs,\n    )\n\nalpha = 0.1\nupper_and_lower_cumsum = (\n    yhat_samples.cumsum().quantile([alpha / 2, 1 - alpha / 2], axis=1).T\n)\n\n\nax.fill_between(\n    upper_and_lower_cumsum.index.to_timestamp(),\n    upper_and_lower_cumsum.iloc[:, 0],\n    upper_and_lower_cumsum.iloc[:, 1],\n    alpha=0.5,\n)\nax.grid(alpha=0.2)\nax.spines[\"left\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n# Superior\nax.spines[\"top\"].set_visible(False)\n\nax.set_xlim(fh.to_timestamp().min(), fh.to_timestamp().max())\n\n# Add samples to legend\n\nfig.legend()\nax.set_title(\"Total number of stars (forecast)\")\nfig.show()",
    "crumbs": [
      "Custom trend"
    ]
  },
  {
    "objectID": "howto/index.html",
    "href": "howto/index.html",
    "title": "How-to",
    "section": "",
    "text": "In this documentation section, you will find how you can create more advanced patterns\n\n  \n    \n      \n        **Custom Time Series Component**\n        Learn how to craft a custom time series component and unlock powerful forecasting enhancements.\n        → Custom Effect\n      \n    \n  \n\n  \n    \n      \n        Custom Trend\n        Follow a hands-on example to create a custom trend component and integrate it seamlessly into your forecasts.\n        → Custom Trend\n      \n    \n  \n\n  \n    \n      \n        Composition of Time Series Components\n        Discover how to combine multiple time series components for more nuanced and accurate forecasting models.\n        → Composite Exogenous Effect\n      \n    \n  \n\n  \n    \n      \n        Beta % Forecast\n        Recipe: model a bounded percentage using a Beta likelihood.\n        → Beta % Forecast"
  },
  {
    "objectID": "development.html",
    "href": "development.html",
    "title": "Contributing to Prophetverse",
    "section": "",
    "text": "If you are brand new to Prophetverse or open-source development, we recommend searching the GitHub “issues” tab to find issues that interest you. Unassigned issues labeled Docs and good first issue are typically good for newer contributors.\nOnce you’ve found an interesting issue, it’s a good idea to assign the issue to yourself, so nobody else duplicates the work on it. On the Github issue, a comment with the exact text take to automatically assign you the issue (this will take seconds and may require refreshing the page to see it).\nIf for whatever reason you are not able to continue working with the issue, please unassign it, so other people know it’s available again. You can check the list of assigned issues, since people may not be working in them anymore. If you want to work on one that is assigned, feel free to kindly ask the current assignee if you can take it (please allow at least a week of inactivity before considering work in the issue discontinued). To submit your contribution make a pull request"
  },
  {
    "objectID": "development.html#finding-an-issue-to-contribute-to",
    "href": "development.html#finding-an-issue-to-contribute-to",
    "title": "Contributing to Prophetverse",
    "section": "",
    "text": "If you are brand new to Prophetverse or open-source development, we recommend searching the GitHub “issues” tab to find issues that interest you. Unassigned issues labeled Docs and good first issue are typically good for newer contributors.\nOnce you’ve found an interesting issue, it’s a good idea to assign the issue to yourself, so nobody else duplicates the work on it. On the Github issue, a comment with the exact text take to automatically assign you the issue (this will take seconds and may require refreshing the page to see it).\nIf for whatever reason you are not able to continue working with the issue, please unassign it, so other people know it’s available again. You can check the list of assigned issues, since people may not be working in them anymore. If you want to work on one that is assigned, feel free to kindly ask the current assignee if you can take it (please allow at least a week of inactivity before considering work in the issue discontinued). To submit your contribution make a pull request"
  },
  {
    "objectID": "development.html#tips-for-a-successful-pull-request",
    "href": "development.html#tips-for-a-successful-pull-request",
    "title": "Contributing to Prophetverse",
    "section": "Tips for a successful pull request",
    "text": "Tips for a successful pull request\nIf you have made it to the Making a pull request phase, one of the core contributors may take a look."
  },
  {
    "objectID": "development.html#what-is-a-good-pull-request",
    "href": "development.html#what-is-a-good-pull-request",
    "title": "Contributing to Prophetverse",
    "section": "What is a good pull request?",
    "text": "What is a good pull request?\n\nReference an open issue for non-trivial changes to clarify the PR’s purpose\nEnsure you have appropriate tests. These should be the first part of any PR\nKeep your pull requests as simple as possible. Larger PRs take longer to review\nEnsure that CI is in a green state. Reviewers may not even look otherwise\nKeep Updating your pull request, either by request or every few days"
  },
  {
    "objectID": "development.html#creating-a-development-environment",
    "href": "development.html#creating-a-development-environment",
    "title": "Contributing to Prophetverse",
    "section": "Creating a development environment",
    "text": "Creating a development environment\n\nStep 1: install python\nStep 1: install poetry\nStep 2: install dev dependencies  poetry install --extras dev"
  },
  {
    "objectID": "development.html#contributing-to-the-documentation",
    "href": "development.html#contributing-to-the-documentation",
    "title": "Contributing to Prophetverse",
    "section": "Contributing to the documentation",
    "text": "Contributing to the documentation\nContributing to the documentation benefits everyone who uses Prophetverse. We encourage you to help us improve the documentation, and you don’t have to be an expert on Prophetverse to do so! In fact, there are sections of the docs that are worse off after being written by experts. If something in the docs doesn’t make sense to you, updating the relevant section after you figure it out is a great way to ensure it will help the next person."
  },
  {
    "objectID": "development.html#about-the-documentation",
    "href": "development.html#about-the-documentation",
    "title": "Contributing to Prophetverse",
    "section": "About the documentation:",
    "text": "About the documentation:\nThe documentation is written in mkdocs, you can learn more at mkdocs getting started guide."
  },
  {
    "objectID": "development.html#contributor-community",
    "href": "development.html#contributor-community",
    "title": "Contributing to Prophetverse",
    "section": "Contributor community :",
    "text": "Contributor community :\nCommunity slack: None yet."
  },
  {
    "objectID": "development.html#contributing-to-the-code-base",
    "href": "development.html#contributing-to-the-code-base",
    "title": "Contributing to Prophetverse",
    "section": "Contributing to the code base",
    "text": "Contributing to the code base\n\nCode standards\nWriting good code is not just about what you write. It is also about how you write it. During Continuous Integration testing, several tools will be run to check your code for stylistic errors. Generating any warnings will cause the test to fail. Thus, good style is a requirement for submitting code to Prophetverse.There are of tools in Prophetverse to help contributors verify their changes before contributing to the project\n\nPytest\nYou can test your code with pytest integration with the poetry command  poetry run pytest\nThe CI tests are computationally intensive, so if you want to do a faster test you can run a smoke test with the command  poetry run pytest -m \"not ci\"\nIf you also wanna run the tests even faster feel free to parallel processing the tests with pytest-xdist.\n\n\nPre-commit\nAdditionally, Continuous Integration will run code formatting checks like black, isort, and mypy and more using pre-commit hooks. Any warnings from these checks will cause the Continuous Integration to fail; therefore, it is helpful to run the check yourself before submitting code. This can be done by installing pre-commit (which should already have happened if you followed the instructions in Setting up your development environment) and then running:\n\npre-commit install\nfrom the root of the Prophetverse repository. Now all of the styling checks will be run each time you commit changes without your needing to run each one manually. In addition, using pre-commit will also allow you to more easily remain up-to-date with our code checks as they change.\n\n\npre-commit usage\nNote that if needed, you can skip these checks with git commit –no-verify.\nIf you don’t want to use pre-commit as part of your workflow, you can still use it to run its checks with one of the following:\n pre-commit run --files &lt;files you have modified&gt;   pre-commit run --from-ref=upstream/main --to-ref=HEAD --all-files \nwithout needing to have done pre-commit install beforehand.\nFinally, we also have some slow pre-commit checks, which don’t run on each commit but which do run during continuous integration. You can trigger them manually with:\n pre-commit run --hook-stage manual --all-files"
  },
  {
    "objectID": "reference/Prophetverse.html",
    "href": "reference/Prophetverse.html",
    "title": "Prophetverse",
    "section": "",
    "text": "sktime.Prophetverse(\n    self,\n    trend='linear',\n    exogenous_effects=None,\n    default_effect=None,\n    feature_transformer=None,\n    noise_scale=None,\n    likelihood='normal',\n    scale=None,\n    rng_key=None,\n    inference_engine=None,\n    broadcast_mode='estimator',\n)\nUnivariate Prophetverse forecaster with multiple likelihood options.\nThis forecaster implements a univariate model with support for different likelihoods. It differs from Facebook’s Prophet in several ways: - Logistic trend is parametrized differently, inferring capacity from data. - Arbitrary sktime transformers can be used (e.g., FourierFeatures or HolidayFeatures). - No default weekly or yearly seasonality; these must be provided via the feature_transformer. - Uses ‘changepoint_interval’ instead of ‘n_changepoints’ for selecting changepoints. - Allows for configuring distinct functions for each exogenous variable effect.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[str, BaseEffect]\nType of trend to use. Either “linear” (default) or “logistic”, or a custom effect object.\n'linear'\n\n\nexogenous_effects\nOptional[List[BaseEffect]]\nList of effect objects defining the exogenous effects.\nNone\n\n\ndefault_effect\nOptional[BaseEffect]\nThe default effect for variables without a specified effect.\nNone\n\n\nfeature_transformer\nsktime transformer\nTransformer object to generate additional features (e.g., Fourier terms).\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the observation noise. Must be greater than 0. (default: 0.05)\nNone\n\n\nlikelihood\nstr\nThe likelihood model to use. One of “normal”, “gamma”, or “negbinomial”. (default: “normal”)\n'normal'\n\n\nscale\noptional\nScaling value inferred from the data.\nNone\n\n\nrng_key\noptional\nA jax.random.PRNGKey instance, or None.\nNone\n\n\ninference_engine\noptional\nAn inference engine for running the model.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf noise_scale is not greater than 0 or an unsupported likelihood is provided.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_test_params\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\nsktime.Prophetverse.get_test_params(parameter_set='default')\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set name (currently ignored).\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing test parameters.",
    "crumbs": [
      "Sktime",
      "Prophetverse"
    ]
  },
  {
    "objectID": "reference/Prophetverse.html#parameters",
    "href": "reference/Prophetverse.html#parameters",
    "title": "Prophetverse",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[str, BaseEffect]\nType of trend to use. Either “linear” (default) or “logistic”, or a custom effect object.\n'linear'\n\n\nexogenous_effects\nOptional[List[BaseEffect]]\nList of effect objects defining the exogenous effects.\nNone\n\n\ndefault_effect\nOptional[BaseEffect]\nThe default effect for variables without a specified effect.\nNone\n\n\nfeature_transformer\nsktime transformer\nTransformer object to generate additional features (e.g., Fourier terms).\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the observation noise. Must be greater than 0. (default: 0.05)\nNone\n\n\nlikelihood\nstr\nThe likelihood model to use. One of “normal”, “gamma”, or “negbinomial”. (default: “normal”)\n'normal'\n\n\nscale\noptional\nScaling value inferred from the data.\nNone\n\n\nrng_key\noptional\nA jax.random.PRNGKey instance, or None.\nNone\n\n\ninference_engine\noptional\nAn inference engine for running the model.\nNone",
    "crumbs": [
      "Sktime",
      "Prophetverse"
    ]
  },
  {
    "objectID": "reference/Prophetverse.html#raises",
    "href": "reference/Prophetverse.html#raises",
    "title": "Prophetverse",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf noise_scale is not greater than 0 or an unsupported likelihood is provided.",
    "crumbs": [
      "Sktime",
      "Prophetverse"
    ]
  },
  {
    "objectID": "reference/Prophetverse.html#methods",
    "href": "reference/Prophetverse.html#methods",
    "title": "Prophetverse",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_test_params\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\nsktime.Prophetverse.get_test_params(parameter_set='default')\nReturn parameters to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set name (currently ignored).\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing test parameters.",
    "crumbs": [
      "Sktime",
      "Prophetverse"
    ]
  },
  {
    "objectID": "reference/ExactLikelihood.html",
    "href": "reference/ExactLikelihood.html",
    "title": "ExactLikelihood",
    "section": "",
    "text": "effects.ExactLikelihood(self, effect_name, reference_df, prior_scale)\nWrap an effect and applies a normal likelihood to its output.\nThis class uses an input as a reference for the effect, and applies a normal likelihood to the output of the effect.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\neffect_name\nstr\nThe effect to use in the likelihood.\nrequired\n\n\nreference_df\npd.DataFrame\nA dataframe with the reference values. Should be in sktime format, and must have the same index as the input data.\nrequired\n\n\nprior_scale\nfloat\nThe scale of the prior distribution for the likelihood.\nrequired",
    "crumbs": [
      "MMM Likelihoods",
      "ExactLikelihood"
    ]
  },
  {
    "objectID": "reference/ExactLikelihood.html#parameters",
    "href": "reference/ExactLikelihood.html#parameters",
    "title": "ExactLikelihood",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\neffect_name\nstr\nThe effect to use in the likelihood.\nrequired\n\n\nreference_df\npd.DataFrame\nA dataframe with the reference values. Should be in sktime format, and must have the same index as the input data.\nrequired\n\n\nprior_scale\nfloat\nThe scale of the prior distribution for the likelihood.\nrequired",
    "crumbs": [
      "MMM Likelihoods",
      "ExactLikelihood"
    ]
  },
  {
    "objectID": "reference/BudgetOptimizer.html",
    "href": "reference/BudgetOptimizer.html",
    "title": "BudgetOptimizer",
    "section": "",
    "text": "budget_optimization.optimizer.BudgetOptimizer(\n    self,\n    objective,\n    constraints,\n    parametrization_transform=None,\n    method='SLSQP',\n    tol=None,\n    bounds=None,\n    options=None,\n    callback=None,\n)\nBudget optimizer using scipy.optimize.minimize.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobjective\nBaseOptimizationObjective\nObjective function object\nrequired\n\n\nconstraints\nlist of BaseConstraint\nList of constraint objects\nrequired\n\n\ndecision_variable_transform\nBaseDecisionVariableTransform\nDecision variable transform object\nrequired\n\n\nmethod\nstr\nOptimization method to use. Default is “SLSQP”.\n'SLSQP'\n\n\ntol\nfloat\nTolerance for termination. Default is None.\nNone\n\n\nbounds\nUnion[List[tuple], dict[str, tuple]]\nBounds for decision variables. If a list, the value is used directly in scipy.optimize.minimize. If a dict, the keys are the column names and the values are the bounds for each column. Default is (0, np.inf) for each column.\nNone\n\n\noptions\ndict\nOptions for the optimization method. Default is None.\nNone\n\n\ncallback\ncallable\nCallback function to be called after each iteration. Default is None.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nwrap_func_with_inv_transform\nWrap a function with parametrization inverse transform\n\n\n\n\n\nbudget_optimization.optimizer.BudgetOptimizer.wrap_func_with_inv_transform(fun)\nWrap a function with parametrization inverse transform",
    "crumbs": [
      "Budget Optimization",
      "BudgetOptimizer"
    ]
  },
  {
    "objectID": "reference/BudgetOptimizer.html#parameters",
    "href": "reference/BudgetOptimizer.html#parameters",
    "title": "BudgetOptimizer",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nobjective\nBaseOptimizationObjective\nObjective function object\nrequired\n\n\nconstraints\nlist of BaseConstraint\nList of constraint objects\nrequired\n\n\ndecision_variable_transform\nBaseDecisionVariableTransform\nDecision variable transform object\nrequired\n\n\nmethod\nstr\nOptimization method to use. Default is “SLSQP”.\n'SLSQP'\n\n\ntol\nfloat\nTolerance for termination. Default is None.\nNone\n\n\nbounds\nUnion[List[tuple], dict[str, tuple]]\nBounds for decision variables. If a list, the value is used directly in scipy.optimize.minimize. If a dict, the keys are the column names and the values are the bounds for each column. Default is (0, np.inf) for each column.\nNone\n\n\noptions\ndict\nOptions for the optimization method. Default is None.\nNone\n\n\ncallback\ncallable\nCallback function to be called after each iteration. Default is None.\nNone",
    "crumbs": [
      "Budget Optimization",
      "BudgetOptimizer"
    ]
  },
  {
    "objectID": "reference/BudgetOptimizer.html#methods",
    "href": "reference/BudgetOptimizer.html#methods",
    "title": "BudgetOptimizer",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nwrap_func_with_inv_transform\nWrap a function with parametrization inverse transform\n\n\n\n\n\nbudget_optimization.optimizer.BudgetOptimizer.wrap_func_with_inv_transform(fun)\nWrap a function with parametrization inverse transform",
    "crumbs": [
      "Budget Optimization",
      "BudgetOptimizer"
    ]
  },
  {
    "objectID": "reference/TotalBudgetConstraint.html",
    "href": "reference/TotalBudgetConstraint.html",
    "title": "TotalBudgetConstraint",
    "section": "",
    "text": "budget_optimization.constraints.TotalBudgetConstraint(\n    self,\n    channels=None,\n    total=None,\n)\nShared budget constraint.\nThis constraint ensures that the sum of the budgets for the specified channels is equal to the total budget.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nchannels\nlist\nList of channels to be constrained. If None, all channels are used.\nNone\n\n\ntotal\nfloat\nTotal budget. If None, the total budget is computed from the input data.\nNone",
    "crumbs": [
      "Budget Constraints",
      "TotalBudgetConstraint"
    ]
  },
  {
    "objectID": "reference/TotalBudgetConstraint.html#parameters",
    "href": "reference/TotalBudgetConstraint.html#parameters",
    "title": "TotalBudgetConstraint",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nchannels\nlist\nList of channels to be constrained. If None, all channels are used.\nNone\n\n\ntotal\nfloat\nTotal budget. If None, the total budget is computed from the input data.\nNone",
    "crumbs": [
      "Budget Constraints",
      "TotalBudgetConstraint"
    ]
  },
  {
    "objectID": "reference/LiftExperimentLikelihood.html",
    "href": "reference/LiftExperimentLikelihood.html",
    "title": "LiftExperimentLikelihood",
    "section": "",
    "text": "effects.LiftExperimentLikelihood(\n    self,\n    effect,\n    lift_test_results,\n    prior_scale,\n    likelihood_scale=1,\n)\nWrap an effect and applies a normal likelihood to its output.\nThis class uses an input as a reference for the effect, and applies a normal likelihood to the output of the effect.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\neffect\nBaseEffect\nThe effect to wrap.\nrequired\n\n\nlift_test_results\npd.DataFrame\nA dataframe with the lift test results. Should be in sktime format, and must have the same index as the input data.\nrequired\n\n\nprior_scale\nfloat\nThe scale of the prior distribution for the likelihood.\nrequired",
    "crumbs": [
      "MMM Likelihoods",
      "LiftExperimentLikelihood"
    ]
  },
  {
    "objectID": "reference/LiftExperimentLikelihood.html#parameters",
    "href": "reference/LiftExperimentLikelihood.html#parameters",
    "title": "LiftExperimentLikelihood",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\neffect\nBaseEffect\nThe effect to wrap.\nrequired\n\n\nlift_test_results\npd.DataFrame\nA dataframe with the lift test results. Should be in sktime format, and must have the same index as the input data.\nrequired\n\n\nprior_scale\nfloat\nThe scale of the prior distribution for the likelihood.\nrequired",
    "crumbs": [
      "MMM Likelihoods",
      "LiftExperimentLikelihood"
    ]
  },
  {
    "objectID": "reference/BetaTargetLikelihood.html",
    "href": "reference/BetaTargetLikelihood.html",
    "title": "BetaTargetLikelihood",
    "section": "",
    "text": "BetaTargetLikelihood\neffects.BetaTargetLikelihood(self, noise_scale=0.05, epsilon=1e-05)"
  },
  {
    "objectID": "reference/PiecewiseLinearTrend.html",
    "href": "reference/PiecewiseLinearTrend.html",
    "title": "PiecewiseLinearTrend",
    "section": "",
    "text": "effects.PiecewiseLinearTrend(\n    self,\n    changepoint_interval=25,\n    changepoint_range=0.8,\n    changepoint_prior_scale=0.001,\n    offset_prior_scale=0.1,\n    squeeze_if_single_series=True,\n    remove_seasonality_before_suggesting_initial_vals=True,\n    global_rate_prior_loc=None,\n    offset_prior_loc=None,\n)\nPiecewise Linear Trend model.\nThis model assumes that the trend is piecewise linear, with changepoints at regular intervals. The number of changepoints is determined by the changepoint_interval and changepoint_range parameters. The changepoint_interval parameter specifies the interval between changepoints, while the changepoint_range parameter specifies the range of the changepoints.\nThis implementation is based on the Prophet_ library. The initial values (global rate and global offset) are suggested using the maximum and minimum values of the time series data.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nchangepoint_interval\nint\nThe interval between changepoints.\n25\n\n\nchangepoint_range\nint\nThe range of the changepoints.\n0.8\n\n\nchangepoint_prior_scale\ndist.Distribution\nThe prior scale for the changepoints.\n0.001\n\n\noffset_prior_scale\nfloat\nThe prior scale for the offset. Default is 0.1.\n0.1\n\n\nsqueeze_if_single_series\nbool\nIf True, squeeze the output if there is only one series. Default is True.\nTrue\n\n\nremove_seasonality_before_suggesting_initial_vals\nbool\nIf True, remove seasonality before suggesting initial values, using sktime’s detrender. Default is True.\nTrue\n\n\nglobal_rate_prior_loc\nfloat\nThe prior location for the global rate. Default is suggested empirically from data.\nNone\n\n\noffset_prior_loc\nfloat\nThe prior location for the offset. Default is suggested empirically from data.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nn_changepoint_per_series\nGet the number of changepoints per series.\n\n\nn_changepoints\nGet the total number of changepoints.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_changepoint_matrix\nReturn the changepoint matrix for the given index.\n\n\n\n\n\neffects.PiecewiseLinearTrend.get_changepoint_matrix(idx)\nReturn the changepoint matrix for the given index.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.PeriodIndex\nThe index for which to compute the changepoint matrix.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\njnp.ndarray: The changepoint matrix.",
    "crumbs": [
      "Trends",
      "PiecewiseLinearTrend"
    ]
  },
  {
    "objectID": "reference/PiecewiseLinearTrend.html#parameters",
    "href": "reference/PiecewiseLinearTrend.html#parameters",
    "title": "PiecewiseLinearTrend",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nchangepoint_interval\nint\nThe interval between changepoints.\n25\n\n\nchangepoint_range\nint\nThe range of the changepoints.\n0.8\n\n\nchangepoint_prior_scale\ndist.Distribution\nThe prior scale for the changepoints.\n0.001\n\n\noffset_prior_scale\nfloat\nThe prior scale for the offset. Default is 0.1.\n0.1\n\n\nsqueeze_if_single_series\nbool\nIf True, squeeze the output if there is only one series. Default is True.\nTrue\n\n\nremove_seasonality_before_suggesting_initial_vals\nbool\nIf True, remove seasonality before suggesting initial values, using sktime’s detrender. Default is True.\nTrue\n\n\nglobal_rate_prior_loc\nfloat\nThe prior location for the global rate. Default is suggested empirically from data.\nNone\n\n\noffset_prior_loc\nfloat\nThe prior location for the offset. Default is suggested empirically from data.\nNone",
    "crumbs": [
      "Trends",
      "PiecewiseLinearTrend"
    ]
  },
  {
    "objectID": "reference/PiecewiseLinearTrend.html#attributes",
    "href": "reference/PiecewiseLinearTrend.html#attributes",
    "title": "PiecewiseLinearTrend",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nn_changepoint_per_series\nGet the number of changepoints per series.\n\n\nn_changepoints\nGet the total number of changepoints.",
    "crumbs": [
      "Trends",
      "PiecewiseLinearTrend"
    ]
  },
  {
    "objectID": "reference/PiecewiseLinearTrend.html#methods",
    "href": "reference/PiecewiseLinearTrend.html#methods",
    "title": "PiecewiseLinearTrend",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_changepoint_matrix\nReturn the changepoint matrix for the given index.\n\n\n\n\n\neffects.PiecewiseLinearTrend.get_changepoint_matrix(idx)\nReturn the changepoint matrix for the given index.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.PeriodIndex\nThe index for which to compute the changepoint matrix.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\njnp.ndarray: The changepoint matrix.",
    "crumbs": [
      "Trends",
      "PiecewiseLinearTrend"
    ]
  },
  {
    "objectID": "reference/SharedBudgetConstraint.html",
    "href": "reference/SharedBudgetConstraint.html",
    "title": "TotalBudgetConstraint",
    "section": "",
    "text": "budget_optimization.constraints.TotalBudgetConstraint(\n    self,\n    channels=None,\n    total=None,\n)\nShared budget constraint.\nThis constraint ensures that the sum of the budgets for the specified channels is equal to the total budget.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nchannels\nlist\nList of channels to be constrained. If None, all channels are used.\nNone\n\n\ntotal\nfloat\nTotal budget. If None, the total budget is computed from the input data.\nNone"
  },
  {
    "objectID": "reference/SharedBudgetConstraint.html#parameters",
    "href": "reference/SharedBudgetConstraint.html#parameters",
    "title": "TotalBudgetConstraint",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nchannels\nlist\nList of channels to be constrained. If None, all channels are used.\nNone\n\n\ntotal\nfloat\nTotal budget. If None, the total budget is computed from the input data.\nNone"
  },
  {
    "objectID": "reference/ChainedEffects.html",
    "href": "reference/ChainedEffects.html",
    "title": "ChainedEffects",
    "section": "",
    "text": "effects.ChainedEffects(self, steps)\nChains multiple effects sequentially, applying them one after the other.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsteps\nList[BaseEffect]\nA list of effects to be applied sequentially.\nrequired",
    "crumbs": [
      "Exogenous effects",
      "ChainedEffects"
    ]
  },
  {
    "objectID": "reference/ChainedEffects.html#parameters",
    "href": "reference/ChainedEffects.html#parameters",
    "title": "ChainedEffects",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsteps\nList[BaseEffect]\nA list of effects to be applied sequentially.\nrequired",
    "crumbs": [
      "Exogenous effects",
      "ChainedEffects"
    ]
  },
  {
    "objectID": "reference/NormalTargetLikelihood.html",
    "href": "reference/NormalTargetLikelihood.html",
    "title": "NormalTargetLikelihood",
    "section": "",
    "text": "NormalTargetLikelihood\neffects.NormalTargetLikelihood(self, noise_scale=0.05)",
    "crumbs": [
      "Target Likelihoods",
      "NormalTargetLikelihood"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Sktime models\n\n\n\nProphetverse\nUnivariate Prophetverse forecaster with multiple likelihood options.\n\n\nHierarchicalProphet\nA Bayesian hierarchical time series forecasting model based on Meta’s Prophet.\n\n\n\n\n\n\nExogenous effects\n\n\n\nLinearEffect\nRepresents a linear effect in a hierarchical prophet model.\n\n\nLinearFourierSeasonality\nLinear Fourier Seasonality effect.\n\n\nLogEffect\nRepresents a log effect as effect = scale * log(rate * data + 1).\n\n\nHillEffect\nRepresents a Hill effect in a time series model.\n\n\nMichaelisMentenEffect\nRepresents a Michaelis-Menten effect for modeling saturation.\n\n\nChainedEffects\nChains multiple effects sequentially, applying them one after the other.\n\n\nGeometricAdstockEffect\nRepresents a Geometric Adstock effect in a time series model.\n\n\n\n\n\n\nMMM Likelihoods\n\n\n\nLiftExperimentLikelihood\nWrap an effect and applies a normal likelihood to its output.\n\n\nExactLikelihood\nWrap an effect and applies a normal likelihood to its output.\n\n\n\n\n\n\nTrends\n\n\n\nPiecewiseLinearTrend\nPiecewise Linear Trend model.\n\n\nPiecewiseLogisticTrend\nPiecewise logistic trend model.\n\n\nFlatTrend\nFlat trend model.\n\n\n\n\n\n\nLikelihoods for the target variable\n\n\n\nMultivariateNormal\nBase class for effects.\n\n\nNormalTargetLikelihood\n\n\n\nGammaTargetLikelihood\n\n\n\nNegativeBinomialTargetLikelihood\n\n\n\n\n\n\n\nBudget Optimization\n\n\n\nBudgetOptimizer\nBudget optimizer using scipy.optimize.minimize.\n\n\n\n\n\n\nBudget Constraints\n\n\n\nTotalBudgetConstraint\nShared budget constraint.\n\n\nMinimumTargetResponse\nMinimum target response constraint.\n\n\n\n\n\n\nObjective Functions\n\n\n\nMinimizeBudget\nMinimize budget constraint objective function.\n\n\nMaximizeKPI\nMaximize the KPI objective function.\n\n\nMaximizeROI\nMaximize return on investment (ROI) objective function.\n\n\n\n\n\n\nBudget Parametrizations",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#sktime",
    "href": "reference/index.html#sktime",
    "title": "Function reference",
    "section": "",
    "text": "Sktime models\n\n\n\nProphetverse\nUnivariate Prophetverse forecaster with multiple likelihood options.\n\n\nHierarchicalProphet\nA Bayesian hierarchical time series forecasting model based on Meta’s Prophet.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#exogenous-effects",
    "href": "reference/index.html#exogenous-effects",
    "title": "Function reference",
    "section": "",
    "text": "Exogenous effects\n\n\n\nLinearEffect\nRepresents a linear effect in a hierarchical prophet model.\n\n\nLinearFourierSeasonality\nLinear Fourier Seasonality effect.\n\n\nLogEffect\nRepresents a log effect as effect = scale * log(rate * data + 1).\n\n\nHillEffect\nRepresents a Hill effect in a time series model.\n\n\nMichaelisMentenEffect\nRepresents a Michaelis-Menten effect for modeling saturation.\n\n\nChainedEffects\nChains multiple effects sequentially, applying them one after the other.\n\n\nGeometricAdstockEffect\nRepresents a Geometric Adstock effect in a time series model.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#mmm-likelihoods",
    "href": "reference/index.html#mmm-likelihoods",
    "title": "Function reference",
    "section": "",
    "text": "MMM Likelihoods\n\n\n\nLiftExperimentLikelihood\nWrap an effect and applies a normal likelihood to its output.\n\n\nExactLikelihood\nWrap an effect and applies a normal likelihood to its output.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#trends",
    "href": "reference/index.html#trends",
    "title": "Function reference",
    "section": "",
    "text": "Trends\n\n\n\nPiecewiseLinearTrend\nPiecewise Linear Trend model.\n\n\nPiecewiseLogisticTrend\nPiecewise logistic trend model.\n\n\nFlatTrend\nFlat trend model.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#target-likelihoods",
    "href": "reference/index.html#target-likelihoods",
    "title": "Function reference",
    "section": "",
    "text": "Likelihoods for the target variable\n\n\n\nMultivariateNormal\nBase class for effects.\n\n\nNormalTargetLikelihood\n\n\n\nGammaTargetLikelihood\n\n\n\nNegativeBinomialTargetLikelihood",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#budget-optimization",
    "href": "reference/index.html#budget-optimization",
    "title": "Function reference",
    "section": "",
    "text": "Budget Optimization\n\n\n\nBudgetOptimizer\nBudget optimizer using scipy.optimize.minimize.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#budget-constraints",
    "href": "reference/index.html#budget-constraints",
    "title": "Function reference",
    "section": "",
    "text": "Budget Constraints\n\n\n\nTotalBudgetConstraint\nShared budget constraint.\n\n\nMinimumTargetResponse\nMinimum target response constraint.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#objective-functions",
    "href": "reference/index.html#objective-functions",
    "title": "Function reference",
    "section": "",
    "text": "Objective Functions\n\n\n\nMinimizeBudget\nMinimize budget constraint objective function.\n\n\nMaximizeKPI\nMaximize the KPI objective function.\n\n\nMaximizeROI\nMaximize return on investment (ROI) objective function.",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#budget-parametrizations",
    "href": "reference/index.html#budget-parametrizations",
    "title": "Function reference",
    "section": "",
    "text": "Budget Parametrizations",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/MaximizeROI.html",
    "href": "reference/MaximizeROI.html",
    "title": "MaximizeROI",
    "section": "",
    "text": "MaximizeROI\nbudget_optimization.objectives.MaximizeROI(self)\nMaximize return on investment (ROI) objective function.",
    "crumbs": [
      "Objective Functions",
      "MaximizeROI"
    ]
  },
  {
    "objectID": "reference/MinimizeBudget.html",
    "href": "reference/MinimizeBudget.html",
    "title": "MinimizeBudget",
    "section": "",
    "text": "MinimizeBudget\nbudget_optimization.objectives.MinimizeBudget(self, scale=1)\nMinimize budget constraint objective function.",
    "crumbs": [
      "Objective Functions",
      "MinimizeBudget"
    ]
  },
  {
    "objectID": "reference/PiecewiseLogisticTrend.html",
    "href": "reference/PiecewiseLogisticTrend.html",
    "title": "PiecewiseLogisticTrend",
    "section": "",
    "text": "effects.PiecewiseLogisticTrend(\n    self,\n    changepoint_interval=25,\n    changepoint_range=0.8,\n    changepoint_prior_scale=0.001,\n    offset_prior_scale=10,\n    capacity_prior=None,\n    squeeze_if_single_series=True,\n    remove_seasonality_before_suggesting_initial_vals=True,\n    global_rate_prior_loc=None,\n    offset_prior_loc=None,\n)\nPiecewise logistic trend model.\nThis logistic trend differs from the original Prophet logistic trend in that it considers a capacity prior distribution. The capacity prior distribution is used to estimate the maximum value that the time series trend can reach.\nIt uses internally the piecewise linear trend model, and then applies a logistic function to the output of the linear trend model.\nThe initial values (global rate and global offset) are suggested using the maximum and minimum values of the time series data.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nchangepoint_interval\nint\nThe interval between changepoints.\n25\n\n\nchangepoint_range\nint\nThe range of the changepoints.\n0.8\n\n\nchangepoint_prior_scale\ndist.Distribution\nThe prior scale for the changepoints.\n0.001\n\n\noffset_prior_scale\nfloat\nThe prior scale for the offset. Default is 0.1.\n10\n\n\nsqueeze_if_single_series\nbool\nIf True, squeeze the output if there is only one series. Default is True.\nTrue\n\n\nremove_seasonality_before_suggesting_initial_vals\nbool\nIf True, remove seasonality before suggesting initial values, using sktime’s detrender. Default is True.\nTrue\n\n\ncapacity_prior\ndist.Distribution\nThe prior distribution for the capacity. Default is a HalfNormal distribution with loc=1.05 and scale=1.\nNone",
    "crumbs": [
      "Trends",
      "PiecewiseLogisticTrend"
    ]
  },
  {
    "objectID": "reference/PiecewiseLogisticTrend.html#parameters",
    "href": "reference/PiecewiseLogisticTrend.html#parameters",
    "title": "PiecewiseLogisticTrend",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nchangepoint_interval\nint\nThe interval between changepoints.\n25\n\n\nchangepoint_range\nint\nThe range of the changepoints.\n0.8\n\n\nchangepoint_prior_scale\ndist.Distribution\nThe prior scale for the changepoints.\n0.001\n\n\noffset_prior_scale\nfloat\nThe prior scale for the offset. Default is 0.1.\n10\n\n\nsqueeze_if_single_series\nbool\nIf True, squeeze the output if there is only one series. Default is True.\nTrue\n\n\nremove_seasonality_before_suggesting_initial_vals\nbool\nIf True, remove seasonality before suggesting initial values, using sktime’s detrender. Default is True.\nTrue\n\n\ncapacity_prior\ndist.Distribution\nThe prior distribution for the capacity. Default is a HalfNormal distribution with loc=1.05 and scale=1.\nNone",
    "crumbs": [
      "Trends",
      "PiecewiseLogisticTrend"
    ]
  },
  {
    "objectID": "the-theory.html",
    "href": "the-theory.html",
    "title": "Mathematical formulation",
    "section": "",
    "text": "Figure 1: Generalized Additive Models are versatile. Prophet is one of the many models that can be built on top of it. The idea of Prophetverse is giving access to that universe.\nProphetverse leverages the Generalized Additive Model (GAM) idea in the original Prophet model and extends it to be more flexible and customizable. The core principle of GAMs is to model the expected value \\(y_{mean}\\) of the endogenous variable \\(Y\\) as the sum of many functions \\(\\{f_i\\}_{i=1}^n\\) of exogenous variables \\(\\{x_i\\}_{i=1}^n\\).\n\\[\ny_{mean} = f_1(x_1) + f_2(x_2) + \\ldots + f_n(t)\\text{, }\\quad n \\in \\mathbb{N}\n\\]\nThe innovation in Prophet is the use of Bayesian GAMs to model time series data. Instead of approximating the time series through auto-regressive models, Prophet treats it as a curve-fitting exercise. This approach results in fast, interpretable, and accurate forecasts. The Prophet formulation is:\n\\[\ny_{\\text{mean}} = \\begin{cases} y_{\\text{mean}}(t) = \\tau(t) + s(t) + h(t) + v(t) & \\text{if additive} \\\\ y_{\\text{mean}}(t) = \\tau(t) + \\tau(t) \\cdot s(t) + \\tau(t) \\cdot h(t) + \\tau(t) \\cdot v(t) & \\text{if multiplicative} \\end{cases}\n\\]\nwhere \\(\\tau(t)\\) is the trend component, \\(s(t)\\) is the seasonality component, \\(h(t)\\) is the holiday component, and \\(v(t)\\) is other regressors components. Those components are hard-coded as linear in the original formulation of Facebook Prophet, but in Prophetverse they are versatile and can be defined by the user. This is the first main difference between Prophet and Prophetverse. The \\(f_i\\) functions are defined by the Effects API , where the user can create their own components and priors, by using the already available ones or by creating new BaseEffect subclasses.\n\\[\\begin{align}\ny_{mean} &= \\sum\\limits_{i=1}^n f_i(x_i(t), \\{f_j(x_j)\\}_{j&lt;i}) \\\\\n         &= f_1(x_1(t)) + f_2(x_2(t), f_1(x_1(t))) + \\ldots + f_n(t, \\{f_j(x_j)\\}_{j&lt;n})\n\\end{align}\\]\nwhere\nThis definition superseeds the Prophet formulation because effects are ordered, so that the output of previous effects can be used as input for the next ones. This allows for complex interactions between exogenous variables."
  },
  {
    "objectID": "the-theory.html#likelihood",
    "href": "the-theory.html#likelihood",
    "title": "Mathematical formulation",
    "section": "Likelihood",
    "text": "Likelihood\nIn the original Prophet, the likelihood is a Normal distribution, but in Prophetverse it can be Normal, Gamma, Negative Binomial, or Beta.\n\\[\ny \\sim \\mathcal{likelihood}(\\phi(\\hat{y}_{mean}), \\sigma^2)\\quad \\text{where} \\quad\n\\sigma \\sim HalfNormal(\\sigma_{hyper})\n\\]\nwhere \\(\\sigma_{hyper}\\) is a hyperparameter and \\(\\phi\\) is a function that maps the mean to the support of the likelihood. For normal likelihood, \\(\\phi\\) is the identity function, but for Gamma, Negative Binomial, and Beta, it is\n\\[\n\\phi(k) = \\begin{cases}\nk & \\text{if } k &gt; z \\\\\nz\\exp(k-z) & \\text{if } k \\leq z\n\\end{cases}\n\\]\nfor some small threshold \\(z\\). We set \\(z = 10^{-5}\\) in our implementation. The reason for this is to avoid zero or negative values in the support of the likelihood, which can lead to error. For Beta likelihood, we also ensure values stay below 1 using a similar transformation."
  },
  {
    "objectID": "the-theory.html#trend",
    "href": "the-theory.html#trend",
    "title": "Mathematical formulation",
    "section": "Trend",
    "text": "Trend\nThere are mainly two types of trends supported: linear and logistic. We will first take a look at the original mathematical formulation of Prophet’s paper, and then simplify it to obtain a simpler and more interpretable version.\n\nLinear trend\n\nOriginal formulation\nThe linear trend is modeled as a piecewise linear functions with changepoints. Let \\(M\\) be the number of changepoints, \\(\\delta \\in \\mathbb{R}^M\\) be the rate adjustment at each changepoint, \\(\\{\\kappa_i\\}_{i=1}^M\\) be the changepoint times, and be \\(a(t) \\in \\{0,1\\}^M\\) be a vector which assumes, at each index, 1 if the corresponding changepoint is greater than \\(t\\) and 0 otherwise. In addition, let \\(k\\) represent the global rate and \\(m\\) the global offset. Then, the linear trend is defined as:\n\\[\n\\tau(t) = (k + a(t)^T\\delta)t + (m + a(t)^T\\gamma), \\quad \\text{where} \\quad \\gamma_i = \\kappa_i\\delta_i\n\\]\nThe first part accounts for the rate adjustment at each changepoint, and the second part corrects the offset at each changepoint, so that the trend is continuous.\n\n\nProphetverse’s equivalent formulation\nThis can be simplified as a first-order spline regression with \\(M\\) knots (changepoints). Let \\(b(t) \\in \\mathbb{R}^M\\) be a vector so that \\(b(t)_i = (t - \\kappa_i)_+\\) (the positive part of \\(t - \\kappa_i\\)). Then, the piecewise linear trend value for time \\(t\\) can be written as:\n\\[\n\\tau(t) = b(t)^T \\delta + kt + m\n\\]\nWe can also write the trend for all \\(t \\in \\{t_1,\\dots, t_T\\}\\) as a matrix multiplication. Let \\(\\mathbf{B} \\in \\mathbb{R}^{T \\times M+2}\\) be the matrix whose rows are \\(b'(t) = \\left[ b(t), t, 1 \\right]\\) for each time \\(t\\). In other words, it is the spline basis matrix. The \\(t\\) and \\(1\\) at the end of the vector are included to account for the global rate and offset. Furthermore, consider the vector \\(\\delta' = \\left[ \\delta, k, m \\right]\\). Then, the trend vector \\(G \\in \\mathbb{R}^T\\), \\(G_i = \\tau(\\mathbf{t}_i)\\), can be written as:\n\\[\\begin{align}\nG &= \\mathbf{B}\\delta' \\\\\n\\end{align}\\]\n\\[\\begin{align}\nG &=  \\begin{bmatrix}\n(t_0 - \\kappa_0)_+ & (t_0 - \\kappa_1)_+ & \\ldots & (t_0 - \\kappa_{M-1})_+ & t_0 & 1 \\\\\n(t_1 - \\kappa_0)_+ & (t_1 - \\kappa_1)_+ & \\ldots & (t_1 - \\kappa_{M-1})_+ & t_1 & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n(t_{T-1} - \\kappa_0)_+ & (t_{T-1} - \\kappa_1)_+ & \\ldots & (t_{T-1} - \\kappa_{M-1})_+ & t_{T-1} & 1 \\\\\n\\end{bmatrix} \\begin{bmatrix}\n\\delta_0 \\\\\n\\delta_1 \\\\\n\\vdots \\\\\n\\delta_{M-1} \\\\\nk \\\\\nm \\\\\n\\end{bmatrix}\n\\end{align}\\]\n\n\n\n\n\n\nTipExample\n\n\n\nOne possible realization of \\(\\mathbf{B}\\) is:\n\\[\n\\begin{bmatrix}\n0 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 1 & 1 \\\\\n1 & 0 & 0 & 0 & 2 & 1 \\\\\n2 & 1 & 0 & 0 & 3 & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\nT-1 & T-2 & T-3 & T-4 & T-1 & 1 \\\\\n\\end{bmatrix}\n\\]\n\n\n\n\n\nLogistic trend\nThe logistic trend of the original model uses the piecewise logistic linear trend to change the rate at which \\(t\\) grows. We will not explain the mathematical formulation of the original paper here, and will already leverage what we have learned about the linear trend to simplify it.\n\\[\nG = \\frac{C}{1 + \\exp(-\\mathbf{B}\\delta')}\n\\]\nwhere \\(C\\) is the logistic capacity, which should be passed as input to Prophet, but is a random variable in Prophetverse.\n\n\nChangepoint priors\nA Laplace prior is put on the rate adjustment \\(\\delta_i \\sim Laplace(0, \\sigma_{\\delta})\\) where \\(\\sigma_{\\delta}\\) is a hyperparameter. The changepoint times \\(\\kappa_i\\) can be predefined by the user, or can be uniformly distributed in the training data. The offset and rate prior location are set in a “smart” way, by checking analytically what would be the values that fit the maximum and minimum points of the time series.\n\n\n\n\n\n\nNote\n\n\n\nAlthough those trend are the ones that come with the library, the user can define any trend, including a trend that depends on some exogenous variable. Flexibility is the key here."
  },
  {
    "objectID": "the-theory.html#seasonality",
    "href": "the-theory.html#seasonality",
    "title": "Mathematical formulation",
    "section": "Seasonality",
    "text": "Seasonality\nTo model seasonality, Prophetverse uses a Fourier series to approximate periodic functions, allowing the model to fit complex seasonal patterns flexibly. This approach involves determining the number of Fourier terms (K), which corresponds to the complexity of the seasonality. The formula for a seasonal component s(t) in terms of a Fourier series is given as:\n\\[\ns(t) = \\sum_{k=1}^K \\left( a_k \\cos\\left(\\frac{2\\pi kt}{P}\\right) + b_k \\sin\\left(\\frac{2\\pi kt}{P}\\right) \\right)\n\\]\nHere, P is the period (e.g., 365.25 for yearly seasonality), and \\(a_k\\) and \\(b_k\\) are the Fourier coefficients that the model estimates. The choice of K depends on the granularity of the seasonal changes one wishes to capture. A Normal prior is placed on the coefficients, \\(a_k, b_k \\sim \\mathcal{N}(0, \\sigma_s)\\), where \\(\\sigma_s\\) is a hyperparameter.\nSee LinearFourierSeasonality for more details on the hyperparameters of the effect\n\nMatrix Formulation of Fourier Series\nTo efficiently compute the seasonality for multiple time points, we can represent the Fourier series in a matrix form. This method is especially useful for handling large datasets and simplifies the implementation of the model in computational software. Let \\(T\\) be the number of time points, and create a design matrix \\(X\\) of size \\(T \\times 2K\\). Each row of \\(X\\) corresponds to a time point and contains all Fourier basis functions evaluated at that time:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n\\cos\\left(\\frac{2\\pi \\cdot 1 \\cdot t_1}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot 1 \\cdot t_1}{P}\\right) & \\cdots & \\cos\\left(\\frac{2\\pi \\cdot K \\cdot t_1}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot K \\cdot t_1}{P}\\right) \\\\\n\\cos\\left(\\frac{2\\pi \\cdot 1 \\cdot t_2}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot 1 \\cdot t_2}{P}\\right) & \\cdots & \\cos\\left(\\frac{2\\pi \\cdot K \\cdot t_2}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot K \\cdot t_2}{P}\\right) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\cos\\left(\\frac{2\\pi \\cdot 1 \\cdot t_T}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot 1 \\cdot t_T}{P}\\right) & \\cdots & \\cos\\left(\\frac{2\\pi \\cdot K \\cdot t_T}{P}\\right) & \\sin\\left(\\frac{2\\pi \\cdot K \\cdot t_T}{P}\\right)\n\\end{bmatrix}\n\\]\nCoefficient Vector:\nDefine a vector \\(\\beta\\) of length 2K containing the coefficients \\(a_1, b_1, \\dots, a_K, b_K\\):\n\\[\n\\mathbf{\\beta} = \\begin{bmatrix}\na_1 \\\\\nb_1 \\\\\n\\vdots \\\\\na_K \\\\\nb_K\n\\end{bmatrix}\n\\]\nThe seasonality for all time points can then be computed through the matrix product of \\(X\\) and \\(\\beta\\):\n\\[\n\\mathbf{s} = \\mathbf{X} \\mathbf{\\beta}\n\\]\nEach element of vector \\(s\\), denoted as \\(s_i\\), represents the seasonality at time \\(t_i\\).\nThis matrix approach not only makes the computation faster and more scalable but also simplifies integration with other components of the forecasting model. One drawback is that it assumes a constant seasonality, but an user can also define a seasonality that changes with time in Prophetverse, by creating a custom Effect class."
  },
  {
    "objectID": "the-theory.html#multivariate-model",
    "href": "the-theory.html#multivariate-model",
    "title": "Mathematical formulation",
    "section": "Multivariate model",
    "text": "Multivariate model\nProphetverse also supports multivariate forecasting. In this case, the model is essentially the same, but for now only Normal Likelihood is supported. Depending on the usage of the library, we may add other likelihoods in the future (please open an issue if you need it!). In that case, all other components are estimated in the same way, but the likelihood is a multivariate distribution. The mean of the distribution is a vector, and the covariance matrix prior is a LKJ distribution."
  },
  {
    "objectID": "mmm/budget_allocation.html",
    "href": "mmm/budget_allocation.html",
    "title": "Budget Optimization for Single and Multiple Time Series",
    "section": "",
    "text": "In this tutorial, you’ll learn how to use Prophetverse’s budget-optimization module to:\nYou’ll also see how to switch between different parametrizations without hassle, such as:\nBy the end, you’ll know how to pick the right setup for your campaign goals and make adjustments in seconds.",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "mmm/budget_allocation.html#setting-up-the-problem",
    "href": "mmm/budget_allocation.html#setting-up-the-problem",
    "title": "Budget Optimization for Single and Multiple Time Series",
    "section": "1.1. Setting Up the Problem",
    "text": "1.1. Setting Up the Problem\nFirst, let’s set up our environment and load the data for a single time series optimization.\n\nimport numpyro\n\nnumpyro.enable_x64()\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport pandas as pd\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n\n\n1.1.1. Load synthetic data\nWe will load a synthetic dataset and a pre-fitted Prophetverse model.\n\nfrom prophetverse.datasets._mmm.dataset1 import get_dataset\n\ny, X, lift_tests, true_components, model = get_dataset()\n\n\n\n1.1.2. Utility plotting functions\nThis helper function will allow us to compare spend before and after optimization.\n\n\nCode\ndef plot_spend_comparison(\n    X_baseline,\n    X_optimized,\n    channels,\n    indexer,\n    *,\n    baseline_title=\"Baseline Spend: Pre-Optimization\",\n    optimized_title=\"Optimized Spend: Maximizing KPI\",\n    figsize=(8, 4),\n):\n    fig, ax = plt.subplots(1, 2, figsize=figsize)\n\n    X_baseline.loc[indexer, channels].plot(ax=ax[0], linewidth=2)\n    X_optimized.loc[indexer, channels].plot(ax=ax[1], linewidth=2, linestyle=\"--\")\n\n    ax[0].set_title(baseline_title, fontsize=14, weight=\"bold\")\n    ax[1].set_title(optimized_title, fontsize=14, weight=\"bold\")\n\n    for a in ax:\n        a.set_ylabel(\"Spend\")\n        a.set_xlabel(\"Date\")\n        a.legend(loc=\"upper right\", frameon=True)\n        a.grid(axis=\"x\", visible=False)\n        a.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n        a.xaxis.set_major_formatter(mdates.DateFormatter(\"%b\"))\n\n    # Align y-axis\n    y_max = max(\n        X_baseline.loc[indexer, channels].max().max(),\n        X_optimized.loc[indexer, channels].max().max(),\n    )\n    for a in ax:\n        a.set_ylim(0, y_max * 1.05)\n\n    plt.tight_layout()\n    return fig, ax",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "mmm/budget_allocation.html#budget-optimization",
    "href": "mmm/budget_allocation.html#budget-optimization",
    "title": "Budget Optimization for Single and Multiple Time Series",
    "section": "1.2. Budget Optimization",
    "text": "1.2. Budget Optimization\nThe budget-optimization module is composed of three main components:\n\nThe objective function: What you want to optimize (e.g., maximize KPI).\nThe constraints: Rules the optimization must follow (e.g., total budget).\nThe parametrization transform: How the problem is parametrized (e.g., daily spend vs. channel shares).\n\n\n1.2.1. Maximizing a KPI\nThe BudgetOptimizer class is the main entry point. By default, it optimizes the daily spend for each channel to maximize a given KPI.\n\nfrom prophetverse.budget_optimization import (\n    BudgetOptimizer,\n    TotalBudgetConstraint,\n    MaximizeKPI,\n)\n\nbudget_optimizer = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[TotalBudgetConstraint()],\n    options={\"disp\": True, \"maxiter\":1000},\n)\n\nLet’s define our optimization horizon:\n\nhorizon = pd.period_range(\"2004-12-01\", \"2004-12-31\", freq=\"D\")\n\nNow, we run the optimization:\n\nimport time\n\nstart_time = time.time()\nX_opt = budget_optimizer.optimize(\n    model=model,\n    X=X,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\noptimization_time = time.time() - start_time\nprint(f\"Optimization completed in {optimization_time:.2f} seconds\")\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -754679704.6820625\n            Iterations: 108\n            Function evaluations: 125\n            Gradient evaluations: 104\nOptimization completed in 3.44 seconds\n\n\n\nBaseline vs. optimized spend\nLet’s compare the model’s predictions before and after the optimization.\n\ny_pred_baseline = model.predict(X=X, fh=horizon)\ny_pred_opt = model.predict(X=X_opt, fh=horizon)\n\nfig, ax = plot_spend_comparison(\n    X,\n    X_opt,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    horizon,\n)\n\nkpi_gain = y_pred_opt.sum() / y_pred_baseline.sum() - 1\nfig.suptitle(f\"KPI gain: +{kpi_gain:.2%}\", fontsize=16,weight=\"bold\", y=1.02)\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n\n\n\n\n\n\n1.2.2. Reparametrization: Optimizing channel share\nInstead of daily spend, we can optimize the share of the budget for each channel. This is useful for keeping a fixed spending pattern (e.g., for seasonal campaigns).\n\nfrom prophetverse.budget_optimization import InvestmentPerChannelTransform\n\nbudget_optimizer_reparam = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[TotalBudgetConstraint()],\n    parametrization_transform=InvestmentPerChannelTransform(),\n    options={\"disp\": True},\n)\n\nX_opt_reparam = budget_optimizer_reparam.optimize(\n    model=model,\n    X=X,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -738733788.63574\n            Iterations: 13\n            Function evaluations: 42\n            Gradient evaluations: 12\n\n\n\nBaseline vs. optimized spend\n\ny_pred_opt_reparam = model.predict(X=X_opt_reparam, fh=horizon)\n\nfig, ax = plot_spend_comparison(\n    X,\n    X_opt_reparam,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    horizon,\n)\n\nkpi_gain = y_pred_opt_reparam.sum() / y_pred_baseline.sum() - 1\nfig.suptitle(f\"KPI gain: +{kpi_gain:.2%}\", fontsize=16, weight=\"bold\", y=1.02)\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n\n\n\n\n\n\n1.2.3. Minimizing budget to reach a target\nWe can also change the objective to find the minimum investment required to achieve a specific KPI target. Let’s say we want a 30% increase in KPI compared to 2003.\n\nfrom prophetverse.budget_optimization import (\n    MinimizeBudget,\n    MinimumTargetResponse,\n)\n\ntarget = y.loc[\"2003-12\"].sum() * 1.30\n\nbudget_optimizer_min = BudgetOptimizer(\n    objective=MinimizeBudget(),\n    constraints=[MinimumTargetResponse(target_response=target, constraint_type=\"eq\")],\n    options={\"disp\": True, \"maxiter\" : 300},\n)\n\nX0 = X.copy()\nX_opt_min = budget_optimizer_min.optimize(\n    model=model,\n    X=X0,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 3796555.321062599\n            Iterations: 201\n            Function evaluations: 204\n            Gradient evaluations: 201\n\n\n\nBudget and prediction comparison\n\nplot_spend_comparison(\n    X0,\n    X_opt_min,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    indexer=horizon,\n)\nplt.show()\n\ny_pred_baseline_min = model.predict(X=X0, fh=horizon)\ny_pred_opt_min = model.predict(X=X_opt_min, fh=horizon)\n\nprint(\n    f\"MMM Predictions \\n\",\n    f\"Baseline KPI: {y_pred_baseline_min.sum()/1e9:.2f} B \\n\",\n    f\"Optimized KPI: {y_pred_opt_min.sum()/1e9:.2f} B \\n\",\n    f\"Target KPI: {target/1e9:.2f} B \\n\",\n    \"Baseline spend: \",\n    X0.loc[horizon, [\"ad_spend_search\", \"ad_spend_social_media\"]].sum().sum(),\n    \"\\n\",\n    \"Optimized spend: \",\n    X_opt_min.loc[horizon, [\"ad_spend_search\", \"ad_spend_social_media\"]].sum().sum(),\n    \"\\n\",\n)\n\n\n\n\n\n\n\n\nMMM Predictions \n Baseline KPI: 0.73 B \n Optimized KPI: 0.97 B \n Target KPI: 0.97 B \n Baseline spend:  1250679.3427392421 \n Optimized spend:  3796555.3210625993",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "mmm/budget_allocation.html#setting-up-the-problem-for-panel-data",
    "href": "mmm/budget_allocation.html#setting-up-the-problem-for-panel-data",
    "title": "Budget Optimization for Single and Multiple Time Series",
    "section": "2.1. Setting Up the Problem for Panel Data",
    "text": "2.1. Setting Up the Problem for Panel Data\nThe main difference is that for panel data, we use a multi-index DataFrame, following sktime conventions.\n\n2.1.1. Load synthetic panel data\n\nfrom prophetverse.datasets._mmm.dataset1_panel import get_dataset\n\ny_panel, X_panel, lift_tests_panel, true_components_panel, fitted_model_panel = get_dataset()\n\ny_panel\n\n\n\n\n\n\n\n\n\n0\n\n\ngroup\ndate\n\n\n\n\n\na\n2000-01-01\n1.120218e+07\n\n\n2000-01-02\n1.146048e+07\n\n\n2000-01-03\n1.156324e+07\n\n\n2000-01-04\n1.161396e+07\n\n\n2000-01-05\n1.162758e+07\n\n\n...\n...\n...\n\n\nb\n2004-12-28\n2.473478e+07\n\n\n2004-12-29\n2.718986e+07\n\n\n2004-12-30\n2.554932e+07\n\n\n2004-12-31\n2.343510e+07\n\n\n2005-01-01\n2.078426e+07\n\n\n\n\n3656 rows × 1 columns\n\n\n\n\n\n2.1.2. Utility plotting functions for panel data\nWe’ll define a new plotting function to handle the multi-indexed data.\n\n\nCode\ndef plot_spend_comparison_panel(\n    X_baseline,\n    X_optimized,\n    channels,\n    indexer,\n    *,\n    baseline_title=\"Baseline Spend: Pre-Optimization\",\n    optimized_title=\"Optimized Spend: Maximizing KPI\",\n    figsize=(8, 4),\n):\n    series_idx = X_baseline.index.droplevel(-1).unique().tolist()\n    fig, axs = plt.subplots(len(series_idx), 2, figsize=figsize, squeeze=False)\n\n    for i, series in enumerate(series_idx):\n        _X_baseline = X_baseline.loc[series]\n        _X_optimized = X_optimized.loc[series]\n        ax_row = axs[i]\n        _X_baseline.loc[indexer, channels].plot(ax=ax_row[0], linewidth=2)\n        _X_optimized.loc[indexer, channels].plot(\n            ax=ax_row[1], linewidth=2, linestyle=\"--\"\n        )\n\n        ax_row[0].set_title(f\"{series}: {baseline_title}\", fontsize=14, weight=\"bold\")\n        ax_row[1].set_title(f\"{series}: {optimized_title}\", fontsize=14, weight=\"bold\")\n\n        for a in ax_row:\n            a.set_ylabel(\"Spend\")\n            a.set_xlabel(\"Date\")\n            a.legend(loc=\"upper right\", frameon=True)\n            a.grid(axis=\"x\", visible=False)\n            a.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n            a.xaxis.set_major_formatter(mdates.DateFormatter(\"%b\"))\n\n        y_max = max(\n            _X_baseline.loc[indexer, channels].max().max(),\n            _X_optimized.loc[indexer, channels].max().max(),\n        )\n        for a in ax_row:\n            a.set_ylim(0, y_max * 1.05)\n\n    plt.tight_layout()\n    return fig, axs",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "mmm/budget_allocation.html#budget-optimization-for-panel-data",
    "href": "mmm/budget_allocation.html#budget-optimization-for-panel-data",
    "title": "Budget Optimization for Single and Multiple Time Series",
    "section": "2.2. Budget Optimization for Panel Data",
    "text": "2.2. Budget Optimization for Panel Data\n\n2.2.1. Maximizing a KPI\nBy default, BudgetOptimizer will optimize the daily spend for each channel for each series.\n\nbudget_optimizer_panel = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[TotalBudgetConstraint()],\n    options={\"disp\": True, \"maxiter\": 1000},\n)\n\nX_opt_panel = budget_optimizer_panel.optimize(\n    model=fitted_model_panel,\n    X=X_panel,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -1723014730.7615\n            Iterations: 198\n            Function evaluations: 198\n            Gradient evaluations: 197\n\n\n\nBaseline vs. optimized spend\n\ny_pred_baseline_panel = fitted_model_panel.predict(X=X_panel, fh=horizon)\ny_pred_opt_panel = fitted_model_panel.predict(X=X_opt_panel, fh=horizon)\n\nfig, ax = plot_spend_comparison_panel(\n    X_panel,\n    X_opt_panel,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    horizon,\n)\n\nkpi_gain = y_pred_opt_panel.values.sum() / y_pred_baseline_panel.values.sum() - 1\nfig.suptitle(f\"Total KPI gain: +{kpi_gain:.2%}\", fontsize=16, weight=\"bold\", y=1.03)\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n\n\n\n\n\n\n2.2.2. Reparametrization for Panel Data\nWith panel data, we have more reparametrization options.\n\nOptimizing channel share (globally)\nThis optimizes the share of budget for each channel, keeping the total investment and spending pattern fixed. The channel shares are the same across all series.\n\nfrom prophetverse.budget_optimization import InvestmentPerChannelTransform\n\nbudget_optimizer_panel_ch = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[TotalBudgetConstraint()],\n    parametrization_transform=InvestmentPerChannelTransform(),\n    options={\"disp\": True},\n)\n\nX_opt_panel_ch = budget_optimizer_panel_ch.optimize(\n    model=fitted_model_panel,\n    X=X_panel,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n# You can plot the results using plot_spend_comparison_panel\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -1674672737.6774194\n            Iterations: 13\n            Function evaluations: 13\n            Gradient evaluations: 13\n\n\n\n\nOptimizing investment per series\nThis keeps the channel shares fixed within each series but optimizes the allocation of the total budget across the different series.\n\nfrom prophetverse.budget_optimization.parametrization_transformations import InvestmentPerSeries\n\nbudget_optimizer_panel_s = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[TotalBudgetConstraint()],\n    parametrization_transform=InvestmentPerSeries(),\n    options={\"disp\": True},\n)\n\nX_opt_panel_s = budget_optimizer_panel_s.optimize(\n    model=fitted_model_panel,\n    X=X_panel,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n# You can plot the results using plot_spend_comparison_panel\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -1680449341.6035411\n            Iterations: 13\n            Function evaluations: 13\n            Gradient evaluations: 13\n\n\n\n\nOptimizing share per channel and series\nThis is the most granular reparametrization, optimizing the share of budget for each channel within each series.\n\nfrom prophetverse.budget_optimization.parametrization_transformations import InvestmentPerChannelAndSeries\n\nbudget_optimizer_panel_cs = BudgetOptimizer(\n    objective=MaximizeKPI(),\n    constraints=[TotalBudgetConstraint()],\n    parametrization_transform=InvestmentPerChannelAndSeries(),\n    options={\"disp\": True},\n)\n\nX_opt_panel_cs = budget_optimizer_panel_cs.optimize(\n    model=fitted_model_panel,\n    X=X_panel,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n# You can plot the results using plot_spend_comparison_panel\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -1717879098.745954\n            Iterations: 27\n            Function evaluations: 26\n            Gradient evaluations: 26\n\n\n\n\n\n2.2.3. Minimizing budget to reach a target with Panel Data\nLet’s find the minimum budget to achieve a 20% KPI increase across all series.\n\ntarget_panel = y_panel.loc[pd.IndexSlice[:, horizon],].values.sum() * 1.2\n\nbudget_optimizer_min_panel = BudgetOptimizer(\n    objective=MinimizeBudget(),\n    constraints=[MinimumTargetResponse(target_response=target_panel, constraint_type=\"eq\")],\n    options={\"disp\": True, \"maxiter\": 300},\n)\n\nX0_panel = X_panel.copy()\nX_opt_min_panel = budget_optimizer_min_panel.optimize(\n    model=fitted_model_panel,\n    X=X0_panel,\n    horizon=horizon,\n    columns=[\"ad_spend_search\", \"ad_spend_social_media\"],\n)\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 7755666.679734472\n            Iterations: 285\n            Function evaluations: 287\n            Gradient evaluations: 285\n\n\n\nBudget and prediction comparison\n\nplot_spend_comparison_panel(\n    X0_panel,\n    X_opt_min_panel,\n    [\"ad_spend_search\", \"ad_spend_social_media\"],\n    indexer=horizon,\n)\nplt.show()\n\ny_pred_baseline_min_panel = fitted_model_panel.predict(X=X0_panel, fh=horizon)\ny_pred_opt_min_panel = fitted_model_panel.predict(X=X_opt_min_panel, fh=horizon)\n\nprint(\n    f\"MMM Predictions \\n\",\n    f\"Baseline KPI: {y_pred_baseline_min_panel.values.sum()/1e9:.2f} B \\n\",\n    f\"Optimized KPI: {y_pred_opt_min_panel.values.sum()/1e9:.2f} B \\n\",\n    f\"Target KPI: {target_panel/1e9:.2f} B \\n\",\n    \"Baseline spend: \",\n    X0_panel.loc[\n        pd.IndexSlice[:, horizon], [\"ad_spend_search\", \"ad_spend_social_media\"]\n    ]\n    .sum()\n    .sum(),\n    \"\\n\",\n    \"Optimized spend: \",\n    X_opt_min_panel.loc[\n        pd.IndexSlice[:, horizon], [\"ad_spend_search\", \"ad_spend_social_media\"]\n    ]\n    .sum()\n    .sum(),\n    \"\\n\",\n)\n\n\n\n\n\n\n\n\nMMM Predictions \n Baseline KPI: 1.63 B \n Optimized KPI: 1.96 B \n Target KPI: 1.96 B \n Baseline spend:  4179163.3068621266 \n Optimized spend:  7755666.679734475",
    "crumbs": [
      "Budget Optimization"
    ]
  },
  {
    "objectID": "mmm/saturation_and_adstock.html",
    "href": "mmm/saturation_and_adstock.html",
    "title": "Saturation and Adstock",
    "section": "",
    "text": "There are many available effects available by default on Prophetverse. To get a glimpse of them, you can query all_objects from skbase.lookup:\nfrom skbase.lookup import all_objects\nfrom prophetverse.effects import BaseEffect\n\nall_objects(object_types=[BaseEffect], package_name=\"prophetverse\", as_dataframe=True)\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n\n\n\n\n\n\nname\nobject\n\n\n\n\n0\nBetaTargetLikelihood\n&lt;class 'prophetverse.effects.target.univariate...\n\n\n1\nChainedEffects\n&lt;class 'prophetverse.effects.chain.ChainedEffe...\n\n\n2\nConstant\n&lt;class 'prophetverse.effects.constant.Constant'&gt;\n\n\n3\nCoupledExactLikelihood\n&lt;class 'prophetverse.effects.coupled.CoupledEx...\n\n\n4\nExactLikelihood\n&lt;class 'prophetverse.effects.exact_likelihood....\n\n\n5\nFlatTrend\n&lt;class 'prophetverse.effects.trend.flat.FlatTr...\n\n\n6\nForward\n&lt;class 'prophetverse.effects.forward.Forward'&gt;\n\n\n7\nGammaTargetLikelihood\n&lt;class 'prophetverse.effects.target.univariate...\n\n\n8\nGeometricAdstockEffect\n&lt;class 'prophetverse.effects.adstock.Geometric...\n\n\n9\nHillEffect\n&lt;class 'prophetverse.effects.hill.HillEffect'&gt;\n\n\n10\nHurdleTargetLikelihood\n&lt;class 'prophetverse.effects.target.hurdle.Hur...\n\n\n11\nIdentity\n&lt;class 'prophetverse.effects.identity.Identity'&gt;\n\n\n12\nIgnoreInput\n&lt;class 'prophetverse.effects.ignore_input.Igno...\n\n\n13\nLiftExperimentLikelihood\n&lt;class 'prophetverse.effects.lift_likelihood.L...\n\n\n14\nLinearEffect\n&lt;class 'prophetverse.effects.linear.LinearEffe...\n\n\n15\nLinearFourierSeasonality\n&lt;class 'prophetverse.effects.fourier.LinearFou...\n\n\n16\nLinearProxyLikelihood\n&lt;class 'prophetverse.effects.proxy_likelihood....\n\n\n17\nLogEffect\n&lt;class 'prophetverse.effects.log.LogEffect'&gt;\n\n\n18\nMichaelisMentenEffect\n&lt;class 'prophetverse.effects.michaelis_menten....\n\n\n19\nMultiplyEffects\n&lt;class 'prophetverse.effects.operations.Multip...\n\n\n20\nMultivariateNormal\n&lt;class 'prophetverse.effects.target.multivaria...\n\n\n21\nNegativeBinomialTargetLikelihood\n&lt;class 'prophetverse.effects.target.univariate...\n\n\n22\nNormalTargetLikelihood\n&lt;class 'prophetverse.effects.target.univariate...\n\n\n23\nPanelBHLinearEffect\n&lt;class 'prophetverse.effects.linear.PanelBHLin...\n\n\n24\nPiecewiseLinearTrend\n&lt;class 'prophetverse.effects.trend.piecewise.P...\n\n\n25\nPiecewiseLogisticTrend\n&lt;class 'prophetverse.effects.trend.piecewise.P...\n\n\n26\nSumEffects\n&lt;class 'prophetverse.effects.operations.SumEff...\n\n\n27\nTargetLikelihood\n&lt;class 'prophetverse.effects.target.univariate...\n\n\n28\nWeibullAdstockEffect\n&lt;class 'prophetverse.effects.adstock.WeibullAd...\nNot bad, right? The best part is that you can combine them in a flexible way to create your own custom effects, and even create your own effects if you want to. Below, we showcase some interesting combinations you can use, and how to visualize their prior predictive distribution.",
    "crumbs": [
      "Saturation and Adstock"
    ]
  },
  {
    "objectID": "mmm/saturation_and_adstock.html#loading-the-dataset",
    "href": "mmm/saturation_and_adstock.html#loading-the-dataset",
    "title": "Saturation and Adstock",
    "section": "Loading the dataset",
    "text": "Loading the dataset\nWe load a synthetic dataset with daily frequency, to use in this example.\n\nimport matplotlib.pyplot as plt\nfrom prophetverse.datasets._mmm.dataset1 import get_dataset\n\ny, X = get_dataset(return_y_and_X_only=True)\n\n\ny.head()\n\n2000-01-01    10815512.0\n2000-01-02    11120677.0\n2000-01-03    11260387.0\n2000-01-04    11322533.0\n2000-01-05    11321180.0\nFreq: D, dtype: float32\n\n\n\nfig, ax = plt.subplots(figsize=(10, 4))\nX.plot.line(ax=ax)\nfig.show()",
    "crumbs": [
      "Saturation and Adstock"
    ]
  },
  {
    "objectID": "mmm/saturation_and_adstock.html#saturations",
    "href": "mmm/saturation_and_adstock.html#saturations",
    "title": "Saturation and Adstock",
    "section": "Saturations",
    "text": "Saturations\nThe most common saturation functions are Hill, Michaelis-Menten, and Logarithmic. They are all available in Prophetverse:\n\nfrom prophetverse.effects import MichaelisMentenEffect, HillEffect, LogEffect\nimport numpyro\nfrom numpyro import distributions as dist\n\nmm_saturation = MichaelisMentenEffect(\n    effect_mode=\"additive\",\n    max_effect_prior=dist.HalfNormal(0.4),\n    half_saturation_prior=dist.HalfNormal(40000),\n)\n\nlog_saturation = LogEffect(\n    effect_mode=\"additive\",\n    scale_prior=dist.HalfNormal(0.2),\n    rate_prior=dist.HalfNormal(0.0001),\n)\n\nhill_saturation = HillEffect(\n    effect_mode=\"additive\",\n    max_effect_prior=dist.HalfNormal(0.4),\n    half_max_prior=dist.HalfNormal(40000),\n    slope_prior=dist.InverseGamma(4, 2),\n)\n\nmm_saturation\n\nMichaelisMentenEffect(effect_mode='additive',\n                      half_saturation_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a945dd150 with batch shape () and event shape ()&gt;,\n                      max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a945db790 with batch shape () and event shape ()&gt;)Please rerun this cell to show the HTML repr or trust the notebook.MichaelisMentenEffectMichaelisMentenEffect(effect_mode='additive',\n                      half_saturation_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a945dd150 with batch shape () and event shape ()&gt;,\n                      max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a945db790 with batch shape () and event shape ()&gt;)\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince Prophetverse and all effects are estimators, you can set and inspect the hyperparameters through get_params and set_params methods, and of course plug them into an automated hyperparameter tuning workflow. Checkout our Hyperparameter Tuning tutorial for more details.\n\n\n\nVisualizing prior predictive distributions\nTo get a better sense of how these effects behave, we can visualize their prior predictive distributions using the plot_prior_predictive utility from prophetverse.utils.plotting.\nWe first visualize how the output of the effect behaves with respect to the input:\n\nfrom prophetverse.utils.plotting import plot_prior_predictive\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plot_prior_predictive(\n    mm_saturation,\n    X=X[[\"ad_spend_search\"]],\n    mode=\"ad_spend_search\",\n    matplotlib_kwargs=dict(figsize=(6, 3)),\n)\n\nfig.show()\n\n\n\n\n\n\n\n\nAnd through time, when applied to a time series:\n\nfig, ax = plot_prior_predictive(\n    mm_saturation,\n    X=X[[\"ad_spend_search\"]],\n    mode=\"time\",\n    matplotlib_kwargs=dict(figsize=(6, 3)),\n)\nfig.show()",
    "crumbs": [
      "Saturation and Adstock"
    ]
  },
  {
    "objectID": "mmm/saturation_and_adstock.html#adstock",
    "href": "mmm/saturation_and_adstock.html#adstock",
    "title": "Saturation and Adstock",
    "section": "Adstock",
    "text": "Adstock\nTo model the delayed effect of advertising on sales, we can use adstock transformations.\nLet’s visualize first how Weibull Adstock behaves when applied to the input:\n\nfrom prophetverse import GeometricAdstockEffect, WeibullAdstockEffect\n\nadstock = WeibullAdstockEffect(\n    scale_prior=dist.HalfNormal(10),\n    concentration_prior=dist.HalfNormal(2),\n)\n\nfig, ax = plot_prior_predictive(\n    adstock,\n    X=X[[\"ad_spend_search\"]].iloc[100:180],\n    mode=\"time\",\n    matplotlib_kwargs=dict(figsize=(6, 3)),\n)\nX[[\"ad_spend_search\"]].iloc[100:180].plot(ax=ax, color=\"tab:orange\")\n\nfor line, label in zip(ax.lines, [\"After Adstock\", \"Input Data\"]):\n    line.set_label(label)\nax.legend()\nfig.show()\n\n\n\n\n\n\n\n\nAnd how Geometric Adstock behaves:\n\nadstock = GeometricAdstockEffect(\n    decay_prior=dist.Beta(10, 10),\n    # Set normalize=True if you want an impulse of size 1 to have total\n    # cumulative mass 1 over infinite horizon (weights sum to 1)\n    normalize=True,\n)\n\nfig, ax = plot_prior_predictive(\n    adstock,\n    X=X[[\"ad_spend_search\"]],\n    mode=\"time\",\n    matplotlib_kwargs=dict(figsize=(6, 3)),\n)\nX[[\"ad_spend_search\"]].plot(ax=ax, color=\"tab:orange\")\n\n\nax.set(xlim=(\"2000-04-15\", \"2000-07-15\"), ylim=(80_000, 110_000))\n\nfor line, label in zip(ax.lines, [\"After Adstock\", \"Input Data\"]):\n    line.set_label(label)\nax.legend()\nfig.show()",
    "crumbs": [
      "Saturation and Adstock"
    ]
  },
  {
    "objectID": "mmm/saturation_and_adstock.html#combining-saturation-and-adstock",
    "href": "mmm/saturation_and_adstock.html#combining-saturation-and-adstock",
    "title": "Saturation and Adstock",
    "section": "Combining Saturation and Adstock",
    "text": "Combining Saturation and Adstock\nAnd you can of course compose them! Use ChainedEffects to combine multiple effects in a sequence. Below, we combine Michaelis-Menten saturation with Weibull Adstock. You\n\nfrom prophetverse import ChainedEffects\n\nsaturation_with_adstock = ChainedEffects(\n    steps=[\n        (\n            \"adstock_on_investment\",\n            WeibullAdstockEffect(\n                scale_prior=dist.HalfNormal(2),\n                concentration_prior=dist.HalfNormal(2),\n            ),\n        ),\n        (\"saturation\", mm_saturation),\n        (\n            \"adstock_on_output\",\n            WeibullAdstockEffect(\n                scale_prior=dist.HalfNormal(2),\n                concentration_prior=dist.HalfNormal(2),\n            ),\n        ),\n    ]\n)\n\nsaturation_with_adstock\n\nChainedEffects(steps=[('adstock_on_investment',\n                       WeibullAdstockEffect(concentration_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a94464410 with batch shape () and event shape ()&gt;,\n                                            scale_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a94338b10 with batch shape () and event shape ()&gt;)),\n                      ('saturation',\n                       MichaelisMentenEffect(effect_mode='a...\n                                             max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a945db790 with batch shape () and event shape ()&gt;)),\n                      ('adstock_on_output',\n                       WeibullAdstockEffect(concentration_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a944680d0 with batch shape () and event shape ()&gt;,\n                                            scale_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a70db1690 with batch shape () and event shape ()&gt;))])Please rerun this cell to show the HTML repr or trust the notebook.ChainedEffectsChainedEffects(steps=[('adstock_on_investment',\n                       WeibullAdstockEffect(concentration_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a94464410 with batch shape () and event shape ()&gt;,\n                                            scale_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a94338b10 with batch shape () and event shape ()&gt;)),\n                      ('saturation',\n                       MichaelisMentenEffect(effect_mode='a...\n                                             max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a945db790 with batch shape () and event shape ()&gt;)),\n                      ('adstock_on_output',\n                       WeibullAdstockEffect(concentration_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a944680d0 with batch shape () and event shape ()&gt;,\n                                            scale_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a70db1690 with batch shape () and event shape ()&gt;))])WeibullAdstockEffectWeibullAdstockEffect(concentration_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a94464410 with batch shape () and event shape ()&gt;,\n                     scale_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a94338b10 with batch shape () and event shape ()&gt;)MichaelisMentenEffectMichaelisMentenEffect(effect_mode='additive',\n                      half_saturation_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a945dd150 with batch shape () and event shape ()&gt;,\n                      max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a945db790 with batch shape () and event shape ()&gt;)WeibullAdstockEffectWeibullAdstockEffect(concentration_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a944680d0 with batch shape () and event shape ()&gt;,\n                     scale_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f4a70db1690 with batch shape () and event shape ()&gt;)\n\n\n\nfig, ax = plot_prior_predictive(\n    mm_saturation,\n    X=X[[\"ad_spend_search\"]],\n    mode=\"ad_spend_search\",\n    matplotlib_kwargs=dict(figsize=(6, 3)),\n)\nfig.show()",
    "crumbs": [
      "Saturation and Adstock"
    ]
  },
  {
    "objectID": "mmm/saturation_and_adstock.html#extra-visualizing-the-impulse-response-of-adstock",
    "href": "mmm/saturation_and_adstock.html#extra-visualizing-the-impulse-response-of-adstock",
    "title": "Saturation and Adstock",
    "section": "Extra: Visualizing the impulse response of adstock",
    "text": "Extra: Visualizing the impulse response of adstock\nThe impulse response is one of the clearest ways to understand what adstock is doing.\nBy feeding in a single one-day spike of spend (an impulse), we can isolate the effect of the adstock transformation without confounding from the full time series.\n\nIsolates carryover → shows exactly how much of today’s spend impacts tomorrow and the following days.\n\nBuilds intuition → a short tail means fast decay; a long tail means the campaign keeps influencing for weeks.\n\nCompares models → geometric adstock has an exponential-like drop, while Weibull can flexibly capture rises before decay.\n\nBusiness meaning → directly answers “If I invest 1 unit today, what incremental impact should I expect tomorrow, next week, and beyond?”\n\nIn short, the impulse response is the “fingerprint” of an adstock model — it makes lag and memory structures visible and interpretable.\n\nimport pandas as pd\n\n\ndef plot_impulse_response(adstock):\n    X_impulse = pd.DataFrame(\n        index=pd.period_range(start=\"2021-01-01\", periods=100, freq=\"D\"),\n        data=[0] * 50 + [1] + [0] * 49,\n    ).astype(float)\n\n    fig, ax = plot_prior_predictive(\n        adstock,\n        X=X_impulse,\n        mode=\"time\",\n        matplotlib_kwargs=dict(figsize=(6, 3)),\n    )\n    X_impulse.plot(ax=ax, color=\"tab:orange\")\n    for line, label in zip(ax.lines, [\"After Adstock\", \"Input Data\"]):\n        line.set_label(label)\n    ax.legend()\n    fig.show()\n\n\nadstock = GeometricAdstockEffect(\n    decay_prior=dist.Delta(0.7),\n    normalize=True,\n)\nplot_impulse_response(adstock)\n\n\n\n\n\n\n\n\nAnd for Weibull Adstock:\n\nadstock = WeibullAdstockEffect(\n    scale_prior=dist.Delta(5),\n    concentration_prior=dist.Delta(1.5),\n)\nplot_impulse_response(adstock)",
    "crumbs": [
      "Saturation and Adstock"
    ]
  },
  {
    "objectID": "mmm/fitting_and_calibration.html",
    "href": "mmm/fitting_and_calibration.html",
    "title": "Forecasting, Calibration, and Unified Marketing Measurement",
    "section": "",
    "text": "In this tutorial, we walk through the lifecycle of a modern Marketing Mix Model (MMM), from time-series forecasting to incorporating causal evidence like lift tests and attribution.\nYou will learn:\n👉 Why this matters: MMMs are foundational for budget allocation. But good predictions are not enough — we need credible effect estimates to make real-world decisions.\nLet’s get started!\nSetting up some libraries, float64 precision, and plot style:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpyro\nimport numpyro.distributions as dist\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nnumpyro.enable_x64()\nfrom prophetverse.datasets._mmm.dataset1 import get_dataset\n\ny, X, lift_tests, true_components, _ = get_dataset()\nlift_test_search, lift_test_social = lift_tests\n\nprint(f\"y shape: {y.shape}, X shape: {X.shape}\")\nX.head()\n\ny shape: (1828,), X shape: (1828, 2)\n\n\n\n\n\n\n\n\n\nad_spend_search\nad_spend_social_media\n\n\n\n\n2000-01-01\n89076.191178\n98587.488958\n\n\n2000-01-02\n88891.993106\n99066.321168\n\n\n2000-01-03\n89784.955064\n97334.106903\n\n\n2000-01-04\n89931.220681\n101747.300585\n\n\n2000-01-05\n89184.319596\n93825.221809",
    "crumbs": [
      "Fitting, calibrating and Unified Marketing Measurement"
    ]
  },
  {
    "objectID": "mmm/fitting_and_calibration.html#part-1-forecasting-with-adstock-saturation-effects",
    "href": "mmm/fitting_and_calibration.html#part-1-forecasting-with-adstock-saturation-effects",
    "title": "Forecasting, Calibration, and Unified Marketing Measurement",
    "section": "Part 1: Forecasting with Adstock & Saturation Effects",
    "text": "Part 1: Forecasting with Adstock & Saturation Effects\nHere we’ll build a time-series forecasting model that includes:\n\nTrend and seasonality\n\nLagged media effects (Adstock)\n\nDiminishing returns (Saturation / Hill curves)\n\n🔎 Why this matters:\nRaw spend is not immediately effective, and it doesn’t convert linearly.\nCapturing these dynamics is essential to make ROI estimates realistic.\n\nfrom prophetverse.effects import (\n    PiecewiseLinearTrend,\n    LinearFourierSeasonality,\n    ChainedEffects,\n    GeometricAdstockEffect,\n    HillEffect,\n)\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.engine.optimizer import LBFGSSolver\n\nyearly = (\n    \"yearly_seasonality\",\n    LinearFourierSeasonality(freq=\"D\", sp_list=[365.25], fourier_terms_list=[5], prior_scale=0.1, effect_mode=\"multiplicative\"),\n    None,\n)\n\nweekly = (\n    \"weekly_seasonality\",\n    LinearFourierSeasonality(freq=\"D\", sp_list=[7], fourier_terms_list=[3], prior_scale=0.05, effect_mode=\"multiplicative\"),\n    None,\n)\n\nhill = HillEffect(\n    half_max_prior=dist.HalfNormal(1),\n    slope_prior=dist.InverseGamma(2, 1),\n    max_effect_prior=dist.HalfNormal(1),\n    effect_mode=\"additive\",\n    input_scale=1e6,\n)\n\nchained_search = (\n    \"ad_spend_search\",\n    ChainedEffects([(\"adstock\", GeometricAdstockEffect()), (\"saturation\", hill)]),\n    \"ad_spend_search\",\n)\nchained_social = (\n    \"ad_spend_social_media\",\n    ChainedEffects([(\"adstock\", GeometricAdstockEffect()), (\"saturation\", hill)]),\n    \"ad_spend_social_media\",\n)\n\n\nbaseline_model = Prophetverse(\n    trend=PiecewiseLinearTrend(changepoint_interval=100),\n    exogenous_effects=[yearly, weekly, chained_search, chained_social],\n    inference_engine=MAPInferenceEngine(\n        num_steps=5000,\n        optimizer=LBFGSSolver(memory_size=300, max_linesearch_steps=300),\n    ),\n)\nbaseline_model.fit(y=y, X=X)\n\nProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 Chained...\n                                                                   max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n                                                                   slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;))]),\n                                 'ad_spend_social_media')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 Chained...\n                                                                   max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n                                                                   slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;))]),\n                                 'ad_spend_social_media')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=100)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=0.1, sp_list=[365.25])LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[3],\n                         freq='D', prior_scale=0.05, sp_list=[7])ad_spend_searchChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;)ad_spend_social_mediaChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine(num_steps=5000,\n                   optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                         memory_size=300))\n\n\n\ny_pred = baseline_model.predict(X=X, fh=X.index)\n\nplt.figure(figsize=(8, 4))\ny.plot(label=\"Observed\")\ny_pred.plot(label=\"Predicted\")\nplt.title(\"In-Sample Forecast: Observed vs Predicted\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n1.1 Component-Level Diagnostics\nWith predict_components, we can obtain the model’s components.\n\ny_pred_components = baseline_model.predict_components(X=X, fh=X.index)\ny_pred_components.head()\n\n\n\n\n\n\n\n\nad_spend_search\nad_spend_social_media\nmean\nobs\ntrend\nweekly_seasonality\nyearly_seasonality\n\n\n\n\n2000-01-01\n0.073604\n9.938440e+06\n1.013309e+07\n1.011244e+07\n209634.497522\n9025.631631\n-24007.411623\n\n\n2000-01-02\n0.076372\n1.076251e+07\n1.095672e+07\n1.091247e+07\n218456.794123\n354.701181\n-24601.520915\n\n\n2000-01-03\n0.077417\n1.099979e+07\n1.120479e+07\n1.126304e+07\n227279.090724\n2828.076884\n-25111.506395\n\n\n2000-01-04\n0.077880\n1.113381e+07\n1.133319e+07\n1.129276e+07\n236101.387324\n-11188.123242\n-25532.228540\n\n\n2000-01-05\n0.078070\n1.113023e+07\n1.134782e+07\n1.129920e+07\n244923.683925\n-1479.166867\n-25858.762580\n\n\n\n\n\n\n\nIn a real use-casee, you would not have access to the ground truth of the components. We use them here to show how the model behaves, and how incorporing extra information can improve it.\n\nfig, axs = plt.subplots(4, 1, figsize=(8, 12), sharex=True)\nfor i, name in enumerate(\n    [\"trend\", \"yearly_seasonality\", \"ad_spend_search\", \"ad_spend_social_media\"]\n):\n    true_components[name].plot(ax=axs[i], label=\"True\", color=\"black\")\n    y_pred_components[name].plot(ax=axs[i], label=\"Estimated\")\n    axs[i].set_title(name)\n    axs[i].legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.2 Backtesting with Cross-Validation\nWe use rolling-window CV to assess out-of-sample accuracy using MAPE.\n🧠 Caution: Low error ≠ correct attribution. But high error often indicates a bad model.\n\nfrom sktime.split import ExpandingWindowSplitter\nfrom sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\nfrom sktime.forecasting.model_evaluation import evaluate\n\nmetric = MeanAbsolutePercentageError()\ncv = ExpandingWindowSplitter(\n    initial_window=365 * 3, step_length=180, fh=list(range(1, 180))\n)\ncv_results = evaluate(\n    forecaster=baseline_model, y=y, X=X, cv=cv, scoring=metric, return_data=True\n)\ncv_results\n\n\n\n\n\n\n\n\ntest_MeanAbsolutePercentageError\nfit_time\npred_time\nlen_train_window\ncutoff\ny_train\ny_test\ny_pred\n\n\n\n\n0\n0.066920\n48.902712\n0.881904\n1095\n2002-12-30\n2000-01-01 1.081551e+07 2000-01-02 1.112...\n2002-12-31 1.306520e+07 2003-01-01 1.429...\n2002-12-31 1.116013e+07 2003-01-01 1.167...\n\n\n1\n0.059334\n20.530493\n0.805120\n1275\n2003-06-28\n2000-01-01 1.081551e+07 2000-01-02 1.112...\n2003-06-29 1.725815e+07 2003-06-30 1.696...\n2003-06-29 1.622098e+07 2003-06-30 1.658...\n\n\n2\n0.052217\n22.043921\n0.778676\n1455\n2003-12-25\n2000-01-01 1.081551e+07 2000-01-02 1.112...\n2003-12-26 2.511414e+07 2003-12-27 2.496...\n2003-12-26 2.437670e+07 2003-12-27 2.476...\n\n\n3\n0.057910\n54.754551\n0.790010\n1635\n2004-06-22\n2000-01-01 1.081551e+07 2000-01-02 1.112...\n2004-06-23 2.913561e+07 2004-06-24 2.878...\n2004-06-23 3.056426e+07 2004-06-24 3.018...\n\n\n\n\n\n\n\nThe average error across folds is:\n\ncv_results[\"test_MeanAbsolutePercentageError\"].mean()\n\nnp.float64(0.05909504316526455)\n\n\nWe can visualize them by iterating the dataframe:\n\nfor idx, row in cv_results.iterrows():\n    plt.figure(figsize=(8, 2))\n    observed = pd.concat([row[\"y_train\"].iloc[-100:], row[\"y_test\"]])\n    observed.plot(label=\"Observed\", color=\"black\")\n    row[\"y_pred\"].plot(label=\"Prediction\")\n    plt.title(f\"Fold {idx + 1} – MAPE: {row['test_MeanAbsolutePercentageError']:.2%}\")\n    plt.legend()\n    plt.show()\n    if idx &gt; 3:\n        break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.3 Saturation Curves\nThese curves show diminishing marginal effect as spend increases.\n🔍 Insight: This shape helps guide budget allocation decisions (e.g. where additional spend will have little return).\nNote how the model captures a saturation effect, but it is still far from the correct shape.\nThis is why, in many situations, you will need calibration to correct the model’s behavior. This is what we will do in the next section.\n\nfig, axs = plt.subplots(figsize=(8, 6), nrows=1, ncols=2)\n\nfor ax, channel in zip(axs, [\"ad_spend_search\", \"ad_spend_social_media\"]):\n    ax.scatter(\n        X[channel],\n        y_pred_components[channel],\n        alpha=0.6,\n        label=channel,\n    )\n    ax.scatter(\n        X[channel],\n        true_components[channel],\n        color=\"black\",\n        label=\"True Effect\",\n    )\n    ax.set(\n        xlabel=\"Daily Spend\",\n        ylabel=\"Incremental Effect\",\n        title=f\"{channel} - Saturation Curve\",\n    )\n    ax.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Fitting, calibrating and Unified Marketing Measurement"
    ]
  },
  {
    "objectID": "mmm/fitting_and_calibration.html#part-2-calibration-with-causal-evidence",
    "href": "mmm/fitting_and_calibration.html#part-2-calibration-with-causal-evidence",
    "title": "Forecasting, Calibration, and Unified Marketing Measurement",
    "section": "Part 2: Calibration with Causal Evidence",
    "text": "Part 2: Calibration with Causal Evidence\nTime-series alone cannot disentangle correlated channels.\nWe integrate lift tests (local experiments) and attribution models (high-resolution signal) to correct this.\n\nlift_test_search\n\n\n\n\n\n\n\n\nlift\nx_start\nx_end\n\n\n\n\n2000-09-04\n0.986209\n88991.849098\n88616.674279\n\n\n2003-07-17\n1.602655\n25147.476284\n30137.721832\n\n\n2004-04-11\n0.762057\n58323.761863\n46723.500000\n\n\n2003-01-06\n0.758514\n23857.995845\n21913.505457\n\n\n2003-03-07\n1.332419\n29036.114641\n31676.191662\n\n\n2001-01-17\n1.142551\n71184.337297\n80915.848594\n\n\n2003-04-12\n1.654931\n36588.754410\n46777.071912\n\n\n2002-02-16\n0.825554\n67892.914951\n55030.729840\n\n\n2001-10-05\n0.929293\n69504.770296\n65128.356512\n\n\n2000-10-02\n0.994120\n85176.526599\n88936.541885\n\n\n2003-06-05\n0.760863\n27988.449143\n24654.281482\n\n\n2000-07-07\n0.897929\n97458.154049\n79856.919150\n\n\n2003-09-28\n1.570214\n42328.076114\n54497.175586\n\n\n2002-08-24\n0.863911\n35824.789963\n33256.068138\n\n\n2000-11-28\n1.242113\n69593.024187\n92943.044427\n\n\n2001-12-28\n0.881569\n71500.973097\n69504.085790\n\n\n2001-09-02\n1.197270\n69966.800254\n93695.953636\n\n\n2002-12-16\n1.660649\n7810.413156\n9322.237192\n\n\n2004-08-30\n1.286178\n47599.593625\n56735.271957\n\n\n2001-02-18\n1.254430\n64837.275444\n81784.074961\n\n\n2000-05-15\n1.044025\n100464.794301\n121779.610581\n\n\n2004-08-13\n0.967493\n57864.200257\n56080.324427\n\n\n2001-02-02\n1.212479\n67857.642170\n88071.282712\n\n\n2001-05-21\n1.178636\n71337.927880\n93445.016948\n\n\n2003-05-06\n0.644814\n25609.287364\n21843.082411\n\n\n2000-07-31\n1.012828\n92617.293678\n85766.681515\n\n\n2002-07-03\n1.267595\n57443.596207\n66595.562035\n\n\n2004-09-27\n1.369890\n41546.443755\n50176.823146\n\n\n2000-10-25\n0.985062\n72757.343228\n70195.986606\n\n\n2002-11-02\n0.834577\n19043.272758\n18373.397899\n\n\n\n\n\n\n\n\n2.1 Visualizing Lift Tests\nEach experiment records: pre-spend (x_start), post-spend (x_end), and measured lift. These give us causal “ground truth” deltas.\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Scatter plot for pre-spend and observed lift\nax.scatter(lift_test_search[\"x_start\"], [1] * len(lift_test_search), label=\"Pre-Spend\", alpha=0.6)\nax.scatter(lift_test_search[\"x_end\"], lift_test_search[\"lift\"], label=\"Observed Lift\", alpha=0.6)\n\n# Annotate with arrows to show lift effect\nfor _, row in lift_test_search.iterrows():\n    ax.annotate(\n        \"\",\n        xy=(row[\"x_end\"], row[\"lift\"]),\n        xytext=(row[\"x_start\"], 1),\n        arrowprops=dict(arrowstyle=\"-&gt;\", alpha=0.5),\n    )\n\n# Add horizontal line and labels\nax.axhline(1, linestyle=\"--\", color=\"gray\", alpha=0.7)\nax.set(\n    title=\"Search Ads Lift Tests\",\n    xlabel=\"Spend\",\n    ylabel=\"Revenue Ratio\",\n)\n\n# Add legend and finalize layout\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.2 Improve Estimates via LiftExperimentLikelihood\nThis adds a new likelihood term that makes the model match lift observations.\n🔁 Still Bayesian: It incorporates test variance and model uncertainty.\nSince we use sktime interface, we have access to get_params() and set_params(**kwargs) methods. This allows us to easily swap effects and likelihoods. When we define our model, the effect’s name become a key in the model’s get_params() dictionary. We can use this to set the effect’s parameters directly.\n\nfrom prophetverse.effects.lift_likelihood import LiftExperimentLikelihood\n\nmodel_lift = baseline_model.clone()\nmodel_lift.set_params(\n    ad_spend_search=LiftExperimentLikelihood(\n        effect=baseline_model.get_params()[\"ad_spend_search\"],\n        lift_test_results=lift_test_search,\n        prior_scale=0.05,\n    ),\n    ad_spend_social_media=LiftExperimentLikelihood(\n        effect=baseline_model.get_params()[\"ad_spend_social_media\"],\n        lift_test_results=lift_test_social,\n        prior_scale=0.05,\n    ),\n)\n\nmodel_lift.fit(y=y, X=X)\n\nProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 LiftExp...\n2002-07-03  1.053316   25537.869750   38025.254409\n2004-09-27  1.010456    8523.749415   11037.466255\n2000-10-25  0.980559   50964.957119   65038.733878\n2002-11-02  1.355164    1195.765141    1603.253926,\n                                                          prior_scale=0.05),\n                                 'ad_spend_social_media')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 LiftExp...\n2002-07-03  1.053316   25537.869750   38025.254409\n2004-09-27  1.010456    8523.749415   11037.466255\n2000-10-25  0.980559   50964.957119   65038.733878\n2002-11-02  1.355164    1195.765141    1603.253926,\n                                                          prior_scale=0.05),\n                                 'ad_spend_social_media')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=100)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=0.1, sp_list=[365.25])LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[3],\n                         freq='D', prior_scale=0.05, sp_list=[7])ad_spend_searchLiftExperimentLikelihood(effect=ChainedEffects(steps=[('adstock',\n                                                       GeometricAdstockEffect()),\n                                                      ('saturation',\n                                                       HillEffect(effect_mode='additive',\n                                                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n                                                                  input_scale=1000000.0,\n                                                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f52...\n2001-02-02  1.212479   67857.642170   88071.282712\n2001-05-21  1.178636   71337.927880   93445.016948\n2003-05-06  0.644814   25609.287364   21843.082411\n2000-07-31  1.012828   92617.293678   85766.681515\n2002-07-03  1.267595   57443.596207   66595.562035\n2004-09-27  1.369890   41546.443755   50176.823146\n2000-10-25  0.985062   72757.343228   70195.986606\n2002-11-02  0.834577   19043.272758   18373.397899,\n                         prior_scale=0.05)effect: ChainedEffectsChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;)ad_spend_social_mediaLiftExperimentLikelihood(effect=ChainedEffects(steps=[('adstock',\n                                                       GeometricAdstockEffect()),\n                                                      ('saturation',\n                                                       HillEffect(effect_mode='additive',\n                                                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n                                                                  input_scale=1000000.0,\n                                                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f52...\n2001-02-02  1.085461   38129.336619   47626.107330\n2001-05-21  0.961702   50010.574434   73721.957893\n2003-05-06  1.099837    2893.044038    2702.425237\n2000-07-31  1.028477  105196.357631  125200.174092\n2002-07-03  1.053316   25537.869750   38025.254409\n2004-09-27  1.010456    8523.749415   11037.466255\n2000-10-25  0.980559   50964.957119   65038.733878\n2002-11-02  1.355164    1195.765141    1603.253926,\n                         prior_scale=0.05)effect: ChainedEffectsChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine(num_steps=5000,\n                   optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                         memory_size=300))\n\n\n\ncomponents_lift = model_lift.predict_components(X=X, fh=X.index)\ncomponents_lift.head()\n\n\n\n\n\n\n\n\nad_spend_search\nad_spend_social_media\nmean\nobs\ntrend\nweekly_seasonality\nyearly_seasonality\n\n\n\n\n2000-01-01\n6.895928e+06\n3.175475e+06\n1.048024e+07\n1.045962e+07\n440290.870109\n18676.793573\n-50131.808812\n\n\n2000-01-02\n7.567480e+06\n3.223349e+06\n1.119076e+07\n1.114657e+07\n449462.158533\n793.117824\n-50328.503839\n\n\n2000-01-03\n7.681071e+06\n3.232674e+06\n1.132772e+07\n1.138588e+07\n458633.446958\n5733.189499\n-50386.314189\n\n\n2000-01-04\n7.702444e+06\n3.238810e+06\n1.133666e+07\n1.129629e+07\n467804.735382\n-22094.853508\n-50300.443181\n\n\n2000-01-05\n7.675580e+06\n3.235106e+06\n1.133469e+07\n1.128615e+07\n476976.023807\n-2907.821599\n-50066.435307\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(figsize=(8, 6), ncols=2)\n\nfor ax, channel in zip(axs, [\"ad_spend_search\", \"ad_spend_social_media\"]):\n    ax.scatter(\n        X[channel],\n        y_pred_components[channel],\n        label=\"Baseline\",\n        alpha=0.6,\n        s=50,\n    )\n    ax.scatter(\n        X[channel],\n        components_lift[channel],\n        label=\"With Lift Test\",\n        alpha=0.6,\n        s=50,\n    )\n    ax.plot(\n        X[channel],\n        true_components[channel],\n        label=\"True\",\n        color=\"black\",\n        linewidth=2,\n    )\n    ax.set(\n        title=f\"{channel} Predicted Effects\",\n        xlabel=\"Daily Spend\",\n        ylabel=\"Incremental Effect\",\n    )\n    ax.axhline(0, linestyle=\"--\", color=\"gray\", alpha=0.7)\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nMuch better, right? And it was implemented with a really modular and flexible code. You could wrap any effect with LiftExperimentLikelihood to add lift test data to guide its behaviour. Nevertheless, this is not the end of the story.\n\n\n2.3 Add Attribution Signals with ExactLikelihood\nAttribution models can provide daily signals. If available, you can incorporate them by adding another likelihood term via ExactLikelihood.\nWe create a synthetic attribution signal by multiplying the true effect with a random noise factor.\n\nfrom prophetverse.effects import ExactLikelihood\n\nrng = np.random.default_rng(42)\n\n# Generate attribution signals for search and social media channels\nattr_search = true_components[[\"ad_spend_search\"]] * rng.normal(\n    1, 0.1, size=(len(y), 1)\n)\nattr_social = true_components[[\"ad_spend_social_media\"]] * rng.normal(\n    1, 0.1, size=(len(y), 1)\n)\n\n# Display the first few rows of the social media attribution signal\nattr_social.head()\n\n\n\n\n\n\n\n\nad_spend_social_media\n\n\n\n\n2000-01-01\n2.171846e+06\n\n\n2000-01-02\n2.625088e+06\n\n\n2000-01-03\n2.983027e+06\n\n\n2000-01-04\n2.836756e+06\n\n\n2000-01-05\n2.520331e+06\n\n\n\n\n\n\n\n\nmodel_umm = model_lift.clone()\nmodel_umm.set_params(\n    exogenous_effects=model_lift.get_params()[\"exogenous_effects\"]\n    + [\n        (\n            \"attribution_search\",\n            ExactLikelihood(\"ad_spend_search\", attr_search, 0.01),\n            None,\n        ),\n        (\n            \"attribution_social_media\",\n            ExactLikelihood(\"ad_spend_social_media\", attr_social, 0.01),\n            None,\n        ),\n    ]\n)\nmodel_umm.fit(y=y, X=X)\n\nProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 LiftExp...\n2000-01-04           2.836756e+06\n2000-01-05           2.520331e+06\n...                           ...\n2004-12-28           1.012305e+06\n2004-12-29           1.108765e+06\n2004-12-30           1.070854e+06\n2004-12-31           1.027756e+06\n2005-01-01           1.012141e+06\n\n[1828 rows x 1 columns]),\n                                 None)],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('ad_spend_search',\n                                 LiftExp...\n2000-01-04           2.836756e+06\n2000-01-05           2.520331e+06\n...                           ...\n2004-12-28           1.012305e+06\n2004-12-29           1.108765e+06\n2004-12-30           1.070854e+06\n2004-12-31           1.027756e+06\n2005-01-01           1.012141e+06\n\n[1828 rows x 1 columns]),\n                                 None)],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=100))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=100)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=0.1, sp_list=[365.25])LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[3],\n                         freq='D', prior_scale=0.05, sp_list=[7])ad_spend_searchLiftExperimentLikelihood(effect=ChainedEffects(steps=[('adstock',\n                                                       GeometricAdstockEffect()),\n                                                      ('saturation',\n                                                       HillEffect(effect_mode='additive',\n                                                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n                                                                  input_scale=1000000.0,\n                                                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f52...\n2001-02-02  1.212479   67857.642170   88071.282712\n2001-05-21  1.178636   71337.927880   93445.016948\n2003-05-06  0.644814   25609.287364   21843.082411\n2000-07-31  1.012828   92617.293678   85766.681515\n2002-07-03  1.267595   57443.596207   66595.562035\n2004-09-27  1.369890   41546.443755   50176.823146\n2000-10-25  0.985062   72757.343228   70195.986606\n2002-11-02  0.834577   19043.272758   18373.397899,\n                         prior_scale=0.05)effect: ChainedEffectsChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;)ad_spend_social_mediaLiftExperimentLikelihood(effect=ChainedEffects(steps=[('adstock',\n                                                       GeometricAdstockEffect()),\n                                                      ('saturation',\n                                                       HillEffect(effect_mode='additive',\n                                                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n                                                                  input_scale=1000000.0,\n                                                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f52...\n2001-02-02  1.085461   38129.336619   47626.107330\n2001-05-21  0.961702   50010.574434   73721.957893\n2003-05-06  1.099837    2893.044038    2702.425237\n2000-07-31  1.028477  105196.357631  125200.174092\n2002-07-03  1.053316   25537.869750   38025.254409\n2004-09-27  1.010456    8523.749415   11037.466255\n2000-10-25  0.980559   50964.957119   65038.733878\n2002-11-02  1.355164    1195.765141    1603.253926,\n                         prior_scale=0.05)effect: ChainedEffectsChainedEffects(steps=[('adstock', GeometricAdstockEffect()),\n                      ('saturation',\n                       HillEffect(effect_mode='additive',\n                                  half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n                                  input_scale=1000000.0,\n                                  max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n                                  slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect()HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f526011da90 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f5260132790 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f5248b14590 with batch shape () and event shape ()&gt;)ExactLikelihoodExactLikelihood(effect_name='ad_spend_search', prior_scale=0.01,\n                reference_df=            ad_spend_search\n2000-01-01     8.682649e+06\n2000-01-02     7.542228e+06\n2000-01-03     9.091821e+06\n2000-01-04     9.259557e+06\n2000-01-05     6.785853e+06\n...                     ...\n2004-12-28     3.547301e+06\n2004-12-29     2.771639e+06\n2004-12-30     2.972279e+06\n2004-12-31     3.226681e+06\n2005-01-01     3.605501e+06\n\n[1828 rows x 1 columns])ExactLikelihoodExactLikelihood(effect_name='ad_spend_social_media', prior_scale=0.01,\n                reference_df=            ad_spend_social_media\n2000-01-01           2.171846e+06\n2000-01-02           2.625088e+06\n2000-01-03           2.983027e+06\n2000-01-04           2.836756e+06\n2000-01-05           2.520331e+06\n...                           ...\n2004-12-28           1.012305e+06\n2004-12-29           1.108765e+06\n2004-12-30           1.070854e+06\n2004-12-31           1.027756e+06\n2005-01-01           1.012141e+06\n\n[1828 rows x 1 columns])inference_engineMAPInferenceEngineMAPInferenceEngine(num_steps=5000,\n                   optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                         memory_size=300))\n\n\n\ncomponents_umm = model_umm.predict_components(X=X, fh=X.index)\n\nfig, axs = plt.subplots(2, 1, figsize=(8, 10), sharex=True)\nfor ax, channel in zip(axs, [\"ad_spend_search\", \"ad_spend_social_media\"]):\n    ax.scatter(X[channel], y_pred_components[channel], label=\"Baseline\", alpha=0.4)\n    ax.scatter(X[channel], components_lift[channel], label=\"With Lift Test\", alpha=0.4)\n    ax.scatter(X[channel], components_umm[channel], label=\"With Attribution\", alpha=0.4)\n    ax.plot(X[channel], true_components[channel], label=\"True Effect\", color=\"black\")\n    ax.set_title(channel)\n    ax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nEven better! And, due to sktime-like interface, wrapping and adding new effects is easy.",
    "crumbs": [
      "Fitting, calibrating and Unified Marketing Measurement"
    ]
  },
  {
    "objectID": "mmm/fitting_and_calibration.html#final-thoughts-toward-unified-marketing-measurement",
    "href": "mmm/fitting_and_calibration.html#final-thoughts-toward-unified-marketing-measurement",
    "title": "Forecasting, Calibration, and Unified Marketing Measurement",
    "section": "Final Thoughts: Toward Unified Marketing Measurement",
    "text": "Final Thoughts: Toward Unified Marketing Measurement\n✅ What we learned:\n1. Adstock + saturation are essential to capture media dynamics.\n2. Good predictions ≠ good attribution.\n3. Causal data like lift tests can correct misattribution.\n4. Attribution signals add further constraints.\n🛠️ Use this when:\n* Channels are correlated → Use lift tests.\n* You have granular model output → Add attribution likelihoods.\n🧪 Model selection tip:\nAlways validate causal logic, not just fit quality.\nWith Prophetverse, you can combine observational, experimental, and model-based signals into one coherent MMM+UMM pipeline.",
    "crumbs": [
      "Fitting, calibrating and Unified Marketing Measurement"
    ]
  },
  {
    "objectID": "mmm/adstock.html",
    "href": "mmm/adstock.html",
    "title": "Using Adstock effect",
    "section": "",
    "text": "from prophetverse import WeibullAdstockEffect, GeometricAdstockEffect, ChainedEffects\n\n\nfrom prophetverse.datasets._mmm.dataset1 import get_dataset\n\n\ny, X, _, _, _ = get_dataset()\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nX_search = X[[\"ad_spend_search\"]]\n\n\nadstock = WeibullAdstockEffect()"
  },
  {
    "objectID": "mmm/getting_familiar.html",
    "href": "mmm/getting_familiar.html",
    "title": "Basics of Prophetverse API",
    "section": "",
    "text": "Prophetverse is a powerful tool for building customized and glass-box forecasting and mix models. In Prophetverse, we define each component of the model as a separate effect, making this library extremely flexible to attend your specific needs.\nIn this page, we will:",
    "crumbs": [
      "Basics of MMM with Prophetverse"
    ]
  },
  {
    "objectID": "mmm/getting_familiar.html#data-structures-y-and-x",
    "href": "mmm/getting_familiar.html#data-structures-y-and-x",
    "title": "Basics of Prophetverse API",
    "section": "1. Data Structures (y and X)",
    "text": "1. Data Structures (y and X)\nProphetverse uses the sktime forecasting API. The essentials:\n\ny: a pandas DataFrame indexed by a time index (pd.DatetimeIndex or pd.PeriodIndex). Single column for univariate MMM (e.g. revenue):\n\n\n\n\n\n\n\n\n\n\nrevenue\n\n\n\n\n2020-01-01\n0.023137\n\n\n2020-01-02\n0.662805\n\n\n2020-01-03\n0.742520\n\n\n2020-01-04\n0.942708\n\n\n2020-01-05\n0.354839\n\n\n\n\n\n\n\nFor panel datasets (e.g. in the case of multiple products or regions), use a MultiIndex, where the first index level is the entity (e.g. product or region) and the second level is the time.\n\n\n\n\n\n\n\n\n\n\nrevenue\n\n\nproduct\n\n\n\n\n\n\nproduct_a\n2020-01-01\n0.637908\n\n\n2020-01-02\n0.262712\n\n\n2020-01-03\n0.592915\n\n\n2020-01-04\n0.558257\n\n\n2020-01-05\n0.149614\n\n\n...\n...\n...\n\n\nproduct_c\n2020-12-27\n0.274603\n\n\n2020-12-28\n0.666997\n\n\n2020-12-29\n0.610935\n\n\n2020-12-30\n0.070760\n\n\n2020-12-31\n0.638122\n\n\n\n\n1098 rows × 1 columns\n\n\n\n\nX: a pandas DataFrame aligned on the same index containing exogenous variables (media spend, price, promotions, macro, etc.). Columns are arbitrary names.\n\n\n\n\n\n\n\nThe index type should always be the same for y and X, and every dataframe you use. After choosing Datetime or Period index for y, use the same type for X.\n\n\n\n\nExample of dataset\nHere we load a synthetic dataset:\n\nfrom prophetverse.datasets._mmm.dataset1 import get_dataset\n\n(y, X, *_) = get_dataset()\n\n\ny.head()\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n2000-01-01    10815512.0\n2000-01-02    11120677.0\n2000-01-03    11260387.0\n2000-01-04    11322533.0\n2000-01-05    11321180.0\nFreq: D, dtype: float32\n\n\nThe X looks like this:\n\nX.head()\n\n\n\n\n\n\n\n\nad_spend_search\nad_spend_social_media\n\n\n\n\n2000-01-01\n89076.191178\n98587.488958\n\n\n2000-01-02\n88891.993106\n99066.321168\n\n\n2000-01-03\n89784.955064\n97334.106903\n\n\n2000-01-04\n89931.220681\n101747.300585\n\n\n2000-01-05\n89184.319596\n93825.221809\n\n\n\n\n\n\n\nWe will split the dataset into training and testing sets.\n\nfrom sktime.split import temporal_train_test_split\n\ny_train, y_test, X_train, X_test = temporal_train_test_split(y, X, test_size=0.2)",
    "crumbs": [
      "Basics of MMM with Prophetverse"
    ]
  },
  {
    "objectID": "mmm/getting_familiar.html#prophetverse-model",
    "href": "mmm/getting_familiar.html#prophetverse-model",
    "title": "Basics of Prophetverse API",
    "section": "2. Prophetverse model",
    "text": "2. Prophetverse model\nThink of Prophetverse as the conerstone of you MMM model. It is a flexible class that allows you to define the trend, seasonality, and custom exogenous effects of your model.\n\nSimple Prophetverse model\nWe can use a simple Prophetverse model with Linear effects and a seasonality component:\n\nfrom prophetverse import Prophetverse, LinearEffect, LinearFourierSeasonality\n\nfrom prophetverse.utils.regex import starts_with, no_input_columns\n\n\nseasonality_effect = LinearFourierSeasonality(\n    sp_list=[365.25, 7],\n    fourier_terms_list=[10, 3],\n    prior_scale=0.1,\n    freq=\"D\",\n    effect_mode=\"additive\",\n)\n\nad_spend_effect = LinearEffect()\n\nmodel = Prophetverse(\n    exogenous_effects=[\n        (\"ad_spend\", ad_spend_effect, starts_with(\"ad\")),\n        (\"seasonality\", seasonality_effect, no_input_columns),\n    ],\n)\n\nmodel.fit(y=y_train, X=X_train)\n\nProphetverse(exogenous_effects=[('ad_spend', LinearEffect(), '^(?:ad)'),\n                                ('seasonality',\n                                 LinearFourierSeasonality(fourier_terms_list=[10,\n                                                                              3],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25, 7]),\n                                 '^$')])Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('ad_spend', LinearEffect(), '^(?:ad)'),\n                                ('seasonality',\n                                 LinearFourierSeasonality(fourier_terms_list=[10,\n                                                                              3],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25, 7]),\n                                 '^$')])effectsPiecewiseLinearTrendPiecewiseLinearTrend()LinearEffectLinearEffect()LinearFourierSeasonalityLinearFourierSeasonality(fourier_terms_list=[10, 3], freq='D', prior_scale=0.1,\n                         sp_list=[365.25, 7])inference_engineMCMCInferenceEngineMCMCInferenceEngine()\n\n\nBy default, the model will run a MCMC inference to obtain the parameters. We can, however, easily switch to a MAP inference by setting inference_engine=MAPInferenceEngine() in the model constructor. The MAP inference is generally faster but provides point estimates of the parameters.\nTo run in-sample and out-of-sample forecasts of total revenue, we can simply call predict. We need to pass a “forecasting horizon” (fh) object, that should preferably be an index of the type of our y and X’s index. Since we want to forecast for both train and test timepoints, we use y.index as fh, and pass the full X as exogenous variables.\n\nfh = y.index\n\ny_pred = model.predict(fh=fh, X=X)\n\ny_pred\n\n2000-01-01    14483120.0\n2000-01-02    14349842.0\n2000-01-03    14425632.0\n2000-01-04    14234289.0\n2000-01-05    14376405.0\n                 ...    \n2004-12-28    32365182.0\n2004-12-29    32117918.0\n2004-12-30    32005472.0\n2004-12-31    32258116.0\n2005-01-01    32360856.0\nFreq: D, Length: 1828, dtype: float32\n\n\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_forecasts(y_pred):\n    fig, ax = plt.subplots(figsize=(10,5))\n\n    ax.plot(y.index.to_timestamp(), y)\n    ax.plot(y_pred.index, y_pred)\n    ax.axvline(y_train.index.max().to_timestamp(), color=\"black\", linestyle=\"--\", label=\"Train/Test split\")\n    fig.show()\n\nplot_forecasts(y_pred)\n\n\n\n\n\n\n\n\n\nGetting the components\nTo obtain the contribution of each component, you can use the predict_components method:\n\ncomponents = model.predict_components(fh=fh, X=X)\ncomponents.head()\n\n\n\n\n\n\n\n\nad_spend\nmean\nobs\nseasonality\ntrend\n\n\n\n\n2000-01-01\n1951345.875\n14483120.0\n14500401.0\n614.704651\n12531161.0\n\n\n2000-01-02\n1917232.500\n14349842.0\n14266047.0\n-50484.121094\n12483091.0\n\n\n2000-01-03\n2016962.625\n14425632.0\n14459742.0\n-26353.626953\n12435021.0\n\n\n2000-01-04\n1874735.000\n14234289.0\n14181142.0\n-27398.041016\n12386951.0\n\n\n2000-01-05\n2079018.125\n14376405.0\n14344686.0\n-41493.980469\n12338882.0\n\n\n\n\n\n\n\nIf you want to obtain all the sample to compute, for example, probabilistic intervals and measure the risk, you can use the predict_component_samples method:\n\nsamples = model.predict_component_samples(fh=fh, X=X)\nsamples\n\n\n\n\n\n\n\n\n\nad_spend\nmean\nobs\nseasonality\ntrend\n\n\nsample\n\n\n\n\n\n\n\n\n\n\n0\n2000-01-01\n1182749.500\n13707321.0\n12335327.0\n-15333.103516\n12539905.0\n\n\n2000-01-02\n1158945.250\n13661526.0\n13223348.0\n-23272.130859\n12525853.0\n\n\n2000-01-03\n1249228.625\n13742996.0\n12519516.0\n-18033.396484\n12511801.0\n\n\n2000-01-04\n1125272.125\n13619009.0\n14577379.0\n-4012.927979\n12497750.0\n\n\n2000-01-05\n1321673.125\n13799107.0\n12858874.0\n-6265.105469\n12483698.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n999\n2004-12-28\n5269823.000\n31613836.0\n30836816.0\n-16992.765625\n26361004.0\n\n\n2004-12-29\n4986981.000\n31321890.0\n31899526.0\n-45524.140625\n26380432.0\n\n\n2004-12-30\n4852499.000\n31175388.0\n31748786.0\n-76963.437500\n26399854.0\n\n\n2004-12-31\n5050830.500\n31449106.0\n31127286.0\n-21041.710938\n26419318.0\n\n\n2005-01-01\n5125169.000\n31548410.0\n33430684.0\n-15503.842773\n26438742.0\n\n\n\n\n1828000 rows × 5 columns",
    "crumbs": [
      "Basics of MMM with Prophetverse"
    ]
  },
  {
    "objectID": "mmm/index.html",
    "href": "mmm/index.html",
    "title": "Marketing Mix Modeling",
    "section": "",
    "text": "Marketing Mix Modeling (MMM) is a statistical analysis technique that helps in obtaining insights and planning marketing strategies. It is tightly related to Time Series Analysis — we can think of MMM as a special case of Time Series forecasting, where the goal is to understand the incrementality of different exogenous variables on the target variable.\nWhen Prophetverse was created, the objective was to provide a more up-to-date implementation of Facebook’s Prophet model and to add features and customization options that were not available in the original implementation. However, as the library evolved, it became clear that it could be used for more than just forecasting and that it could be a powerful tool for MMM.\nProphetverse has the following features that make it a great choice for MMM:\nParticularly, Prophetverse has a practical interface, based on initialization, fit and predict, that allows users to quickly build and evaluate custom MMM models.\nA typical Prophetverse model workflow is as follows:",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "mmm/index.html#available-effects",
    "href": "mmm/index.html#available-effects",
    "title": "Marketing Mix Modeling",
    "section": "Available Effects",
    "text": "Available Effects\nYou can get a list of all available effects in Prophetverse by running the following code:\n\nfrom skbase.lookup import all_objects\nfrom prophetverse.effects import BaseEffect\n\nall_objects(\n    object_types=[BaseEffect], package_name=\"prophetverse\", as_dataframe=True\n)\n\n\n\n\n\n\n\n\nname\nobject\n\n\n\n\n0\nBetaTargetLikelihood\n&lt;class 'prophetverse.effects.target.univariate...\n\n\n1\nChainedEffects\n&lt;class 'prophetverse.effects.chain.ChainedEffe...\n\n\n2\nConstant\n&lt;class 'prophetverse.effects.constant.Constant'&gt;\n\n\n3\nCoupledExactLikelihood\n&lt;class 'prophetverse.effects.coupled.CoupledEx...\n\n\n4\nExactLikelihood\n&lt;class 'prophetverse.effects.exact_likelihood....\n\n\n5\nFlatTrend\n&lt;class 'prophetverse.effects.trend.flat.FlatTr...\n\n\n6\nForward\n&lt;class 'prophetverse.effects.forward.Forward'&gt;\n\n\n7\nGammaTargetLikelihood\n&lt;class 'prophetverse.effects.target.univariate...\n\n\n8\nGeometricAdstockEffect\n&lt;class 'prophetverse.effects.adstock.Geometric...\n\n\n9\nHillEffect\n&lt;class 'prophetverse.effects.hill.HillEffect'&gt;\n\n\n10\nHurdleTargetLikelihood\n&lt;class 'prophetverse.effects.target.hurdle.Hur...\n\n\n11\nIdentity\n&lt;class 'prophetverse.effects.identity.Identity'&gt;\n\n\n12\nIgnoreInput\n&lt;class 'prophetverse.effects.ignore_input.Igno...\n\n\n13\nLiftExperimentLikelihood\n&lt;class 'prophetverse.effects.lift_likelihood.L...\n\n\n14\nLinearEffect\n&lt;class 'prophetverse.effects.linear.LinearEffe...\n\n\n15\nLinearFourierSeasonality\n&lt;class 'prophetverse.effects.fourier.LinearFou...\n\n\n16\nLinearProxyLikelihood\n&lt;class 'prophetverse.effects.proxy_likelihood....\n\n\n17\nLogEffect\n&lt;class 'prophetverse.effects.log.LogEffect'&gt;\n\n\n18\nMichaelisMentenEffect\n&lt;class 'prophetverse.effects.michaelis_menten....\n\n\n19\nMultiplyEffects\n&lt;class 'prophetverse.effects.operations.Multip...\n\n\n20\nMultivariateNormal\n&lt;class 'prophetverse.effects.target.multivaria...\n\n\n21\nNegativeBinomialTargetLikelihood\n&lt;class 'prophetverse.effects.target.univariate...\n\n\n22\nNormalTargetLikelihood\n&lt;class 'prophetverse.effects.target.univariate...\n\n\n23\nPanelBHLinearEffect\n&lt;class 'prophetverse.effects.linear.PanelBHLin...\n\n\n24\nPiecewiseLinearTrend\n&lt;class 'prophetverse.effects.trend.piecewise.P...\n\n\n25\nPiecewiseLogisticTrend\n&lt;class 'prophetverse.effects.trend.piecewise.P...\n\n\n26\nSumEffects\n&lt;class 'prophetverse.effects.operations.SumEff...\n\n\n27\nTargetLikelihood\n&lt;class 'prophetverse.effects.target.univariate...\n\n\n28\nWeibullAdstockEffect\n&lt;class 'prophetverse.effects.adstock.WeibullAd...\n\n\n\n\n\n\n\nThe following effects may be of interest if you are working on MMM:\n\nGeometricAdstockEffect: The geometric adstock effect is a widely used technique in MMM to account for the lagged effect of advertising on sales. It is based on the idea that the effect of an ad on sales decays over time and that the decay follows a geometric progression.\nHillEffect: The Hill curve accounts for diminishing returns in the effect of an exogenous variable on the target variable.\nChainedEffects: The chained effect is a way to combine multiple effects into a single one. For example, you can use adstock and Hill together.\nLiftExperimentLikelihood: The lift experiment likelihood is a likelihood term that can be used to account for the effect of a lift test on the target variable. It is useful if you want to understand the incrementality of a variable and have already run a lift test to analyze how variations in the input affect the output.\nExactLikelihood: The exact likelihood is a likelihood term that can be used to incorporate a reference value as the incrementality of an exogenous variable. It is useful if another team in your company has already calculated the incrementality of a variable and you want to use it in your MMM model.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "mmm/index.html#related-libraries",
    "href": "mmm/index.html#related-libraries",
    "title": "Marketing Mix Modeling",
    "section": "Related Libraries",
    "text": "Related Libraries\nI invite you to check out other libraries for MMM. Two of them are:\n\nPyMC-Marketing: This is an amazing project by PyMC’s developers. It is a library that provides a set of tools for building Bayesian models for marketing analytics. The documentation is very comprehensive and a great source of information.\nLightweight-MMM: This library, as far as I know, was created by Google developers based on NumPyro. Now, they are developing a new one called Meridian.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "mmm/proxy_variables.html",
    "href": "mmm/proxy_variables.html",
    "title": "Awareness: Latent and Proxy Variables",
    "section": "",
    "text": "What you will learn:",
    "crumbs": [
      "Awareness: Latent and Proxy Variables"
    ]
  },
  {
    "objectID": "mmm/proxy_variables.html#latent-variables",
    "href": "mmm/proxy_variables.html#latent-variables",
    "title": "Awareness: Latent and Proxy Variables",
    "section": "Latent variables",
    "text": "Latent variables\n\n\n\n\n\n\nTip\n\n\n\nFrom Prophetverse v0.10.0 onwards, we have included a latent variable feature. You only need to add “latent/” as a prefix to the effect name, and it will not be considered as a final component in the additive decomposition. Instead, it can be used as an input to other effects or linked to proxy variables.\n\n\nIn Marketing Mix Modeling, we often encounter latent variables that are not directly observable but significantly influence sales. A common example is brand awareness. It can befined as :\n\nThe ability of a potential buyer to recognize or recall that a brand is a member of a certain product category.\nKeller, K. L. (1993). Conceptualizing, Measuring, and Managing Customer-Based Brand Equity.\n\nwhich is clearly affected by marketing activities, and in turn affects sales. Standard MMM approaches may struggle to accurately capture the impact of such latent variables, leading to biased estimates and suboptimal decision-making.\nSince we cannot observe these variables directly, it can be hard to model them. However, we propose in Prophetverse the usage of proxy variables that are correlated with the latent variable of interest to help us infer its true causal effect. For example, we might use survey data on brand recognition or social media engagement metrics as proxies for brand awareness and use it as a soft calibration signal.\nThe idea is simple, but powerful. More formally, if \\(Z(t)\\) is a latent variable representing brand awareness at time \\(t\\), and \\(P(t)\\) is a proxy variable correlated with \\(Z(t)\\), we can add a new likelihood term:\n\\[\nP(t) \\sim \\mathcal{N}(\\beta Z(t), \\sigma^2)\n\\]\nFor a \\(\\beta\\) with user-defined prior, and \\(\\sigma^2\\) representing the uncertainty in the proxy relationship. If there exists a \\(\\beta\\), then the proxy variable provides additional information about the latent variable, helping to better identify its effect on sales.",
    "crumbs": [
      "Awareness: Latent and Proxy Variables"
    ]
  },
  {
    "objectID": "mmm/proxy_variables.html#dataset",
    "href": "mmm/proxy_variables.html#dataset",
    "title": "Awareness: Latent and Proxy Variables",
    "section": "1. Dataset",
    "text": "1. Dataset\n\n1.1. Loading the data\nWe use a synthetic dataset that simulates a marketing scenario with 2 types of investments:\n\nInvestments with impact on brand awareness (e.g., upper-funnel channels like TV, Display, Social Media)\nInvestments with last-click impact (e.g., Search, Affiliates)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom prophetverse.datasets._mmm.dataset2_branding import get_dataset\n\ny, X, true_effect, true_model = get_dataset()\n\ndisplay(X.head())\n\n\n\n\n\n\n\n\nad_spend_awareness\nlast_click_spend\n\n\n\n\n2000-03-31\n102271.901646\n36711.937500\n\n\n2000-04-01\n104467.149891\n36306.410156\n\n\n2000-04-02\n103863.702625\n36007.777344\n\n\n2000-04-03\n102838.194548\n36195.750000\n\n\n2000-04-04\n103186.478348\n36113.398438\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 4))\ny.plot.line(ax=ax)\nfig.show() \n\n\n\n\n\n\n\n\nAnd the investment variables:\n\nfig, ax = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\nX[\"ad_spend_awareness\"].plot.line(ax=ax[0], title=\"Ad Spend Awareness\")\nX[\"last_click_spend\"].plot.line(ax=ax[1], title=\"Last-Click Spend\")\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n\n\n\nSince this is a synthetic dataset, we have access to the ground truth of the effects of investments on sales. In a real-world scenario, these would be unknown. Let’s take a look at them:\n\n\nCode\ndef get_counterfactual(fitted_model, X, column):\n    X_counterfactual = X.copy()\n    X_counterfactual[column] = 0\n    y_pred = fitted_model.predict(X=X, fh=X.index)\n    y_pred_counterfactual = fitted_model.predict(X=X_counterfactual, fh=X.index)\n    delta = y_pred - y_pred_counterfactual\n    return delta\n\n\nad_awareness_effect = get_counterfactual(true_model, X, \"ad_spend_awareness\")\nlast_click_effect = get_counterfactual(true_model, X, \"last_click_spend\")\nfig, ax = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n\n# Top subplot\nX[\"ad_spend_awareness\"].plot.line(\n    ax=ax[0], color=\"orange\", label=\"Ad Spend (Observed)\", legend=False\n)\nax0r = ax[0].twinx()\nad_awareness_effect.plot.line(ax=ax0r, label=\"True Effect\", legend=False)\nax[0].set_title(\"Ad Spend Awareness\")\nhandles0, labels0 = ax[0].get_legend_handles_labels()\nhandles0r, labels0r = ax0r.get_legend_handles_labels()\nax[0].legend(handles0 + handles0r, labels0 + labels0r, loc=\"best\")\n\n# Bottom subplot\nX[\"last_click_spend\"].plot.line(\n    ax=ax[1], color=\"orange\", label=\"Last-Click Spend (Observed)\", legend=False\n)\nax1r = ax[1].twinx()\nlast_click_effect.plot.line(ax=ax1r, label=\"True Effect\", legend=False)\nax[1].set_title(\"Last-Click Spend\")\nhandles1, labels1 = ax[1].get_legend_handles_labels()\nhandles1r, labels1r = ax1r.get_legend_handles_labels()\nax[1].legend(handles1 + handles1r, labels1 + labels1r, loc=\"best\")\n\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n\n\n\n\nA common problem in MMM is the correlation of last-click with sales, which can lead to over-attribution of sales to last-click channels if not properly accounted for. We will see later how the proxy variable can help mitigate this.\n\n\n1.2. Proxy variable\nIn real-world scenarios, proxy variables can be obtained from various sources, such as:\n\nBranded Search Volume: The volume of searches for the brand name on search engines.\nSurvey Data: Periodic surveys measuring brand recognition and recall among the target audience.\nSocial Media Engagement: Metrics such as likes, shares, comments, and mentions related\n\nWe simulate a proxy variable correlated with the true latent awareness effect, adding some noise to it.\n\nimport numpy as np\n\nrng = np.random.default_rng(42)\n\nproxy_variable = (\n    true_effect[[\"latent/awareness\"]] / true_effect[\"latent/awareness\"].max()\n)\nproxy_variable *= rng.uniform(0.9, 1.1, size=len(proxy_variable)).reshape((-1, 1))\n\nproxy_variable = proxy_variable.sample(n=180, random_state=42)\n\nfig, ax = plt.subplots(figsize=(10, 4))\nax.scatter(\n    proxy_variable.index.to_timestamp(),\n    proxy_variable[\"latent/awareness\"].values,\n    color=\"C1\",\n    label=\"Proxy Variable\",\n)",
    "crumbs": [
      "Awareness: Latent and Proxy Variables"
    ]
  },
  {
    "objectID": "mmm/proxy_variables.html#defining-and-fitting-the-model",
    "href": "mmm/proxy_variables.html#defining-and-fitting-the-model",
    "title": "Awareness: Latent and Proxy Variables",
    "section": "2. Defining and Fitting the Model",
    "text": "2. Defining and Fitting the Model\nWe define a model that includes:\n\nPiecewise linear trend\nYearly and weekly seasonality\nLatent awareness, modeled a saturation function applied to the ad spend.\nAwareness-to-sales effect, an effect that accounts the carryover aspect of awareness impact on sales.\nLatent baseline, modeled as the sum of trend, seasonality, and awareness-to-sales effect\nLast-click spend effect, modeled as a Hill saturation function multiplied by the latent baseline\n\nThe diagram below illustrates the model structure, including the latent variables and the proxy variable.\n\n\n\n\n\ngraph TD\n    %% STYLES\n    classDef latent fill:#dbeafe,stroke:#3b82f6,stroke-width:1px;\n    classDef observed fill:#fef3c7,stroke:#f59e0b,stroke-width:1px;\n    classDef proxy fill:#dcfce7,stroke:#16a34a,stroke-width:1px;\n    classDef effect fill:#f3e8ff,stroke:#a855f7,stroke-width:1px;\n\n    %% OBSERVED INPUTS\n    X1[\"Ad Spend Awareness\"]:::observed\n    LastClick[\"Last-Click Spend\"]:::observed\n    Sales[\"Sales (y)\"]:::observed\n\n    %% LATENT STRUCTURE\n    A_spend[\"Latent Ad Spend Awareness\"]:::latent\n    A_sum[\"Latent Awareness\"]:::latent\n    A_adstock[\"Latent Awareness Adstock\"]:::latent\n    Baseline[\"Latent Baseline (trend + seasonality + awareness)\"]:::latent\n    AwarenessToSales[\"Awareness → Sales Effect\"]:::effect\n\n    %% PROXY\n    Proxy[\"Proxy Variable (e.g., Branded Search / Survey)\"]:::proxy\n\n    %% RELATIONSHIPS\n    X1 -- \"Hill saturation function (nonlinear spend response)\" --&gt; A_spend\n\n    A_spend -- \"Summed contribution\" --&gt; A_sum\n    \n\n    A_sum -- \"Weibull adstock (carryover decay)\" --&gt; A_adstock\n    A_adstock -- \"Modulates baseline & seasonality\" --&gt; Baseline\n    A_adstock -- \"Drives multiplicative Awareness→Sales effect\" --&gt; AwarenessToSales\n\n    AwarenessToSales -- \"Direct causal effect on sales\" --&gt; Sales\n\n    Baseline -- \"Combines with last-click spend\" --&gt; LastClick\n    LastClick -- \"Final multiplicative effect on sales\" --&gt; Sales\n\n    A_sum -- \"Proportional proxy (γ·A + noise)\" --&gt; Proxy\n\n    %% GROUPS\n    subgraph Observed\n        X1\n        LastClick\n        Sales\n        Proxy\n    end\n\n    subgraph Latent_Model\n        A_spend\n        A_sum\n        A_adstock\n        Baseline\n        AwarenessToSales\n    end\n\n\n\n\n\n\nWe model the last-click effect as proportional to the latent brand awareness variable, which is influenced by upper-funnel marketing activities:\n\\[\n\\text{Hill}(X_{lc}(t)) \\cdot \\text{Baseline}(t)\n\\]\nwhere \\(\\text{Baseline}(t)\\) includes the latent brand awareness effect, trend, and seasonality components. We basically assume that the last-click effect is stronger when brand awareness is higher, which is a reasonable assumption in many marketing contexts.\n\n2.1. Model without Proxy Variable\nWe first fit a baseline model without the proxy variable to see how it performs. Since last-click is highly correlated with sales, we will see how it overfits to last-click, leading to poor estimation of other components.\nThe cell below defines some effects (click to expand):\n\n\nCode\nfrom prophetverse.effects import (\n    PiecewiseLinearTrend,\n    LinearFourierSeasonality,\n    ChainedEffects,\n    GeometricAdstockEffect,\n    WeibullAdstockEffect,\n    HillEffect,\n    SumEffects,\n    Forward,\n    Constant,\n    MultiplyEffects,\n)\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.engine.optimizer import LBFGSSolver\nimport numpyro.distributions as dist\n\n\n# --- Defining seasonality and trend effects ---\n\ntrend = PiecewiseLinearTrend(changepoint_interval=300)\n\nyearly = (\n    \"yearly_seasonality\",\n    LinearFourierSeasonality(\n        freq=\"D\",\n        sp_list=[365.25],\n        fourier_terms_list=[5],\n        prior_scale=0.1,\n        effect_mode=\"multiplicative\",\n    ),\n    None,\n)\n\nweekly = (\n    \"weekly_seasonality\",\n    LinearFourierSeasonality(\n        freq=\"D\",\n        sp_list=[7],\n        fourier_terms_list=[3],\n        prior_scale=0.05,\n        effect_mode=\"multiplicative\",\n    ),\n    None,\n)\n\n\n# --- Defining marketing effects ---\n\n# First, we set up a Hill saturation object to be reused\nhill = HillEffect(\n    half_max_prior=dist.HalfNormal(1),\n    slope_prior=dist.InverseGamma(2, 1),\n    max_effect_prior=dist.HalfNormal(0.5),\n    effect_mode=\"additive\",\n    input_scale=1e6,\n)\n\n\n# The effect of ad spend on awareness is modeled with a Hill function\n# (nonlinear spend response)\nspend_awareness = (\n    \"latent/awareness\",\n    hill,\n    \"ad_spend_awareness\",\n)\n\n\n# The awareness does not impact sales immediately, but rather has a carryover effect. We model this with a Weibull adstock.\nawareness_to_sales = (\n    \"awareness_to_sales\",\n    ChainedEffects(\n        [\n            (\"saturation\", Forward(\"latent/awareness\")),\n            (\"adstock\", WeibullAdstockEffect(max_lag=90)),\n        ]\n    ),\n    None,\n)\n\n# The baseline is finally modeled as the sum of trend, seasonality, and latent awareness (considering adstock)\nlatent_baseline = (\n    \"latent/baseline\",\n    SumEffects(\n        effects=[\n            (\"trend\", Forward(\"trend\")),\n            (\"yearly_seasonality\", Forward(\"yearly_seasonality\")),\n            (\"weekly_seasonality\", Forward(\"weekly_seasonality\")),\n            (\"awareness\", Forward(\"awareness_to_sales\")),\n        ]\n    ),\n    None,\n)\n\n\nchained_last_click = (\n    \"last_click_spend\",\n    MultiplyEffects(effects=[(\"hill\", hill), \n    (\"baseline\", Forward(\"latent/baseline\"))]),\n    \"last_click_spend\",\n)\n\n\n\nbaseline_model = Prophetverse(\n    trend=trend,\n    exogenous_effects=[\n        yearly,\n        weekly,\n        spend_awareness,\n        awareness_to_sales,\n        latent_baseline,\n        chained_last_click,\n    ],\n    inference_engine=MAPInferenceEngine(\n        num_steps=5000,\n        optimizer=LBFGSSolver(memory_size=300, max_linesearch_steps=300),\n    ),\n)\nbaseline_model.fit(y=y, X=X)\n\nProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('latent/awareness',\n                                 HillEf...\n                                                                      slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7fe9048db450 with batch shape () and event shape ()&gt;)),\n                                                          ('baseline',\n                                                           Forward(effect_name='latent/baseline'))]),\n                                 'last_click_spend')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=300))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('latent/awareness',\n                                 HillEf...\n                                                                      slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7fe9048db450 with batch shape () and event shape ()&gt;)),\n                                                          ('baseline',\n                                                           Forward(effect_name='latent/baseline'))]),\n                                 'last_click_spend')],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=300))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=300)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=0.1, sp_list=[365.25])LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[3],\n                         freq='D', prior_scale=0.05, sp_list=[7])HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7fe8e4ee6f90 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7fe9040ada10 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7fe9048db450 with batch shape () and event shape ()&gt;)awareness_to_salesChainedEffects(steps=[('saturation', Forward(effect_name='latent/awareness')),\n                      ('adstock', WeibullAdstockEffect(max_lag=90))])ForwardForward(effect_name='latent/awareness')WeibullAdstockEffectWeibullAdstockEffect(max_lag=90)latent/baselineSumEffects(effects=[('trend', Forward(effect_name='trend')),\n                    ('yearly_seasonality',\n                     Forward(effect_name='yearly_seasonality')),\n                    ('weekly_seasonality',\n                     Forward(effect_name='weekly_seasonality')),\n                    ('awareness', Forward(effect_name='awareness_to_sales'))])ForwardForward(effect_name='trend')ForwardForward(effect_name='yearly_seasonality')ForwardForward(effect_name='weekly_seasonality')ForwardForward(effect_name='awareness_to_sales')last_click_spendMultiplyEffects(effects=[('hill',\n                          HillEffect(effect_mode='additive',\n                                     half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7fe8e4ee6f90 with batch shape () and event shape ()&gt;,\n                                     input_scale=1000000.0,\n                                     max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7fe9040ada10 with batch shape () and event shape ()&gt;,\n                                     slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7fe9048db450 with batch shape () and event shape ()&gt;)),\n                         ('baseline', Forward(effect_name='latent/baseline'))])HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7fe8e4ee6f90 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7fe9040ada10 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7fe9048db450 with batch shape () and event shape ()&gt;)ForwardForward(effect_name='latent/baseline')inference_engineMAPInferenceEngineMAPInferenceEngine(num_steps=5000,\n                   optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                         memory_size=300))\n\n\nLet’s visualize the predictions of this baseline model:\n\ny_pred = baseline_model.predict(X=X, fh=X.index)\n\nplt.figure(figsize=(8, 4))\ny.plot(label=\"Observed\")\ny_pred.plot(label=\"Predicted\")\nplt.title(\"In-Sample Forecast: Observed vs Predicted\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nForecast quality alone does not tell the full story of how well the model is capturing the underlying dynamics.\nComponent-Level Diagnostics\nWith predict_components, we can obtain the model’s components.\n\ny_pred_components_baseline = baseline_model.predict_components(X=X, fh=X.index)\n\nSince we have access to the ground truth of the components in this synthetic example, we can compare them to see how well the model is capturing the true effects.\n\nfig, axs = plt.subplots(5, 1, figsize=(8, 12), sharex=True)\nfor i, name in enumerate(\n    [\n        \"trend\",\n        \"yearly_seasonality\",\n        \"latent/awareness\",\n        \"awareness_to_sales\",\n        \"last_click_spend\",\n    ]\n):\n    true_effect[name].plot(ax=axs[i], label=\"True\", color=\"black\")\n    y_pred_components_baseline[name].plot(ax=axs[i], label=\"Estimated\")\n    axs[i].set_title(name)\n    axs[i].legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSince the last-click spend is highly correlated with sales, the model tends to over-attribute sales to the last-click effect, leading to poor estimation of other components, especially the latent awareness effect.\n\n\n2.2. Model with Proxy Variable\nWe use LinearProxyLikelihood to add the proxy variable to the model. This effect links the latent awareness variable to the observed proxy variable, helping to better identify the latent effect.\nSince we know that the correlation is positive, we use a HalfNormal prior for the coefficient.\n\nfrom prophetverse.effects.proxy_likelihood import LinearProxyLikelihood\n\n\nproxy_effect = (\n    \"awareness_proxy\",\n    LinearProxyLikelihood(\n        effect_name=\"latent/awareness\",\n        reference_df=proxy_variable,\n        coefficient_prior=dist.HalfNormal(0.2),\n        likelihood_scale=0.05,\n    ),\n    None,\n)\n\nmodel = Prophetverse(\n    trend=PiecewiseLinearTrend(changepoint_interval=300),\n    exogenous_effects=[\n        yearly,\n        weekly,\n        spend_awareness,\n        awareness_to_sales,\n        latent_baseline,\n        chained_last_click,\n        proxy_effect,\n    ],\n    inference_engine=MAPInferenceEngine(\n        num_steps=5000,\n        optimizer=LBFGSSolver(memory_size=300, max_linesearch_steps=300),\n    ),\n)\n\nmodel.fit(y=y, X=X)\n\nProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('latent/awareness',\n                                 HillEf...\n2004-05-15          0.842808\n2002-11-06          0.076063\n2002-12-31          0.079602\n2002-03-06          0.744027\n...                      ...\n2004-10-21          0.351357\n2004-05-23          0.813127\n2000-05-29          0.914647\n2003-06-26          0.040826\n2001-03-17          0.791952\n\n[180 rows x 1 columns]),\n                                 None)],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=300))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('yearly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[5],\n                                                          freq='D',\n                                                          prior_scale=0.1,\n                                                          sp_list=[365.25]),\n                                 None),\n                                ('weekly_seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[3],\n                                                          freq='D',\n                                                          prior_scale=0.05,\n                                                          sp_list=[7]),\n                                 None),\n                                ('latent/awareness',\n                                 HillEf...\n2004-05-15          0.842808\n2002-11-06          0.076063\n2002-12-31          0.079602\n2002-03-06          0.744027\n...                      ...\n2004-10-21          0.351357\n2004-05-23          0.813127\n2000-05-29          0.914647\n2003-06-26          0.040826\n2001-03-17          0.791952\n\n[180 rows x 1 columns]),\n                                 None)],\n             inference_engine=MAPInferenceEngine(num_steps=5000,\n                                                 optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                                                       memory_size=300)),\n             trend=PiecewiseLinearTrend(changepoint_interval=300))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=300)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[5],\n                         freq='D', prior_scale=0.1, sp_list=[365.25])LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[3],\n                         freq='D', prior_scale=0.05, sp_list=[7])HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7fe8e4ee6f90 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7fe9040ada10 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7fe9048db450 with batch shape () and event shape ()&gt;)awareness_to_salesChainedEffects(steps=[('saturation', Forward(effect_name='latent/awareness')),\n                      ('adstock', WeibullAdstockEffect(max_lag=90))])ForwardForward(effect_name='latent/awareness')WeibullAdstockEffectWeibullAdstockEffect(max_lag=90)latent/baselineSumEffects(effects=[('trend', Forward(effect_name='trend')),\n                    ('yearly_seasonality',\n                     Forward(effect_name='yearly_seasonality')),\n                    ('weekly_seasonality',\n                     Forward(effect_name='weekly_seasonality')),\n                    ('awareness', Forward(effect_name='awareness_to_sales'))])ForwardForward(effect_name='trend')ForwardForward(effect_name='yearly_seasonality')ForwardForward(effect_name='weekly_seasonality')ForwardForward(effect_name='awareness_to_sales')last_click_spendMultiplyEffects(effects=[('hill',\n                          HillEffect(effect_mode='additive',\n                                     half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7fe8e4ee6f90 with batch shape () and event shape ()&gt;,\n                                     input_scale=1000000.0,\n                                     max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7fe9040ada10 with batch shape () and event shape ()&gt;,\n                                     slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7fe9048db450 with batch shape () and event shape ()&gt;)),\n                         ('baseline', Forward(effect_name='latent/baseline'))])HillEffectHillEffect(effect_mode='additive',\n           half_max_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7fe8e4ee6f90 with batch shape () and event shape ()&gt;,\n           input_scale=1000000.0,\n           max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7fe9040ada10 with batch shape () and event shape ()&gt;,\n           slope_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7fe9048db450 with batch shape () and event shape ()&gt;)ForwardForward(effect_name='latent/baseline')LinearProxyLikelihoodLinearProxyLikelihood(coefficient_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7fe8d0c7df90 with batch shape () and event shape ()&gt;,\n                      effect_name='latent/awareness',\n                      reference_df=            latent/awareness\n2001-07-26          0.925163\n2004-05-15          0.842808\n2002-11-06          0.076063\n2002-12-31          0.079602\n2002-03-06          0.744027\n...                      ...\n2004-10-21          0.351357\n2004-05-23          0.813127\n2000-05-29          0.914647\n2003-06-26          0.040826\n2001-03-17          0.791952\n\n[180 rows x 1 columns])inference_engineMAPInferenceEngineMAPInferenceEngine(num_steps=5000,\n                   optimizer=LBFGSSolver(max_linesearch_steps=300,\n                                         memory_size=300))\n\n\nLet’s visualize the predictions of this proxy variable model:\n\ny_pred_model = model.predict(X=X, fh=X.index)\n\nplt.figure(figsize=(8, 4))\ny.plot(label=\"Observed\")\ny_pred_model.plot(label=\"Predicted\")\nplt.title(\"In-Sample Forecast: Observed vs Predicted\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nComponent-Level Diagnostics\nWith predict_components, we can obtain the model’s components.\n\ny_pred_components = model.predict_components(X=X, fh=X.index)\n\nIn a real use-casee, you would not have access to the ground truth of the components. We use them here to show how the model behaves, and how incorporing extra information can improve it.\n\nfig, axs = plt.subplots(5, 1, figsize=(8, 12), sharex=True)\nfor i, name in enumerate(\n    [\n        \"trend\",\n        \"yearly_seasonality\",\n        \"latent/awareness\",\n        \"awareness_to_sales\",\n        \"last_click_spend\",\n    ]\n):\n\n    true_effect[name].plot(ax=axs[i], label=\"True\", color=\"black\")\n    y_pred_components[name].plot(ax=axs[i], label=\"Estimated\")\n    axs[i].set_title(name)\n    axs[i].legend()\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Awareness: Latent and Proxy Variables"
    ]
  },
  {
    "objectID": "mmm/proxy_variables.html#comparing-models",
    "href": "mmm/proxy_variables.html#comparing-models",
    "title": "Awareness: Latent and Proxy Variables",
    "section": "3. Comparing models",
    "text": "3. Comparing models\nHere, we do our final comparison of the models, looking at the counterfactual impact of zeroing out the awareness ad spend. This shows how important it is to include the proxy variable to correctly attribute the impact of awareness spend.\n\ndef plot_compare_models(column):\n\n    delta_true = get_counterfactual(true_model, X, column)\n    delta_baseline = get_counterfactual(baseline_model, X, column)\n    delta_proxy = get_counterfactual(model, X, column)\n\n    plt.figure(figsize=(8, 4))\n    delta_true.plot(label=\"True Effect\", color=\"black\")\n    delta_baseline.plot(label=\"Baseline Model\")\n    delta_proxy.plot(label=\"Proxy Variable Model\")\n    plt.title(f\"Counterfactual Impact of Zeroing {column}\")\n    plt.legend()\n    plt.show()\n\n\nplot_compare_models(\"ad_spend_awareness\")\n\n\n\n\n\n\n\n\nWe see that, although not perfect, the model with the proxy variable is able to much better capture the true impact of awareness spend on sales, while the baseline model without the proxy variable fails to do so.",
    "crumbs": [
      "Awareness: Latent and Proxy Variables"
    ]
  },
  {
    "objectID": "mmm/proxy_variables.html#how-to-cite-this-package",
    "href": "mmm/proxy_variables.html#how-to-cite-this-package",
    "title": "Awareness: Latent and Proxy Variables",
    "section": "How to cite this package",
    "text": "How to cite this package\nIf you use Prophetverse or any of this idea in your package/paper, please cite this package according to DOI on Readme.",
    "crumbs": [
      "Awareness: Latent and Proxy Variables"
    ]
  },
  {
    "objectID": "mmm/mediators_and_frontdoor.html",
    "href": "mmm/mediators_and_frontdoor.html",
    "title": "Mediator variables, sales funnel and frontdoor adjustment",
    "section": "",
    "text": "In Marketing Mix Modeling (MMM), we often encounter variables that sit between the marketing investment and the final conversion (sales). These are called mediators. Common examples include website visits, app installs, lead generation, or footfall in a store.\nA common dilemma is whether to include these mediators in the MMM:\nThis tutorial demonstrates how to use Prophetverse to model this causal chain explicitly. We will build a model that respects the funnel structure: Investment drives Visits, and Visits drive Sales. This allows us to use the data from visits to improve our estimates without breaking the causal inference for investment ROI.",
    "crumbs": [
      "Mediators and Front-door Adjustment"
    ]
  },
  {
    "objectID": "mmm/mediators_and_frontdoor.html#data-generation",
    "href": "mmm/mediators_and_frontdoor.html#data-generation",
    "title": "Mediator variables, sales funnel and frontdoor adjustment",
    "section": "Data Generation",
    "text": "Data Generation\nFirst, let’s generate some synthetic data where the ground truth is known. We assume a simple funnel:\n\nInvestment drives Visits (with adstock and saturation).\nVisits drive Sales (with a conversion rate).\nThere is no direct path from Investment to Sales (Direct Effect = 0).\nThere is also some seasonality affecting both.\n\n\n\n\n\n\ngraph LR\n    I[Investment] --&gt; V[Visits]\n    V --&gt; S[Sales]\n    Se[Seasonality] --&gt; I\n    Se --&gt; S\n\n\n\n\n\n\n\n\nCode\nfrom prophetverse import (\n    Prophetverse,\n    Constant,\n    LinearFourierSeasonality,\n    GeometricAdstockEffect,\n    MultiplyEffects,\n    Forward,\n    IgnoreInput,\n    MichaelisMentenEffect,\n    PriorPredictiveInferenceEngine,\n    ChainedEffects,\n)\nfrom prophetverse.utils.regex import exact\nimport numpyro.distributions as dist\nimport numpy as np\nimport pandas as pd\n\n# Synthetic data\n\n\nrng = np.random.default_rng(42)\nn = 356\nidx = pd.period_range(start=\"2020-01-01\", periods=n, freq=\"D\")\nX = pd.DataFrame(index=idx)\nX[\"investment\"] = rng.normal(loc=500, scale=100, size=n).clip(min=0)\nX[\"investment\"] += 100 * np.sin(np.linspace(0, n, n) * 2 * np.pi / 30.25)  # seasonality\n# Smooth a little\nX[\"investment\"] = X[\"investment\"].rolling(window=7, min_periods=1).mean()\n\n\ntrue_model = Prophetverse(\n    trend=Constant(prior=dist.Delta(1000.0)),\n    inference_engine=PriorPredictiveInferenceEngine(),\n    exogenous_effects=[\n        (\n            \"latent/seasonality\",\n            LinearFourierSeasonality(\n                sp_list=[30.25], fourier_terms_list=[1], freq=\"D\", prior_scale=0.5\n            ),\n            None,\n        ),\n        (\n            \"seasonality\",\n            MultiplyEffects(\n                effects=[\n                    (\"trend\", Forward(\"trend\")),\n                    (\"seasonality\", Forward(\"latent/seasonality\")),\n                ]\n            ),\n            None,\n        ),\n        (\n            \"latent/visits\",\n            MultiplyEffects(\n                effects=[\n                    (\"trend\", Forward(\"trend\")),\n                    (\n                        \"investment\",\n                        ChainedEffects(\n                            steps=[\n                                (\n                                    \"adstock\",\n                                    GeometricAdstockEffect(\n                                        decay_prior=dist.Delta(0.5),\n                                        normalize=True,\n                                    ),\n                                ),\n                                (\n                                    \"saturation\",\n                                    MichaelisMentenEffect(\n                                        \"additive\",\n                                        max_effect_prior=dist.Delta(2.1),\n                                        half_saturation_prior=dist.Delta(400.0),\n                                    ),\n                                ),\n                            ]\n                        ),\n                    ),\n                ],\n            ),\n            exact(\"investment\"),\n        ),\n        (\n            \"visit_sales\",\n            MultiplyEffects(\n                effects=[\n                    (\"awareness\", Forward(\"latent/visits\")),\n                    (\n                        \"conversion\",\n                        Constant(prior=dist.Delta(0.5)),\n                    ),\n                ]\n            ),\n            None,\n        ),\n    ],\n)\n\n# The y will be ignored - we just want to sample from the prior predictive\ntrue_model.fit(y=pd.Series(index=idx, data=0), X=X)\ncomponents = true_model.predict_components(X=X, fh=X.index)\n\ny = components[\"obs\"].to_frame(\"sales\") * rng.normal(\n    loc=1.0, scale=0.01, size=n\n).reshape((-1, 1))\nX[\"visits\"] = components[\"latent/visits\"]\n\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(figsize=(6, 6), nrows=3, sharex=True)\ny.plot(ax=axs[0], title=\"Sales\", color=\"C0\")\naxs[0].set_ylabel(\"Sales\")\n\nX[\"visits\"].plot(ax=axs[1], title=\"Visits\", color=\"C1\")\naxs[1].set_ylabel(\"Visits\")\nX[\"investment\"].plot(ax=axs[2], title=\"Ad Investment\", color=\"C2\")\naxs[2].set_ylabel(\"Investment\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom prophetverse import (\n    Prophetverse,\n    Constant,\n    LinearFourierSeasonality,\n    GeometricAdstockEffect,\n    MultiplyEffects,\n    Forward,\n    IgnoreInput,\n    MichaelisMentenEffect,\n    MAPInferenceEngine,\n    ChainedEffects,\n    FlatTrend,\n    CoupledExactLikelihood,\n    Identity,\n)\nfrom prophetverse.utils.regex import exact\nimport numpyro.distributions as dist\nimport numpy as np\nimport pandas as pd\nimport numpyro\n\nnumpyro.enable_x64()\n\nTo evaluate our models, we define a helper function to calculate the Total Treatment Effect of the investment. This is done by comparing the predicted sales with the actual investment versus a counterfactual scenario where investment is zero.\n\ndef get_treatment_effect(model, X):\n    # True model (unobserved)\n    y_true_cf_0 = model.predict(X=X.assign(investment=0.0), fh=X.index)\n    y_true_cf_1 = model.predict(X=X, fh=X.index)\n    return y_true_cf_1 - y_true_cf_0\n\ndelta_true = get_treatment_effect(true_model, X)",
    "crumbs": [
      "Mediators and Front-door Adjustment"
    ]
  },
  {
    "objectID": "mmm/mediators_and_frontdoor.html#naive-model---using-mediator-as-input",
    "href": "mmm/mediators_and_frontdoor.html#naive-model---using-mediator-as-input",
    "title": "Mediator variables, sales funnel and frontdoor adjustment",
    "section": "Naive model - using mediator as input",
    "text": "Naive model - using mediator as input\nIn the naive approach, we simply include both investment and visits as features in the model. We might think that “more data is better”, but in causal inference, this is a classic mistake known as adjusting for a mediator.\nSince visits is a direct consequence of investment and a direct cause of sales, including it in the regression “blocks” the path from investment to sales. The model will attribute the sales lift to visits (which is closer to the outcome) and find little to no effect for investment.\nLet’s see what happens when we fit this model.\n\nfrom prophetverse import (\n    Prophetverse,\n    Constant,\n    LinearFourierSeasonality,\n    GeometricAdstockEffect,\n    MultiplyEffects,\n    Forward,\n    IgnoreInput,\n    MichaelisMentenEffect,\n    MAPInferenceEngine,\n    ChainedEffects,\n    FlatTrend,\n    CoupledExactLikelihood,\n    Identity,\n    LinearEffect,\n)\nfrom prophetverse.utils.regex import exact\nimport numpyro.distributions as dist\nimport numpy as np\nimport pandas as pd\nimport numpyro\n\nnumpyro.enable_x64()\n\n\nseasonality = (\n    \"seasonality\",\n    MultiplyEffects(\n        effects=[\n            (\"trend\", Forward(\"trend\")),\n            (\n                \"seasonality\",\n                LinearFourierSeasonality(\n                    sp_list=[30.25],\n                    fourier_terms_list=[1],\n                    freq=\"D\",\n                    prior_scale=0.1,\n                ),\n            ),\n        ]\n    ),\n    None,\n)\n\n\ninvestment_saturation_adstock = MultiplyEffects(\n    effects=[\n        (\"trend\", Forward(\"trend\")),\n        (\n            \"investment\",\n            ChainedEffects(\n                steps=[\n                    (\n                        \"adstock\",\n                        GeometricAdstockEffect(\n                            decay_prior=dist.InverseGamma(4, 2),\n                            normalize=True,\n                        ),\n                    ),\n                    (\n                        \"saturation\",\n                        MichaelisMentenEffect(\n                            \"additive\",\n                            max_effect_prior=dist.HalfNormal(1),\n                            half_saturation_prior=dist.HalfNormal(400),\n                        ),\n                    ),\n                ]\n            ),\n        ),\n    ],\n)\n\n\nnaive_model = Prophetverse(\n    trend=FlatTrend(changepoint_prior_scale=1000),\n    inference_engine=MAPInferenceEngine(progress_bar=True),\n    exogenous_effects=[\n        # Multiplicative seasonality\n        seasonality,\n        # Investment\n        (\n            \"investment\",\n            investment_saturation_adstock,\n            exact(\"investment\"),\n        ),\n        # Visits\n        (\n            \"visit_sales\",\n            LinearEffect(\"additive\", prior=dist.Beta(2, 2)),\n            exact(\"visits\"),\n        ),\n    ],\n    scale=1,\n)\n\nnaive_model.fit(y=y, X=X)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]100%|██████████| 1/1 [00:04&lt;00:00,  4.02s/it, init loss: 6820.3190, avg. loss [1-1]: 6820.3190]100%|██████████| 1/1 [00:04&lt;00:00,  4.02s/it, init loss: 6820.3190, avg. loss [1-1]: 6820.3190]\n\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 MultiplyEffects(effects=[('trend',\n                                                           Forward(effect_name='trend')),\n                                                          ('seasonality',\n                                                           LinearFourierSeasonality(fourier_terms_list=[1],\n                                                                                    freq='D',\n                                                                                    prior_scale=0.1,\n                                                                                    sp_list=[30.25]))]),\n                                 None),\n                                ('investment',\n                                 MultiplyEffects(effects=[('trend',\n                                                           Forward(effect_name='trend')),\n                                                          ('investment',\n                                                           ChainedEffects(steps=[('adstock',...\n                                                                                                        max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f79981c4510 with batch shape () and event shape ()&gt;))]))]),\n                                 '^investment$'),\n                                ('visit_sales',\n                                 LinearEffect(effect_mode='additive',\n                                              prior=&lt;numpyro.distributions.continuous.Beta object at 0x7f79b028ae50 with batch shape () and event shape ()&gt;),\n                                 '^visits$')],\n             inference_engine=MAPInferenceEngine(progress_bar=True), scale=1,\n             trend=FlatTrend(changepoint_prior_scale=1000))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 MultiplyEffects(effects=[('trend',\n                                                           Forward(effect_name='trend')),\n                                                          ('seasonality',\n                                                           LinearFourierSeasonality(fourier_terms_list=[1],\n                                                                                    freq='D',\n                                                                                    prior_scale=0.1,\n                                                                                    sp_list=[30.25]))]),\n                                 None),\n                                ('investment',\n                                 MultiplyEffects(effects=[('trend',\n                                                           Forward(effect_name='trend')),\n                                                          ('investment',\n                                                           ChainedEffects(steps=[('adstock',...\n                                                                                                        max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f79981c4510 with batch shape () and event shape ()&gt;))]))]),\n                                 '^investment$'),\n                                ('visit_sales',\n                                 LinearEffect(effect_mode='additive',\n                                              prior=&lt;numpyro.distributions.continuous.Beta object at 0x7f79b028ae50 with batch shape () and event shape ()&gt;),\n                                 '^visits$')],\n             inference_engine=MAPInferenceEngine(progress_bar=True), scale=1,\n             trend=FlatTrend(changepoint_prior_scale=1000))effectsFlatTrendFlatTrend(changepoint_prior_scale=1000)seasonalityMultiplyEffects(effects=[('trend', Forward(effect_name='trend')),\n                         ('seasonality',\n                          LinearFourierSeasonality(fourier_terms_list=[1],\n                                                   freq='D', prior_scale=0.1,\n                                                   sp_list=[30.25]))])ForwardForward(effect_name='trend')LinearFourierSeasonalityLinearFourierSeasonality(fourier_terms_list=[1], freq='D', prior_scale=0.1,\n                         sp_list=[30.25])investmentMultiplyEffects(effects=[('trend', Forward(effect_name='trend')),\n                         ('investment',\n                          ChainedEffects(steps=[('adstock',\n                                                 GeometricAdstockEffect(decay_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f799827fb50 with batch shape () and event shape ()&gt;,\n                                                                        normalize=True)),\n                                                ('saturation',\n                                                 MichaelisMentenEffect(effect_mode='additive',\n                                                                       half_saturation_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f7998356790 with batch shape () and event shape ()&gt;,\n                                                                       max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f79981c4510 with batch shape () and event shape ()&gt;))]))])ForwardForward(effect_name='trend')investment: ChainedEffectsChainedEffects(steps=[('adstock',\n                       GeometricAdstockEffect(decay_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f799827fb50 with batch shape () and event shape ()&gt;,\n                                              normalize=True)),\n                      ('saturation',\n                       MichaelisMentenEffect(effect_mode='additive',\n                                             half_saturation_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f7998356790 with batch shape () and event shape ()&gt;,\n                                             max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f79981c4510 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect(decay_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f799827fb50 with batch shape () and event shape ()&gt;,\n                       normalize=True)MichaelisMentenEffectMichaelisMentenEffect(effect_mode='additive',\n                      half_saturation_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f7998356790 with batch shape () and event shape ()&gt;,\n                      max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f79981c4510 with batch shape () and event shape ()&gt;)LinearEffectLinearEffect(effect_mode='additive',\n             prior=&lt;numpyro.distributions.continuous.Beta object at 0x7f79b028ae50 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine(progress_bar=True)\n\n\n\ndelta_naive = get_treatment_effect(naive_model, X)\n\nfig, ax = plt.subplots(figsize=(6, 4))\ndelta_true.plot(ax=ax, label=\"True Counterfactual Effect\", color=\"C0\")\ndelta_naive[\"sales\"].plot(\n    ax=ax,\n    label=\"Naive Estimated Counterfactual Effect\",\n    color=\"C1\",\n    linestyle=\"--\",\n)\nax.set_title(\"Sales Prediction with mediator as input\")\nax.set_ylabel(\"Sales\")\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the naive model significantly underestimates the effect of investment. It attributes the sales to visits, ignoring that investment caused those visits.",
    "crumbs": [
      "Mediators and Front-door Adjustment"
    ]
  },
  {
    "objectID": "mmm/mediators_and_frontdoor.html#front-door-adjustment-model",
    "href": "mmm/mediators_and_frontdoor.html#front-door-adjustment-model",
    "title": "Mediator variables, sales funnel and frontdoor adjustment",
    "section": "Front-door adjustment model",
    "text": "Front-door adjustment model\nTo correctly estimate the effect of investment while utilizing the data from visits, we need to model the causal structure. This is often called a Front-door adjustment or simply structural modeling.\nIn Prophetverse, we can achieve this by defining a Latent Variable for visits.\n\nLatent Visits: We define latent/visits as a function of investment (plus trend/seasonality). This represents the “expected visits” given the investment.\nCoupling: We link this latent/visits to the observed visits data using CoupledExactLikelihood. This tells the model: “The latent/visits variable should be close to the observed visits data.”\nSales Model: We use latent/visits as an input to explain sales.\n\nThis way, investment is the source of the variation in latent/visits, which in turn explains sales. The model understands that investment causes sales through visits.\n\n\n\n\n\ngraph LR\n    I[Investment] --&gt; LV((Latent Visits))\n    LV -.-&gt;|Coupled| V[Observed Visits]\n    LV --&gt; S[Sales]\n\n\n\n\n\n\nLet’s implement this in Prophetverse.\n\nmodel = Prophetverse(\n    trend=FlatTrend(changepoint_prior_scale=1000),\n    inference_engine=MAPInferenceEngine(progress_bar=True),\n    exogenous_effects=[\n        # Multiplicative seasonality\n        seasonality,\n        # Now, the investment creates a latent component of \"visits\"\n        (\n            \"latent/visits\",\n            investment_saturation_adstock,\n            exact(\"investment\"),\n        ),\n        # We use identity to put the mediator as a latent component\n        (\n            \"latent/visit_input\",\n            Identity(),\n            exact(\"visits\"),\n        ),\n        # Both latent components should be coupled\n        (\n            \"latent/coupled_likelihood\",\n            CoupledExactLikelihood(\n                source_effect_name=\"latent/visits\",\n                target_effect_name=\"latent/visit_input\",\n                prior_scale=0.1,\n            ),\n            None,\n        ),\n        # We then convert the latent component to sales\n        (\n            \"visit_sales\",\n            MultiplyEffects(\n                effects=[\n                    (\"awareness\", Forward(\"latent/visits\")),\n                    (\n                        \"conversion\",\n                        Constant(prior=dist.Beta(2, 2)),\n                    ),\n                ]\n            ),\n            None,\n        ),\n    ],\n    scale=1,\n)\n\n\nmodel.fit(y=y, X=X)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]100%|██████████| 1/1 [00:06&lt;00:00,  6.82s/it, init loss: 6336.8862, avg. loss [1-1]: 6336.8862]100%|██████████| 1/1 [00:06&lt;00:00,  6.82s/it, init loss: 6336.8862, avg. loss [1-1]: 6336.8862]\n\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 MultiplyEffects(effects=[('trend',\n                                                           Forward(effect_name='trend')),\n                                                          ('seasonality',\n                                                           LinearFourierSeasonality(fourier_terms_list=[1],\n                                                                                    freq='D',\n                                                                                    prior_scale=0.1,\n                                                                                    sp_list=[30.25]))]),\n                                 None),\n                                ('latent/visits',\n                                 MultiplyEffects(effects=[('trend',\n                                                           Forward(effect_name='trend')),\n                                                          ('investment',\n                                                           ChainedEffects(steps=[('adstoc...\n                                                        target_effect_name='latent/visit_input'),\n                                 None),\n                                ('visit_sales',\n                                 MultiplyEffects(effects=[('awareness',\n                                                           Forward(effect_name='latent/visits')),\n                                                          ('conversion',\n                                                           Constant(prior=&lt;numpyro.distributions.continuous.Beta object at 0x7f7964f05190 with batch shape () and event shape ()&gt;))]),\n                                 None)],\n             inference_engine=MAPInferenceEngine(progress_bar=True), scale=1,\n             trend=FlatTrend(changepoint_prior_scale=1000))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 MultiplyEffects(effects=[('trend',\n                                                           Forward(effect_name='trend')),\n                                                          ('seasonality',\n                                                           LinearFourierSeasonality(fourier_terms_list=[1],\n                                                                                    freq='D',\n                                                                                    prior_scale=0.1,\n                                                                                    sp_list=[30.25]))]),\n                                 None),\n                                ('latent/visits',\n                                 MultiplyEffects(effects=[('trend',\n                                                           Forward(effect_name='trend')),\n                                                          ('investment',\n                                                           ChainedEffects(steps=[('adstoc...\n                                                        target_effect_name='latent/visit_input'),\n                                 None),\n                                ('visit_sales',\n                                 MultiplyEffects(effects=[('awareness',\n                                                           Forward(effect_name='latent/visits')),\n                                                          ('conversion',\n                                                           Constant(prior=&lt;numpyro.distributions.continuous.Beta object at 0x7f7964f05190 with batch shape () and event shape ()&gt;))]),\n                                 None)],\n             inference_engine=MAPInferenceEngine(progress_bar=True), scale=1,\n             trend=FlatTrend(changepoint_prior_scale=1000))effectsFlatTrendFlatTrend(changepoint_prior_scale=1000)seasonalityMultiplyEffects(effects=[('trend', Forward(effect_name='trend')),\n                         ('seasonality',\n                          LinearFourierSeasonality(fourier_terms_list=[1],\n                                                   freq='D', prior_scale=0.1,\n                                                   sp_list=[30.25]))])ForwardForward(effect_name='trend')LinearFourierSeasonalityLinearFourierSeasonality(fourier_terms_list=[1], freq='D', prior_scale=0.1,\n                         sp_list=[30.25])latent/visitsMultiplyEffects(effects=[('trend', Forward(effect_name='trend')),\n                         ('investment',\n                          ChainedEffects(steps=[('adstock',\n                                                 GeometricAdstockEffect(decay_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f799827fb50 with batch shape () and event shape ()&gt;,\n                                                                        normalize=True)),\n                                                ('saturation',\n                                                 MichaelisMentenEffect(effect_mode='additive',\n                                                                       half_saturation_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f7998356790 with batch shape () and event shape ()&gt;,\n                                                                       max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f79981c4510 with batch shape () and event shape ()&gt;))]))])ForwardForward(effect_name='trend')investment: ChainedEffectsChainedEffects(steps=[('adstock',\n                       GeometricAdstockEffect(decay_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f799827fb50 with batch shape () and event shape ()&gt;,\n                                              normalize=True)),\n                      ('saturation',\n                       MichaelisMentenEffect(effect_mode='additive',\n                                             half_saturation_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f7998356790 with batch shape () and event shape ()&gt;,\n                                             max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f79981c4510 with batch shape () and event shape ()&gt;))])GeometricAdstockEffectGeometricAdstockEffect(decay_prior=&lt;numpyro.distributions.continuous.InverseGamma object at 0x7f799827fb50 with batch shape () and event shape ()&gt;,\n                       normalize=True)MichaelisMentenEffectMichaelisMentenEffect(effect_mode='additive',\n                      half_saturation_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f7998356790 with batch shape () and event shape ()&gt;,\n                      max_effect_prior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f79981c4510 with batch shape () and event shape ()&gt;)IdentityIdentity()CoupledExactLikelihoodCoupledExactLikelihood(prior_scale=0.1, source_effect_name='latent/visits',\n                       target_effect_name='latent/visit_input')visit_salesMultiplyEffects(effects=[('awareness', Forward(effect_name='latent/visits')),\n                         ('conversion',\n                          Constant(prior=&lt;numpyro.distributions.continuous.Beta object at 0x7f7964f05190 with batch shape () and event shape ()&gt;))])ForwardForward(effect_name='latent/visits')ConstantConstant(prior=&lt;numpyro.distributions.continuous.Beta object at 0x7f7964f05190 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine(progress_bar=True)\n\n\n\ndelta_frontdoor = get_treatment_effect(model, X)\n\nfig, ax = plt.subplots(figsize=(6, 4))\ndelta_true.plot(ax=ax, label=\"True Counterfactual Effect\", color=\"C0\")\ndelta_naive[\"sales\"].plot(ax=ax, label=\"Naive Estimated Counterfactual Effect\", color=\"C1\", linestyle=\"--\")\ndelta_frontdoor[\"sales\"].plot(ax=ax, label=\"Front-door Estimated Counterfactual Effect\", color=\"C2\", linestyle=\"--\")\nax.set_title(\"Sales Prediction with Front-door Adjustment\")\nax.set_ylabel(\"Sales\")\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe plot above shows that the Front-door adjustment model (Green) recovers the True Counterfactual Effect (Blue) much better than the Naive model (Orange).\nBy explicitly modeling the mechanism Investment -&gt; Visits -&gt; Sales, we can:\n\nUse the visits data to constrain the model and reduce uncertainty.\nCorrectly attribute the sales lift to the original investment.\nEstimate the conversion rate from visits to sales separately from the effect of investment on visits.\n\nThis approach is powerful for MMM when you have rich funnel data and want to understand the full impact of your marketing activities.",
    "crumbs": [
      "Mediators and Front-door Adjustment"
    ]
  },
  {
    "objectID": "index.html#the-flexible-bayesian-forecasting-and-marketing-mix-modeling-package",
    "href": "index.html#the-flexible-bayesian-forecasting-and-marketing-mix-modeling-package",
    "title": "Prophetverse",
    "section": "The flexible bayesian forecasting and Marketing Mix Modeling package",
    "text": "The flexible bayesian forecasting and Marketing Mix Modeling package\nProphetverse leverages the theory behind the Prophet model for time series forecasting and expands it into a more general framework, enabling custom priors, non-linear effects for exogenous variables and other likelihoods. Built on top of sktime and numpyro, Prophetverse aims to provide a flexible and easy-to-use library for time series forecasting with a focus on interpretability and customizability. It is particularly useful for Marketing Mix Modeling, where understanding the effect of different marketing channels on sales is crucial."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Prophetverse",
    "section": "Getting started",
    "text": "Getting started\n\nInstallation\nTo install with pip:\npip install prophetverse\nOr with poetry:\npoetry add prophetverse\n\n\nForecasting with default values\nThe Prophetverse model provides an interface compatible with sktime. Here’s an example of how to use it:\nfrom prophetverse.sktime import Prophetverse\n\n# Create the model\nmodel = Prophetverse()\n\n# Fit the model\nmodel.fit(y=y, X=X)\n\n# Forecast in sample\ny_pred = model.predict(X=X, fh=y.index)"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Prophetverse",
    "section": "Features",
    "text": "Features\nProphetverse is similar to the original Prophet model in many aspects, but it has some differences and new features. The following table summarizes the main features of Prophetverse and compares them with the original Prophet model:\n\n\n\n\n\n\n\n\n\nFeature\nProphetverse\nOriginal Prophet\nMotivation\n\n\n\n\nLogistic trend\nCapacity as a random variable\nCapacity as a hyperparameter, user input required\nThe capacity is usually unknown by the users. Having it as a variable is useful for Total Addressable Market inference\n\n\nCustom trend\nCustomizable trend functions\nNot available\nUsers can create custom trends and leverage their knowledge about the timeseries to enhance long-term accuracy\n\n\nLikelihoods\nGaussian, Gamma, Negative Binomial, and Beta\nGaussian only\nGaussian likelihood fails to provide good forecasts to positive-only and count data (sales, for example)\n\n\nCustom priors\nSupports custom priors for model parameters and exogenous variables\nNot supported\nForcing positive coefficients, using prior knowledge to model the timeseries\n\n\nCustom exogenous effects\nNon-linear and customizable effects for exogenous variables, shared coefficients between time series\nNot available\nUsers can create any kind of relationship between exogenous variables and the timeseries, which can be useful for Marketing Mix Modeling and other applications.\n\n\nChangepoints\nUses changepoint interval\nUses changepoint number\nThe changepoint number is not stable in the sense that, when the size of timeseries increases, its impact on forecast changes. Think about setting a changepoint number when timeseries has 6 months, and forecasting in future with 2 years of data (4x time original size). Re-tuning would be required. Prophetverse is expected to be more stable\n\n\nScaling\nTime series scaled internally, exogenous variables scaled by the user\nTime series scaled internally\nScaling y is needed to enhance user experience with hyperparameters. On the other hand, not scaling the exogenous variables provide more control to the user and they can leverage sktime’s transformers to handle that.\n\n\nSeasonality\nFourier terms for seasonality passed as exogenous variables\nBuilt-in seasonality handling\nSetting up seasonality requires almost zero effort by using LinearFourierSeasonality in Prophetverse. The idea is to allow the user to create custom seasonalities easily, without hardcoding it in the code.\n\n\nMultivariate model\nHierarchical model with multivariate normal likelihood and LKJ prior, bottom-up forecast\nNot available\nHaving shared coefficients, using global information to enhance individual forecast.\n\n\nImplementation\nNumpyro\nStan"
  },
  {
    "objectID": "reference/GeometricAdstockEffect.html",
    "href": "reference/GeometricAdstockEffect.html",
    "title": "GeometricAdstockEffect",
    "section": "",
    "text": "effects.GeometricAdstockEffect(\n    self,\n    decay_prior=None,\n    raise_error_if_fh_changes=False,\n    normalize=False,\n)\nRepresents a Geometric Adstock effect in a time series model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndecay_prior\nDistribution\nPrior distribution for the decay parameter (controls the rate of decay).\nNone\n\n\nrase_error_if_fh_changes\nbool\nWhether to raise an error if the forecasting horizon changes during predict\nrequired\n\n\nnormalize\nbool\nIf True, scales the geometric carryover so a unit impulse sums to 1 (multiplies by (1 - decay)); keeps backwards compatibility when False.\nFalse",
    "crumbs": [
      "Exogenous effects",
      "GeometricAdstockEffect"
    ]
  },
  {
    "objectID": "reference/GeometricAdstockEffect.html#parameters",
    "href": "reference/GeometricAdstockEffect.html#parameters",
    "title": "GeometricAdstockEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndecay_prior\nDistribution\nPrior distribution for the decay parameter (controls the rate of decay).\nNone\n\n\nrase_error_if_fh_changes\nbool\nWhether to raise an error if the forecasting horizon changes during predict\nrequired\n\n\nnormalize\nbool\nIf True, scales the geometric carryover so a unit impulse sums to 1 (multiplies by (1 - decay)); keeps backwards compatibility when False.\nFalse",
    "crumbs": [
      "Exogenous effects",
      "GeometricAdstockEffect"
    ]
  },
  {
    "objectID": "reference/MaximizeKPI.html",
    "href": "reference/MaximizeKPI.html",
    "title": "MaximizeKPI",
    "section": "",
    "text": "MaximizeKPI\nbudget_optimization.objectives.MaximizeKPI(self)\nMaximize the KPI objective function.",
    "crumbs": [
      "Objective Functions",
      "MaximizeKPI"
    ]
  },
  {
    "objectID": "reference/WeibullAdstockEffect.html",
    "href": "reference/WeibullAdstockEffect.html",
    "title": "WeibullAdstockEffect",
    "section": "",
    "text": "effects.WeibullAdstockEffect(\n    self,\n    scale_prior=None,\n    concentration_prior=None,\n    max_lag=None,\n    raise_error_if_fh_changes=False,\n)\nRepresents a Weibull Adstock effect in a time series model.\nThe Weibull adstock applies a convolution of the input with a Weibull probability density function, allowing for more flexible carryover patterns compared to geometric adstock.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nscale_prior\nDistribution\nPrior distribution for the scale parameter of the Weibull distribution. If None, defaults to GammaReparametrized(2, 1).\nNone\n\n\nconcentration_prior\nDistribution\nPrior distribution for the concentration (shape) parameter of the Weibull distribution. If None, defaults to GammaReparametrized(2, 1).\nNone\n\n\nmax_lag\nint\nMaximum lag to consider for the adstock effect. If None, automatically determined based on the Weibull distribution parameters.\nNone\n\n\nraise_error_if_fh_changes\nbool\nWhether to raise an error if the forecasting horizon changes during predict\nFalse"
  },
  {
    "objectID": "reference/WeibullAdstockEffect.html#parameters",
    "href": "reference/WeibullAdstockEffect.html#parameters",
    "title": "WeibullAdstockEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nscale_prior\nDistribution\nPrior distribution for the scale parameter of the Weibull distribution. If None, defaults to GammaReparametrized(2, 1).\nNone\n\n\nconcentration_prior\nDistribution\nPrior distribution for the concentration (shape) parameter of the Weibull distribution. If None, defaults to GammaReparametrized(2, 1).\nNone\n\n\nmax_lag\nint\nMaximum lag to consider for the adstock effect. If None, automatically determined based on the Weibull distribution parameters.\nNone\n\n\nraise_error_if_fh_changes\nbool\nWhether to raise an error if the forecasting horizon changes during predict\nFalse"
  },
  {
    "objectID": "reference/budget_optimization.BudgetOptimizer.html",
    "href": "reference/budget_optimization.BudgetOptimizer.html",
    "title": "budget_optimization.BudgetOptimizer",
    "section": "",
    "text": "budget_optimization.BudgetOptimizer(\n    self,\n    objective,\n    constraints,\n    parametrization_transform=None,\n    method='SLSQP',\n    tol=None,\n    bounds=None,\n    options=None,\n    callback=None,\n)\nBudget optimizer using scipy.optimize.minimize.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobjective\nBaseOptimizationObjective\nObjective function object\nrequired\n\n\nconstraints\nlist of BaseConstraint\nList of constraint objects\nrequired\n\n\ndecision_variable_transform\nBaseDecisionVariableTransform\nDecision variable transform object\nrequired\n\n\nmethod\nstr\nOptimization method to use. Default is “SLSQP”.\n'SLSQP'\n\n\ntol\nfloat\nTolerance for termination. Default is None.\nNone\n\n\nbounds\nUnion[List[tuple], dict[str, tuple]]\nBounds for decision variables. If a list, the value is used directly in scipy.optimize.minimize. If a dict, the keys are the column names and the values are the bounds for each column. Default is (0, np.inf) for each column.\nNone\n\n\noptions\ndict\nOptions for the optimization method. Default is None.\nNone\n\n\ncallback\ncallable\nCallback function to be called after each iteration. Default is None.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nwrap_func_with_inv_transform\nWrap a function with parametrization inverse transform\n\n\n\n\n\nbudget_optimization.BudgetOptimizer.wrap_func_with_inv_transform(\n    fun,\n)\nWrap a function with parametrization inverse transform"
  },
  {
    "objectID": "reference/budget_optimization.BudgetOptimizer.html#parameters",
    "href": "reference/budget_optimization.BudgetOptimizer.html#parameters",
    "title": "budget_optimization.BudgetOptimizer",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nobjective\nBaseOptimizationObjective\nObjective function object\nrequired\n\n\nconstraints\nlist of BaseConstraint\nList of constraint objects\nrequired\n\n\ndecision_variable_transform\nBaseDecisionVariableTransform\nDecision variable transform object\nrequired\n\n\nmethod\nstr\nOptimization method to use. Default is “SLSQP”.\n'SLSQP'\n\n\ntol\nfloat\nTolerance for termination. Default is None.\nNone\n\n\nbounds\nUnion[List[tuple], dict[str, tuple]]\nBounds for decision variables. If a list, the value is used directly in scipy.optimize.minimize. If a dict, the keys are the column names and the values are the bounds for each column. Default is (0, np.inf) for each column.\nNone\n\n\noptions\ndict\nOptions for the optimization method. Default is None.\nNone\n\n\ncallback\ncallable\nCallback function to be called after each iteration. Default is None.\nNone"
  },
  {
    "objectID": "reference/budget_optimization.BudgetOptimizer.html#methods",
    "href": "reference/budget_optimization.BudgetOptimizer.html#methods",
    "title": "budget_optimization.BudgetOptimizer",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nwrap_func_with_inv_transform\nWrap a function with parametrization inverse transform\n\n\n\n\n\nbudget_optimization.BudgetOptimizer.wrap_func_with_inv_transform(\n    fun,\n)\nWrap a function with parametrization inverse transform"
  },
  {
    "objectID": "reference/FlatTrend.html",
    "href": "reference/FlatTrend.html",
    "title": "FlatTrend",
    "section": "",
    "text": "effects.FlatTrend(self, changepoint_prior_scale=0.1)\nFlat trend model.\nThe mean of the target variable is used as the prior location for the trend.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nchangepoint_prior_scale\nfloat\nThe scale of the prior distribution on the trend changepoints. Defaults to 0.1.\n0.1",
    "crumbs": [
      "Trends",
      "FlatTrend"
    ]
  },
  {
    "objectID": "reference/FlatTrend.html#parameters",
    "href": "reference/FlatTrend.html#parameters",
    "title": "FlatTrend",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nchangepoint_prior_scale\nfloat\nThe scale of the prior distribution on the trend changepoints. Defaults to 0.1.\n0.1",
    "crumbs": [
      "Trends",
      "FlatTrend"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html",
    "href": "reference/HierarchicalProphet.html",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "sktime.HierarchicalProphet(\n    self,\n    trend='linear',\n    feature_transformer=None,\n    exogenous_effects=None,\n    default_effect=None,\n    shared_features=None,\n    noise_scale=0.05,\n    correlation_matrix_concentration=1.0,\n    rng_key=None,\n    inference_engine=None,\n    likelihood=None,\n)\nA Bayesian hierarchical time series forecasting model based on Meta’s Prophet.\nThis method forecasts all bottom series in a hierarchy at once, using a MultivariateNormal as the likelihood function and LKJ priors for the correlation matrix.\nThis forecaster is particularly interesting if you want to fit shared coefficients across series. In that case, shared_features parameter should be a list of feature names that should have that behaviour.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[BaseEffect, str]\nTrend component of the model.\n\"linear\"\n\n\nfeature_transformer\nBaseTransformer\nTransformer for features preprocessing.\nNone\n\n\nexogenous_effects\noptional\nEffects to model exogenous variables.\nNone\n\n\ndefault_effect\noptional\nDefault effect specification.\nNone\n\n\nshared_features\noptional\nFeatures shared across time series.\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the noise distribution.\n0.05\n\n\ncorrelation_matrix_concentration\nfloat\nConcentration parameter for the correlation matrix.\n1.0\n\n\nrng_key\noptional\nRandom number generator key.\nNone\n\n\ninference_engine\noptional\nEngine used for inference.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; from sktime.forecasting.naive import NaiveForecaster\n&gt;&gt;&gt; from sktime.transformations.hierarchical.aggregate import Aggregator\n&gt;&gt;&gt; from sktime.utils._testing.hierarchical import _bottom_hier_datagen\n&gt;&gt;&gt; from prophetverse.sktime.multivariate import HierarchicalProphet\n&gt;&gt;&gt; agg = Aggregator()\n&gt;&gt;&gt; y = _bottom_hier_datagen(\n...     no_bottom_nodes=3,\n...     no_levels=1,\n...     random_seed=123,\n...     length=7,\n... )\n&gt;&gt;&gt; y = agg.fit_transform(y)\n&gt;&gt;&gt; forecaster = HierarchicalProphet()\n&gt;&gt;&gt; forecaster = forecaster.fit(y)\n&gt;&gt;&gt; y_pred = forecaster.predict(fh=[1])\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nn_series\nGet the number of series.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_test_params\nParams to be used in sktime unit tests.\n\n\npredict_samples\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nsktime.HierarchicalProphet.get_test_params(parameter_set='default')\nParams to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set to be used (ignored in this implementation)\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing the test parameters.\n\n\n\n\n\n\n\nsktime.HierarchicalProphet.predict_samples(fh, X=None)\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame\nExogenous variables.\nNone\n\n\nfh\nForecastingHorizon\nForecasting horizon.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nPredicted samples.",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html#parameters",
    "href": "reference/HierarchicalProphet.html#parameters",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntrend\nUnion[BaseEffect, str]\nTrend component of the model.\n\"linear\"\n\n\nfeature_transformer\nBaseTransformer\nTransformer for features preprocessing.\nNone\n\n\nexogenous_effects\noptional\nEffects to model exogenous variables.\nNone\n\n\ndefault_effect\noptional\nDefault effect specification.\nNone\n\n\nshared_features\noptional\nFeatures shared across time series.\nNone\n\n\nnoise_scale\nfloat\nScale parameter for the noise distribution.\n0.05\n\n\ncorrelation_matrix_concentration\nfloat\nConcentration parameter for the correlation matrix.\n1.0\n\n\nrng_key\noptional\nRandom number generator key.\nNone\n\n\ninference_engine\noptional\nEngine used for inference.\nNone",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html#examples",
    "href": "reference/HierarchicalProphet.html#examples",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "&gt;&gt;&gt; from sktime.forecasting.naive import NaiveForecaster\n&gt;&gt;&gt; from sktime.transformations.hierarchical.aggregate import Aggregator\n&gt;&gt;&gt; from sktime.utils._testing.hierarchical import _bottom_hier_datagen\n&gt;&gt;&gt; from prophetverse.sktime.multivariate import HierarchicalProphet\n&gt;&gt;&gt; agg = Aggregator()\n&gt;&gt;&gt; y = _bottom_hier_datagen(\n...     no_bottom_nodes=3,\n...     no_levels=1,\n...     random_seed=123,\n...     length=7,\n... )\n&gt;&gt;&gt; y = agg.fit_transform(y)\n&gt;&gt;&gt; forecaster = HierarchicalProphet()\n&gt;&gt;&gt; forecaster = forecaster.fit(y)\n&gt;&gt;&gt; y_pred = forecaster.predict(fh=[1])",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html#attributes",
    "href": "reference/HierarchicalProphet.html#attributes",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nn_series\nGet the number of series.",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/HierarchicalProphet.html#methods",
    "href": "reference/HierarchicalProphet.html#methods",
    "title": "HierarchicalProphet",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_test_params\nParams to be used in sktime unit tests.\n\n\npredict_samples\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nsktime.HierarchicalProphet.get_test_params(parameter_set='default')\nParams to be used in sktime unit tests.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameter_set\nstr\nThe parameter set to be used (ignored in this implementation)\n'default'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[dict[str, int]]\nA list of dictionaries containing the test parameters.\n\n\n\n\n\n\n\nsktime.HierarchicalProphet.predict_samples(fh, X=None)\nGenerate samples for the given exogenous variables and forecasting horizon.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame\nExogenous variables.\nNone\n\n\nfh\nForecastingHorizon\nForecasting horizon.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nPredicted samples.",
    "crumbs": [
      "Sktime",
      "HierarchicalProphet"
    ]
  },
  {
    "objectID": "reference/LogEffect.html",
    "href": "reference/LogEffect.html",
    "title": "LogEffect",
    "section": "",
    "text": "effects.LogEffect(\n    self,\n    effect_mode='multiplicative',\n    scale_prior=None,\n    rate_prior=None,\n)\nRepresents a log effect as effect = scale * log(rate * data + 1).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nscale_prior\nOptional[Distribution]\nThe prior distribution for the scale parameter., by default Gamma\nNone\n\n\nrate_prior\nOptional[Distribution]\nThe prior distribution for the rate parameter., by default Gamma\nNone\n\n\neffect_mode\neffects_application\nEither “additive” or “multiplicative”, by default “multiplicative”\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "LogEffect"
    ]
  },
  {
    "objectID": "reference/LogEffect.html#parameters",
    "href": "reference/LogEffect.html#parameters",
    "title": "LogEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nscale_prior\nOptional[Distribution]\nThe prior distribution for the scale parameter., by default Gamma\nNone\n\n\nrate_prior\nOptional[Distribution]\nThe prior distribution for the rate parameter., by default Gamma\nNone\n\n\neffect_mode\neffects_application\nEither “additive” or “multiplicative”, by default “multiplicative”\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "LogEffect"
    ]
  },
  {
    "objectID": "reference/MinimumTargetResponse.html",
    "href": "reference/MinimumTargetResponse.html",
    "title": "MinimumTargetResponse",
    "section": "",
    "text": "budget_optimization.constraints.MinimumTargetResponse(\n    self,\n    target_response,\n    constraint_type='ineq',\n)\nMinimum target response constraint.\nThis constraint ensures that the target response is greater than or equal to a specified value. This imposes a restriction on the output of the model, instead of the input.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntarget_response\nfloat\nTarget response value. The model output must be greater than or equal to this value.\nrequired",
    "crumbs": [
      "Budget Constraints",
      "MinimumTargetResponse"
    ]
  },
  {
    "objectID": "reference/MinimumTargetResponse.html#parameters",
    "href": "reference/MinimumTargetResponse.html#parameters",
    "title": "MinimumTargetResponse",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntarget_response\nfloat\nTarget response value. The model output must be greater than or equal to this value.\nrequired",
    "crumbs": [
      "Budget Constraints",
      "MinimumTargetResponse"
    ]
  },
  {
    "objectID": "reference/LinearEffect.html",
    "href": "reference/LinearEffect.html",
    "title": "LinearEffect",
    "section": "",
    "text": "effects.LinearEffect(\n    self,\n    effect_mode='multiplicative',\n    prior=None,\n    broadcast=False,\n)\nRepresents a linear effect in a hierarchical prophet model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprior\nDistribution\nA numpyro distribution to use as prior. Defaults to dist.Normal(0, 1)\nNone\n\n\neffect_mode\neffects_application\nEither “multiplicative” or “additive” by default “multiplicative”.\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "LinearEffect"
    ]
  },
  {
    "objectID": "reference/LinearEffect.html#parameters",
    "href": "reference/LinearEffect.html#parameters",
    "title": "LinearEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nprior\nDistribution\nA numpyro distribution to use as prior. Defaults to dist.Normal(0, 1)\nNone\n\n\neffect_mode\neffects_application\nEither “multiplicative” or “additive” by default “multiplicative”.\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "LinearEffect"
    ]
  },
  {
    "objectID": "reference/MultivariateNormal.html",
    "href": "reference/MultivariateNormal.html",
    "title": "MultivariateNormal",
    "section": "",
    "text": "MultivariateNormal\neffects.MultivariateNormal(\n    self,\n    noise_scale=0.05,\n    correlation_matrix_concentration=1,\n)\nBase class for effects.",
    "crumbs": [
      "Target Likelihoods",
      "MultivariateNormal"
    ]
  },
  {
    "objectID": "reference/HillEffect.html",
    "href": "reference/HillEffect.html",
    "title": "HillEffect",
    "section": "",
    "text": "effects.HillEffect(\n    self,\n    effect_mode='multiplicative',\n    half_max_prior=None,\n    slope_prior=None,\n    max_effect_prior=None,\n    offset_slope=0.0,\n    input_scale=1.0,\n    base_effect_name='trend',\n)\nRepresents a Hill effect in a time series model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nhalf_max_prior\nDistribution\nPrior distribution for the half-maximum parameter\nNone\n\n\nslope_prior\nDistribution\nPrior distribution for the slope parameter\nNone\n\n\nmax_effect_prior\nDistribution\nPrior distribution for the maximum effect parameter\nNone\n\n\neffect_mode\neffects_application\nMode of the effect (either “additive” or “multiplicative”)\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "HillEffect"
    ]
  },
  {
    "objectID": "reference/HillEffect.html#parameters",
    "href": "reference/HillEffect.html#parameters",
    "title": "HillEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nhalf_max_prior\nDistribution\nPrior distribution for the half-maximum parameter\nNone\n\n\nslope_prior\nDistribution\nPrior distribution for the slope parameter\nNone\n\n\nmax_effect_prior\nDistribution\nPrior distribution for the maximum effect parameter\nNone\n\n\neffect_mode\neffects_application\nMode of the effect (either “additive” or “multiplicative”)\n'multiplicative'",
    "crumbs": [
      "Exogenous effects",
      "HillEffect"
    ]
  },
  {
    "objectID": "reference/MichaelisMentenEffect.html",
    "href": "reference/MichaelisMentenEffect.html",
    "title": "MichaelisMentenEffect",
    "section": "",
    "text": "effects.MichaelisMentenEffect(\n    self,\n    effect_mode='multiplicative',\n    max_effect_prior=None,\n    half_saturation_prior=None,\n    base_effect_name='trend',\n)\nRepresents a Michaelis-Menten effect in a time series model.\nThe Michaelis-Menten equation is commonly used in biochemistry to describe enzyme kinetics, but it’s also useful for modeling saturation effects in time series analysis. The effect follows the equation:\neffect = (max_effect * data) / (half_saturation + data)\nWhere: - max_effect is the maximum effect value (Vmax in biochemistry) - half_saturation is the value at which effect = max_effect/2 (Km in biochemistry) - data is the input variable (substrate concentration in biochemistry)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmax_effect_prior\nOptional[Distribution]\nPrior distribution for the maximum effect parameter\nNone\n\n\nhalf_saturation_prior\nOptional[Distribution]\nPrior distribution for the half-saturation parameter\nNone\n\n\neffect_mode\neffects_application\nEither “additive” or “multiplicative”, by default “multiplicative”\n'multiplicative'\n\n\nbase_effect_name\nstr\nName of the base effect to multiply with (if multiplicative)\n'trend'"
  },
  {
    "objectID": "reference/MichaelisMentenEffect.html#parameters",
    "href": "reference/MichaelisMentenEffect.html#parameters",
    "title": "MichaelisMentenEffect",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmax_effect_prior\nOptional[Distribution]\nPrior distribution for the maximum effect parameter\nNone\n\n\nhalf_saturation_prior\nOptional[Distribution]\nPrior distribution for the half-saturation parameter\nNone\n\n\neffect_mode\neffects_application\nEither “additive” or “multiplicative”, by default “multiplicative”\n'multiplicative'\n\n\nbase_effect_name\nstr\nName of the base effect to multiply with (if multiplicative)\n'trend'"
  },
  {
    "objectID": "reference/NegativeBinomialTargetLikelihood.html",
    "href": "reference/NegativeBinomialTargetLikelihood.html",
    "title": "NegativeBinomialTargetLikelihood",
    "section": "",
    "text": "NegativeBinomialTargetLikelihood\neffects.NegativeBinomialTargetLikelihood(self, noise_scale=0.05, epsilon=1e-05)",
    "crumbs": [
      "Target Likelihoods",
      "NegativeBinomialTargetLikelihood"
    ]
  },
  {
    "objectID": "reference/LinearFourierSeasonality.html",
    "href": "reference/LinearFourierSeasonality.html",
    "title": "LinearFourierSeasonality",
    "section": "",
    "text": "effects.LinearFourierSeasonality(\n    self,\n    sp_list,\n    fourier_terms_list,\n    freq,\n    prior_scale=1.0,\n    effect_mode='additive',\n    linear_effect=None,\n)\nLinear Fourier Seasonality effect.\nCompute the linear seasonality using Fourier features.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsp_list\nList[float]\nList of seasonal periods.\nrequired\n\n\nfourier_terms_list\nList[int]\nList of number of Fourier terms to use for each seasonal period.\nrequired\n\n\nfreq\nstr\nFrequency of the time series. Example: “D” for daily, “W” for weekly, etc.\nrequired\n\n\nprior_scale\nfloat\nScale of the prior distribution for the effect, by default 1.0.\n1.0\n\n\neffect_mode\nstr\nEither “multiplicative” or “additive” by default “additive”.\n'additive'",
    "crumbs": [
      "Exogenous effects",
      "LinearFourierSeasonality"
    ]
  },
  {
    "objectID": "reference/LinearFourierSeasonality.html#parameters",
    "href": "reference/LinearFourierSeasonality.html#parameters",
    "title": "LinearFourierSeasonality",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsp_list\nList[float]\nList of seasonal periods.\nrequired\n\n\nfourier_terms_list\nList[int]\nList of number of Fourier terms to use for each seasonal period.\nrequired\n\n\nfreq\nstr\nFrequency of the time series. Example: “D” for daily, “W” for weekly, etc.\nrequired\n\n\nprior_scale\nfloat\nScale of the prior distribution for the effect, by default 1.0.\n1.0\n\n\neffect_mode\nstr\nEither “multiplicative” or “additive” by default “additive”.\n'additive'",
    "crumbs": [
      "Exogenous effects",
      "LinearFourierSeasonality"
    ]
  },
  {
    "objectID": "reference/GammaTargetLikelihood.html",
    "href": "reference/GammaTargetLikelihood.html",
    "title": "GammaTargetLikelihood",
    "section": "",
    "text": "GammaTargetLikelihood\neffects.GammaTargetLikelihood(self, noise_scale=0.05, epsilon=1e-05)",
    "crumbs": [
      "Target Likelihoods",
      "GammaTargetLikelihood"
    ]
  },
  {
    "objectID": "howto/beta_percentage_forecast.html",
    "href": "howto/beta_percentage_forecast.html",
    "title": "Forecasting Percentages",
    "section": "",
    "text": "This how‑to shows how to use the Beta likelihood for a proportion / percentage target using the same sktime-style API demonstrated in other tutorials.\nWhy Beta? Because the target is naturally bounded in (0,1), and Beta likelihood can provide probabilistic intervals in (0,1). The likelihood=\"beta\" option internally applies a link that guarantees valid predictions.",
    "crumbs": [
      "Forecasting percentages"
    ]
  },
  {
    "objectID": "howto/beta_percentage_forecast.html#load-data-and-build-a-01-target",
    "href": "howto/beta_percentage_forecast.html#load-data-and-build-a-01-target",
    "title": "Forecasting Percentages",
    "section": "1. Load data and build a (0,1) target",
    "text": "1. Load data and build a (0,1) target\n\nimport numpy as np\nimport pandas as pd\nfrom sktime.split import temporal_train_test_split\n\nnum_obs = 1000\ny = pd.DataFrame(\n    data={\"value\" : np.sin(np.arange(num_obs)*2*np.pi/30)*0.45 + 0.5},\n    index=pd.period_range(\n        \"2025-01-01\",\n        periods=num_obs,\n        freq=\"D\",\n    )\n) \ny += np.random.normal(0, 0.1, size=y.shape)\ny = y.clip(1e-6, 1 - 1e-6)\n\ny_train, y_test = temporal_train_test_split(y, test_size=0.4)\n\ny_train.plot.line()",
    "crumbs": [
      "Forecasting percentages"
    ]
  },
  {
    "objectID": "howto/beta_percentage_forecast.html#specify-model-components",
    "href": "howto/beta_percentage_forecast.html#specify-model-components",
    "title": "Forecasting Percentages",
    "section": "2. Specify model components",
    "text": "2. Specify model components\nWe use a piecewise linear trend plus weekly and yearly seasonality. The API mirrors the univariate tutorial: pass trend=..., supply exogenous_effects as a list of tuples, and choose likelihood=\"beta\".\n\nimport numpyro\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.effects.target.univariate import BetaTargetLikelihood\nfrom prophetverse.utils import no_input_columns\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.sktime import Prophetverse\n\n\nnumpyro.enable_x64()\n\nseasonality = (\n    \"seasonality\",\n    LinearFourierSeasonality(\n        freq=\"D\",\n        sp_list=[30],  # weekly + yearly\n        fourier_terms_list=[30],\n        prior_scale=1,\n        effect_mode=\"additive\",\n    ),\n    no_input_columns,\n)\n\nmodel = Prophetverse(\n    trend=\"flat\",\n    exogenous_effects=[seasonality],\n    likelihood=BetaTargetLikelihood(noise_scale=0.2),  # &lt;— key change\n    inference_engine=MAPInferenceEngine(),\n    scale=1,\n)\nmodel.fit(y=y_train)\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(fourier_terms_list=[30],\n                                                          freq='D',\n                                                          prior_scale=1,\n                                                          sp_list=[30]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             likelihood=BetaTargetLikelihood(noise_scale=0.2), scale=1,\n             trend='flat')Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(fourier_terms_list=[30],\n                                                          freq='D',\n                                                          prior_scale=1,\n                                                          sp_list=[30]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             likelihood=BetaTargetLikelihood(noise_scale=0.2), scale=1,\n             trend='flat')effectsFlatTrendFlatTrend()LinearFourierSeasonalityLinearFourierSeasonality(fourier_terms_list=[30], freq='D', prior_scale=1,\n                         sp_list=[30])inference_engineMAPInferenceEngineMAPInferenceEngine()",
    "crumbs": [
      "Forecasting percentages"
    ]
  },
  {
    "objectID": "howto/beta_percentage_forecast.html#forecast-the-next-180-days",
    "href": "howto/beta_percentage_forecast.html#forecast-the-next-180-days",
    "title": "Forecasting Percentages",
    "section": "3. Forecast the next 180 days",
    "text": "3. Forecast the next 180 days\n\nimport pandas as pd\n\nfh = y_test.index\npred_share = model.predict(fh=fh)\npred_share.head()\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\n2026-08-24\n0.511203\n\n\n2026-08-25\n0.595006\n\n\n2026-08-26\n0.679610\n\n\n2026-08-27\n0.759363\n\n\n2026-08-28\n0.796536",
    "crumbs": [
      "Forecasting percentages"
    ]
  },
  {
    "objectID": "howto/beta_percentage_forecast.html#plot",
    "href": "howto/beta_percentage_forecast.html#plot",
    "title": "Forecasting Percentages",
    "section": "4. Plot",
    "text": "4. Plot\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(9, 4))\nax.plot(\n    y.index.to_timestamp(), y, label=\"observed daily share\", lw=1, alpha=0.6\n)\nax.plot(pred_share.index.to_timestamp(), pred_share, label=\"forecast\", color=\"C1\")\nax.set_ylabel(\"Daily share of monthly total\")\nax.legend()\nplt.show()",
    "crumbs": [
      "Forecasting percentages"
    ]
  },
  {
    "objectID": "howto/beta_percentage_forecast.html#probabilistic-forecast-quantiles",
    "href": "howto/beta_percentage_forecast.html#probabilistic-forecast-quantiles",
    "title": "Forecasting Percentages",
    "section": "5. Probabilistic forecast (quantiles)",
    "text": "5. Probabilistic forecast (quantiles)\n\nq = model.predict_quantiles(fh=fh, alpha=[0.1, 0.9])\nfig, ax = plt.subplots(figsize=(9, 4))\nax.fill_between(\n    q.index.to_timestamp(),\n    q.iloc[:, 0],\n    q.iloc[:, -1],\n    color=\"C1\",\n    alpha=0.3,\n    label=\"80% PI\",\n)\nax.plot(y.iloc[-(len(fh) + 100):].index.to_timestamp(), y.iloc[-(len(fh) + 100):], lw=1, alpha=0.6, color=\"k\")\nax.set_ylabel(\"Daily share\")\nax.legend()\nplt.show()",
    "crumbs": [
      "Forecasting percentages"
    ]
  },
  {
    "objectID": "howto/beta_percentage_forecast.html#notes-tips",
    "href": "howto/beta_percentage_forecast.html#notes-tips",
    "title": "Forecasting Percentages",
    "section": "Notes & Tips",
    "text": "Notes & Tips\n\nEnsure the target is strictly inside (0,1), excluding the boundaries.\nnoise_scale controls dispersion of the Beta (smaller =&gt; tighter intervals).\nThe same pattern works for conversion rates, CTR, retention proportions, etc.\nSwitch to full Bayesian inference by setting inference_engine=MCMCInferenceEngine(...) if you need richer uncertainty.",
    "crumbs": [
      "Forecasting percentages"
    ]
  },
  {
    "objectID": "howto/custom_effects.html",
    "href": "howto/custom_effects.html",
    "title": "Customizing Exogenous Effects in Prophetverse",
    "section": "",
    "text": "The exogenous effect API allows you to create custom exogenous components for the Prophetverse model. This is useful when we want to model specific patterns or relationships between the exogenous variables and the target variable. For example, enforcing a positive effect of a variable on the mean, or modeling a non-linear relationship.\nIf you have read the theory section, by effect we mean each function \\(f_i\\). You can implement those custom functions by subclassing the BaseEffect class, and then use them in the Prophetverse model. Some effects are already implemented in the library, and you can find them in the prophetverse.effects module.\nWhen creating a model instance, effects can be specified through exogenous_effects parameter of the Prophetverse model. This parameter is a list of tuples of three values: the name, the effect object, and a regex to filter columns related to that effect. The regex is what defines \\(x_i\\) in the previous section. The prophetverse.utils.regex module provides some useful functions to create regex patterns for common use cases, include starts_with, ends_with, contains, and no_input_columns.\nConsider the example below, where we create a model with a linear seasonality effect and a custom effect that uses the feature channel1_investment as input and transforms it with a hill curve, which is a common curve for capturing diminishing returns.\n\nfrom prophetverse.effects import HillEffect, LinearFourierSeasonality\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import exact, no_input_columns, starts_with\n\nexogenous_effects = [\n    (\n        \"seasonality\",\n        LinearFourierSeasonality(\n            freq=\"D\",\n            sp_list=[7, 365.25],\n            fourier_terms_list=[3, 10],\n            prior_scale=0.1,\n            effect_mode=\"multiplicative\",\n        ),\n        no_input_columns,\n    ),\n    (\n        \"channel1_investment_incremental\",\n        HillEffect(effect_mode=\"additive\"),\n        exact(\"channel1_investment\"),\n    ),\n]\n\nmodel = Prophetverse(exogenous_effects=exogenous_effects)\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nCreating such models in Prophetverse is like creating buildings from lego blocks. You define how you model should work, and then you can leverage all the interface to carry out the forecasting and inference tasks.",
    "crumbs": [
      "Custom exogenous effect"
    ]
  },
  {
    "objectID": "howto/custom_effects.html#the-effects-api",
    "href": "howto/custom_effects.html#the-effects-api",
    "title": "Customizing Exogenous Effects in Prophetverse",
    "section": "",
    "text": "The exogenous effect API allows you to create custom exogenous components for the Prophetverse model. This is useful when we want to model specific patterns or relationships between the exogenous variables and the target variable. For example, enforcing a positive effect of a variable on the mean, or modeling a non-linear relationship.\nIf you have read the theory section, by effect we mean each function \\(f_i\\). You can implement those custom functions by subclassing the BaseEffect class, and then use them in the Prophetverse model. Some effects are already implemented in the library, and you can find them in the prophetverse.effects module.\nWhen creating a model instance, effects can be specified through exogenous_effects parameter of the Prophetverse model. This parameter is a list of tuples of three values: the name, the effect object, and a regex to filter columns related to that effect. The regex is what defines \\(x_i\\) in the previous section. The prophetverse.utils.regex module provides some useful functions to create regex patterns for common use cases, include starts_with, ends_with, contains, and no_input_columns.\nConsider the example below, where we create a model with a linear seasonality effect and a custom effect that uses the feature channel1_investment as input and transforms it with a hill curve, which is a common curve for capturing diminishing returns.\n\nfrom prophetverse.effects import HillEffect, LinearFourierSeasonality\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import exact, no_input_columns, starts_with\n\nexogenous_effects = [\n    (\n        \"seasonality\",\n        LinearFourierSeasonality(\n            freq=\"D\",\n            sp_list=[7, 365.25],\n            fourier_terms_list=[3, 10],\n            prior_scale=0.1,\n            effect_mode=\"multiplicative\",\n        ),\n        no_input_columns,\n    ),\n    (\n        \"channel1_investment_incremental\",\n        HillEffect(effect_mode=\"additive\"),\n        exact(\"channel1_investment\"),\n    ),\n]\n\nmodel = Prophetverse(exogenous_effects=exogenous_effects)\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nCreating such models in Prophetverse is like creating buildings from lego blocks. You define how you model should work, and then you can leverage all the interface to carry out the forecasting and inference tasks.",
    "crumbs": [
      "Custom exogenous effect"
    ]
  },
  {
    "objectID": "howto/custom_effects.html#creating-a-custom-effect",
    "href": "howto/custom_effects.html#creating-a-custom-effect",
    "title": "Customizing Exogenous Effects in Prophetverse",
    "section": "Creating a Custom Effect",
    "text": "Creating a Custom Effect\nThe effects can be any object that implements the BaseEffect interface, and you can create your own effects by subclassing BaseEffect and implementing _fit, _transform and _predict methods.\n\n_fit (optional): This method is called during fit() of the forecasting and should be used to initialize any necessary parameters or data structures. It receives the exogenous variables dataframe X, the series y, and the scale factor scale that was used to scale the timeseries.\n_transform (optional): This method receives the exogenous variables dataframe, and should return an object containing the data needed for the effect. This object will be passed to the predict method as data. By default the columns of the dataframe that match the regex pattern are selected, and the result is converted to a jnp.ndarray.\n_predict (mandatory): This method receives the output of _transform and all previously computed effects. It should return the effect values as a jnp.ndarray\n\nIn many cases, the _fit and _transform steps are not needed to be implemented, since the default behaviour may be the desired one. In the example below, we implement a really simple SquaredEffect class, which leverages the default behaviour of the BaseEffect class.\n\nSquared Effect Class\nThe SquaredEffect class receives two hyperparameters: the prior distribution for the scale parameter, and the prior distribution for the offset parameter. If no prior is provided, it uses a Gamma(1, 1) for the scale and a Normal(0, 1) for the offset. Note that here we already see an interesting feature of Prophetverse: by adopting a Gamma Prior, we force the effect to be positive. Any other prior with positive support would work as well. If no such constraint is needed, we can use a Normal(0, 1) prior or any other distribution with support in the real line.\n\nfrom typing import Dict, Optional\n\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro.distributions import Distribution\n\nfrom prophetverse.effects.base import BaseEffect\n\n\nclass SquaredEffect(BaseEffect):\n    \"\"\"Represents a squared effect as effect = scale * (data - offset)^2.\"\"\"\n\n    def __init__(\n        self,\n        scale_prior: Optional[Distribution] = None,\n        offset_prior: Optional[Distribution] = None,\n    ):\n        self.scale_prior = scale_prior or dist.Gamma(1, 1)\n        self.offset_prior = offset_prior or dist.Normal(0, 1)\n        super().__init__()\n\n    def _predict(\n        self,\n        data: jnp.ndarray,\n        predicted_effects: Optional[Dict[str, jnp.ndarray]],\n        params: Dict,\n    ) -&gt; jnp.ndarray:\n        scale = numpyro.sample(\"log_scale\", self.scale_prior)\n        offset = numpyro.sample(\"offset\", self.offset_prior)\n        effect = scale * (data - offset) ** 2\n        return effect\n\nThe _fit and _transform methods are not implemented, and the default behaviour is preserved (the columns of the dataframe that match the regex pattern are selected, and the result is converted to a jnp.ndarray with key “data”).",
    "crumbs": [
      "Custom exogenous effect"
    ]
  },
  {
    "objectID": "howto/custom_effects.html#practical-example",
    "href": "howto/custom_effects.html#practical-example",
    "title": "Customizing Exogenous Effects in Prophetverse",
    "section": "Practical Example",
    "text": "Practical Example\nThe example below is, of course, a toy example, but I hope it illustrates the process of creating a custom effect. We load a synthetic dataset with a squared relationship between the exogenous variable and the target variable, and then we fit a model with the SquaredEffect. The true relationship is 2 * (x - 5) ** 2, and we will see if the model is able to recover it.\n\nLoading the Series\n\nimport matplotlib.pyplot as plt\nfrom sktime.split import temporal_train_test_split\nfrom sktime.utils.plotting import plot_series\n\nfrom prophetverse.datasets import load_synthetic_squared_exogenous\n\ny, X = load_synthetic_squared_exogenous()\ny_train, y_test, X_train, X_test = temporal_train_test_split(\n    y,\n    X,\n    test_size=0.2,\n)\n\ndisplay(y.head())\ndisplay(X.head())\n\nfig, ax = plot_series(\n    y_train, y_test, labels=[\"Train\", \"Test\"], title=\"Target variable\"\n)\nfig.show()\nfig, ax = plot_series(\n    X_train, X_test, labels=[\"Train\", \"Test\"], title=\"Exogenous variable\"\n)\nfig.show()\n\n\n\n\n\n\n\n\ntarget\n\n\ntime\n\n\n\n\n\n2010-01-01\n14.956419\n\n\n2010-01-02\n1.694310\n\n\n2010-01-03\n28.520329\n\n\n2010-01-04\n15.180486\n\n\n2010-01-05\n20.784949\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexogenous\n\n\ntime\n\n\n\n\n\n2010-01-01\n7.965604\n\n\n2010-01-02\n4.949906\n\n\n2010-01-03\n8.727381\n\n\n2010-01-04\n7.276312\n\n\n2010-01-05\n1.847596\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating the Model\n\nfrom prophetverse.effects.trend import PiecewiseLinearTrend\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import exact\n\nmodel = (\n    Prophetverse()\n    &gt;&gt; PiecewiseLinearTrend(\n        changepoint_interval=100,\n        changepoint_prior_scale=0.1,\n        changepoint_range=-100,\n    )\n    &gt;&gt; MAPInferenceEngine()\n) &gt;&gt; (\n    \"exog_effect\",\n    SquaredEffect(\n        scale_prior=dist.Normal(0, 10),\n        offset_prior=dist.Normal(0, 10),\n    ),\n    exact(\"exogenous\"),\n)\nmodel\n\nProphetverse(exogenous_effects=[('exog_effect',\n                                 SquaredEffect(offset_prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f13c0305ed0 with batch shape () and event shape ()&gt;,\n                                               scale_prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f13fc194a50 with batch shape () and event shape ()&gt;),\n                                 '^exogenous$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=100,\n                                        changepoint_prior_scale=0.1,\n                                        changepoint_range=-100))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('exog_effect',\n                                 SquaredEffect(offset_prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f13c0305ed0 with batch shape () and event shape ()&gt;,\n                                               scale_prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f13fc194a50 with batch shape () and event shape ()&gt;),\n                                 '^exogenous$')],\n             inference_engine=MAPInferenceEngine(),\n             trend=PiecewiseLinearTrend(changepoint_interval=100,\n                                        changepoint_prior_scale=0.1,\n                                        changepoint_range=-100))effectsPiecewiseLinearTrendPiecewiseLinearTrend(changepoint_interval=100, changepoint_prior_scale=0.1,\n                     changepoint_range=-100)SquaredEffectSquaredEffect(offset_prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f13c0305ed0 with batch shape () and event shape ()&gt;,\n              scale_prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f13fc194a50 with batch shape () and event shape ()&gt;)inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\nTo fit and plot, we use always the same interface, from sktime library.\n\nmodel.fit(y=y_train, X=X_train)\ny_pred = model.predict(fh=y_test.index, X=X)\ny_pred.head()\n\n\n\n\n\n\n\n\ntarget\n\n\n\n\n2011-07-15\n31.400940\n\n\n2011-07-16\n16.796562\n\n\n2011-07-17\n36.026093\n\n\n2011-07-18\n16.220928\n\n\n2011-07-19\n16.969931\n\n\n\n\n\n\n\n\nplot_series(y, y_pred, labels=[\"True\", \"Predicted\"], title=\"True vs Predicted\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRecovering the Predicted Effect and Components\nThis library adds extra methods to the sktime interface, such as predict_components, which behaves similarly to predict, but returns the components of the forecast as components of the output.\nThe name of the effect in the output dataframe is equal to the one we have passed as first item in the tuple when creating the model. In this case, the name is “exog_effect”.\n\ncomponents = model.predict_components(fh=y.index, X=X)\ncomponents.head()\n\n\n\n\n\n\n\n\nexog_effect\nmean\nobs\ntrend\n\n\n\n\n2010-01-01\n17.593199\n17.405504\n17.538826\n-0.187677\n\n\n2010-01-02\n0.006438\n-0.164963\n-0.247944\n-0.171401\n\n\n2010-01-03\n27.817400\n27.662275\n27.588865\n-0.155124\n\n\n2010-01-04\n10.351486\n10.212640\n10.137514\n-0.138848\n\n\n2010-01-05\n20.049498\n19.926924\n19.955688\n-0.122571\n\n\n\n\n\n\n\nNow, let’s compare it with the true effect. We will plot the true effect and the predicted effect in the same plot.\n\nfig, ax = plt.subplots()\nax.scatter(\n    X[\"exogenous\"], 2 * (X[\"exogenous\"] - 5) ** 2, color=\"black\", label=\"True effect\"\n)\nax.scatter(\n    X[\"exogenous\"],\n    components[\"exog_effect\"],\n    marker=\"x\",\n    color=\"red\",\n    s=10,\n    label=\"Predicted effect\",\n)\nax.set(\n    xlabel=\"Exogenous variable\",\n    ylabel=\"Effect\",\n    title=\"True effect vs Predicted effect\",\n)\nax.legend()\nfig.show()",
    "crumbs": [
      "Custom exogenous effect"
    ]
  },
  {
    "objectID": "howto/effect_api_intro.html",
    "href": "howto/effect_api_intro.html",
    "title": "Customizing exogenous effects",
    "section": "",
    "text": "NoteCustom effect\n\n\n\n\n\nCheckout how to create a non-linear effect using the exogenous effect API, and expand the model with custom effects.\n\nClick here!\n\n\n\n\n\n\n\n\n\n\n\nTipCustom trend\n\n\n\n\n\nYou may also wish to customize the trend component of the model. We have an interesting applied example!\n\nSee here!\n\n\n\n\n\n\n\n\n\n\n\nTipComposition of effects\n\n\n\n\n\nYou can create effect that capture a custom interaction between different exogenous variables.\n\nSee here!\nThe exogenous effect API allows you to create custom exogenous components for the Prophetverse model. This is useful when we want to model specific patterns or relationships between the exogenous variables and the target variable. For example, enforcing a positive effect of a variable on the mean, or modeling a non-linear relationship.\nIf you have read the theory section, by effect we mean each function \\(f_i\\). You can implement those custom functions by subclassing the BaseEffect class, and then use them in the Prophetverse model. Some effects are already implemented in the library, and you can find them in the prophetverse.effects module.\nWhen creating a model instance, effects can be specified through exogenous_effects parameter of the Prophetverse model. This parameter is a list of tuples of three values: the name, the effect object, and a regex to filter columns related to that effect. The regex is what defines \\(x_i\\) in the previous section. The prophetverse.utils.regex module provides some useful functions to create regex patterns for common use cases, include starts_with, ends_with, contains, and no_input_columns.\nFor example:\nThe effects can be any object that implements the BaseEffect interface, and you can create your own effects by subclassing BaseEffect and implementing _fit, _transform and _predict methods.",
    "crumbs": [
      "Introduction to Effects"
    ]
  },
  {
    "objectID": "howto/effect_api_intro.html#example",
    "href": "howto/effect_api_intro.html#example",
    "title": "Customizing exogenous effects",
    "section": "Example",
    "text": "Example\n\nLog Effect\nThe BaseAdditiveOrMultiplicativeEffect provides an init argument effect_mode that allows you to specify if the effect is additive or multiplicative. Let’s take as an example the LogEffect:\n#prophetverse/effects/log.py\n\nfrom typing import Dict, Optional\n\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro.distributions import Distribution\n\nfrom prophetverse.effects.base import (\n    EFFECT_APPLICATION_TYPE,\n    BaseAdditiveOrMultiplicativeEffect,\n)\n\n__all__ = [\"LogEffect\"]\n\n\nclass LogEffect(BaseAdditiveOrMultiplicativeEffect):\n    \"\"\"Represents a log effect as effect = scale * log(rate * data + 1).\n\n    Parameters\n    ----------\n    scale_prior : Optional[Distribution], optional\n        The prior distribution for the scale parameter., by default Gamma\n    rate_prior : Optional[Distribution], optional\n        The prior distribution for the rate parameter., by default Gamma\n    effect_mode : effects_application, optional\n        Either \"additive\" or \"multiplicative\", by default \"multiplicative\"\n    \"\"\"\n\n    def __init__(\n        self,\n        effect_mode: EFFECT_APPLICATION_TYPE = \"multiplicative\",\n        scale_prior: Optional[Distribution] = None,\n        rate_prior: Optional[Distribution] = None,\n    ):\n        self.scale_prior = scale_prior or dist.Gamma(1, 1)\n        self.rate_prior = rate_prior or dist.Gamma(1, 1)\n        super().__init__(effect_mode=effect_mode)\n\n    def _predict(  # type: ignore[override]\n        self,\n        data: jnp.ndarray,\n        predicted_effects: Optional[Dict[str, jnp.ndarray]] = None,\n    ) -&gt; jnp.ndarray:\n        \"\"\"Apply and return the effect values.\n\n        Parameters\n        ----------\n        data : Any\n            Data obtained from the transformed method.\n\n        predicted_effects : Dict[str, jnp.ndarray], optional\n            A dictionary containing the predicted effects, by default None.\n\n        Returns\n        -------\n        jnp.ndarray\n            An array with shape (T,1) for univariate timeseries, or (N, T, 1) for\n            multivariate timeseries, where T is the number of timepoints and N is the\n            number of series.\n        \"\"\"\n        scale = numpyro.sample(\"log_scale\", self.scale_prior)\n        rate = numpyro.sample(\"log_rate\", self.rate_prior)\n        effect = scale * jnp.log(jnp.clip(rate * data + 1, 1e-8, None))\n\n        return effect\nThe _fit and _transform methods are not implemented, and the default behaviour is preserved (the columns of the dataframe that match the regex pattern are selected, and the result is converted to a jnp.ndarray with key “data”).\n\n\nComposition of effects\nWe can go further and create a custom effect that adds a likelihood term to the model. The LiftExperimentLikelihood tackles the use case of having a lift experiment, and wanting to incorporate it to guide the exogenous effect. The likelihood term is added in the _predict method, and the observed lift preprocessed in _transform method.\n\n\n\n\n\n\nTip\n\n\n\nTo see more, check the custom effect how-to.",
    "crumbs": [
      "Introduction to Effects"
    ]
  },
  {
    "objectID": "prompts/custom_effects.html",
    "href": "prompts/custom_effects.html",
    "title": "Prompt for Custom Effects",
    "section": "",
    "text": "Paste the prompt below just before your request to the LLM. This will help the model understand what you want to achieve.\n\"\"\"\nExtension template for creating custom effects in Prophetverse.\n\nThis template provides comprehensive examples and guidance for creating your own effects.\nEffects are the building blocks of Prophetverse models, allowing you to incorporate\nvarious components like trend, seasonality, and exogenous regressors.\n\nKey Methods to Understand:\n--------------------------\n1. `_fit(self, y, X, scale)`: (Optional) Called once during the forecaster's `fit`.\n   Use this to perform pre-computations or fit objects needed later.\n\n2. `_transform(self, X, fh)`: (Optional) Called during both `fit` and `predict`.\n   Converts pandas DataFrame to a format suitable for `_predict` (typically JAX arrays).\n   The default implementation converts selected columns to JAX arrays. The\n   shape for array data is (n_series, n_timepoints, n_features).\n\n3. `_predict(self, data, predicted_effects, **kwargs)`: (Mandatory) Core effect logic.\n   Receives data from `_transform` and returns the computed effect as a JAX array.\n   Use `numpyro.sample` here to define prior distributions for parameters. The\n   shape for array data is (n_series, n_timepoints, n_features) if capability\n   if false for both `capability:panel` and `capability:multivariate_input`.,\n   (n_timepoints, n_features) if  'capability:multivariate_input' is true, and\n   (n_series, n_timepoints, 1) if `capability:panel` is true. The output should\n   be a jax array of shapre (n_series, n_timepoints, 1) if `capability:panel` is true,\n   and (n_timepoints, 1) otherwise.\n\n\nEffect Tags (Control Behavior):\n-------------------------------\n- `capability:panel`: Can handle multiple time series at once.\n- `capability:multivariate_input`: Can process multiple columns simultaneously.\n- `requires_X`: Skip if no matching columns found in X.\n- `applies_to`: Whether effect uses 'X' (exogenous) or 'y' (target) data.\n- `filter_indexes_with_forecating_horizon_at_transform`: Pre-filter data to forecast horizon.\n- `requires_fit_before_transform`: Require fit() before transform().\n- `feature:panel_hyperpriors`: Uses hyperpriors for hierarchical modeling.\n\nHow Tags Modify Behavior:\n-------------------------\nTags are metadata that control how Prophetverse handles your effect. Here's a\ndeeper dive into how they influence the `_fit`, `_transform`, and `_predict` methods:\n\n- `capability:panel` (bool):\n  - If `True`, your effect is expected to handle panel data (multiple time series) directly. `_fit` and `_transform` receive the full DataFrame with a MultiIndex.\n  - If `False` (default), and panel data is provided, Prophetverse automatically broadcasts the effect, applying it to each time series individually.\n\n- `capability:multivariate_input` (bool):\n  - If `True`, your effect is expected to handle a DataFrame with multiple columns as input. `_fit` and `_transform` receive all matching columns at once.\n  - If `False` (default), and a multi-column DataFrame is provided, Prophetverse broadcasts the effect, applying it to each column individually.\n\n- `requires_X` (bool):\n  - If `True` (default), the effect depends on exogenous variables (`X`).\n  - If no columns in `X` match the effect's `regex`, the entire effect is\n    skipped (`_transform` and `_predict` are not called).\n  - If `False`, the effect runs even if `X` is empty or `None`.\n\n- `applies_to` (str: 'X' or 'y'):\n  - Determines the input data for your effect.\n  - If `'X'` (default), the `data` passed to `_fit` and `_transform` is the\n    exogenous variables DataFrame.\n  - If `'y'`, the `data` is the target variable DataFrame.\n\n- `filter_indexes_with_forecating_horizon_at_transform` (bool):\n  - If `True` (default), during `predict`, the DataFrame passed to `_transform`\n    is automatically filtered to only contain dates in the forecasting horizon (`fh`).\n  - This is a convenience to avoid manual filtering inside `_transform`.\n  - Set to `False` if your effect needs to see data outside the forecast horizon\n    during prediction (e.g., for calculating lags).\n\n- `requires_fit_before_transform` (bool):\n  - If `True`, Prophetverse ensures that `_fit` has been called before `_transform`.\n  - This is crucial if `_transform` relies on state computed in `_fit` (e.g.,\n    means, scaling factors).\n  - If `False` (default), `_transform` can be called without `_fit`, which is\n    typical for stateless effects.\n\n- `feature:panel_hyperpriors` (bool):\n  - If `True`, signals that your effect defines hyperpriors for hierarchical\n    models, allowing parameters to be shared and learned across different\n    time series in a panel dataset. This is an advanced feature for\n    implementing hierarchical Bayesian models.\n\"\"\"\n\nfrom typing import Any, Dict, Optional\n\nimport pandas as pd\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\n\nfrom prophetverse.effects.base import BaseEffect\nfrom prophetverse.utils.frame_to_array import series_to_tensor_or_array\n\n\nclass MySimpleEffect(BaseEffect):\n    \"\"\"\n    A simple custom effect example that only overrides `_predict`.\n\n    This template is suitable when no fitting or parameter sampling is required,\n    and the effect is a direct transformation of the input data.\n\n    Parameters\n    ----------\n    scale_factor : float\n        A scaling factor applied to the input data.\n    bias : float\n        A constant bias added to the scaled input.\n    \"\"\"\n\n    _tags = {\n        \"capability:panel\": False,\n        \"capability:multivariate_input\": False,\n        \"requires_X\": True,\n        \"applies_to\": \"X\",\n        \"filter_indexes_with_forecating_horizon_at_transform\": True,\n        \"requires_fit_before_transform\": False,\n    }\n\n    def __init__(self, scale_factor: float = 1.0, bias: float = 0.0):\n        # Init hyperparameters before BaseEffect init\n        # Do not change them!\n        self.scale_factor = scale_factor\n        self.bias = bias\n        super().__init__()\n\n        # Now, do parameter handling\n\n    def _predict(\n        self,\n        data: jnp.ndarray,\n        predicted_effects: Dict[str, jnp.ndarray],\n        *args,\n        **kwargs,\n    ) -&gt; jnp.ndarray:\n        \"\"\"\n        Compute the custom effect by scaling `data` and adding a bias.\n\n        Parameters\n        ----------\n        data : jnp.ndarray\n            Transformed exogenous data from the base `_transform` method.\n        predicted_effects : dict\n            A dictionary of already computed effects in the model (unused here).\n\n        Returns\n        -------\n        jnp.ndarray\n            The computed effect, a JAX array.\n        \"\"\"\n        variable = numpyro.sample(\"variable\", dist.Normal(0.0, 1.0))\n        return data * self.scale_factor + self.bias + variable\n\n\nclass MyCustomEffect(BaseEffect):\n    \"\"\"\n    A full-featured custom effect example.\n\n    This demonstrates how to implement `_fit`, `_transform`, and `_predict`,\n    and how to use tags to control the effect's behavior.\n\n    Steps to implement a new effect:\n      1. Override `_fit` (optional) to compute static quantities from `y` and `X`.\n      2. Override `_transform` (optional) to prepare `X` as JAX arrays.\n      3. Within `_predict`, sample any parameters via `numpyro.sample`.\n      4. Implement `_predict` (required) using `data`, `predicted_effects`, and samples.\n\n    Parameters\n    ----------\n    multiplier : float\n        A multiplier applied in `_predict`.\n    prior_scale : float\n        Scale of the Normal prior for sampling a coefficient.\n    \"\"\"\n\n    _tags = {\n        \"capability:panel\": False,\n        \"capability:multivariate_input\": False,\n        \"requires_X\": True,\n        \"applies_to\": \"X\",\n        \"filter_indexes_with_forecating_horizon_at_transform\": True,\n        \"requires_fit_before_transform\": True,  # We need fit to learn the mean\n    }\n\n    def __init__(self, multiplier: float = 1.0, prior=None):\n        # Init hyperparameters before BaseEffect init\n        # Do not change them!\n        self.multiplier = multiplier\n        self.prior = prior\n        super().__init__()\n        # It's good practice to define priors in __init__\n        self._prior = prior if prior is not None else dist.Normal(0.0, 1.0)\n\n    def _fit(self, y: pd.DataFrame, X: Optional[pd.DataFrame], scale: float = 1.0):\n        \"\"\"\n        (Optional) Fit phase: called once during forecaster.fit().\n\n        This example learns the column means from the training data `X` for centering.\n        \"\"\"\n        if X is not None:\n            # Compute and store column means of X for centering in _transform\n            self._X_mean = X.mean()\n        else:\n            self._X_mean = 0.0\n        super()._fit(y, X, scale)\n\n    def _transform(self, X: pd.DataFrame, fh: pd.Index) -&gt; Any:\n        \"\"\"\n        (Optional) Transform phase: prepares data for `_predict`.\n\n        This example implementation subtracts the stored mean and then converts\n        the data to a JAX array.\n\n        The `_transform` method can return one of the following structures:\n        - A single `jnp.ndarray`: The simplest and most common case.\n        - A `tuple`: Useful for passing multiple arrays or mixed data types.\n          The first element is typically the main data array.\n        - A `dict`: Flexible for passing named arrays and other metadata.\n          Must contain a 'data' key holding the main `jnp.ndarray`.\n        \"\"\"\n        # Center the data using the mean learned in _fit\n        X_proc = X - self._X_mean\n        # Convert to JAX tensor/array\n        return series_to_tensor_or_array(X_proc)\n\n    def _predict(\n        self,\n        data: Any,\n        predicted_effects: Dict[str, jnp.ndarray],\n        *args,\n        **kwargs,\n    ) -&gt; jnp.ndarray:\n        \"\"\"\n        (Mandatory) Prediction phase: core effect computation.\n\n        This is where the main effect logic happens. Use numpyro.sample()\n        to define Bayesian priors for parameters.\n\n        The `data` argument receives the output of `_transform`. Your implementation\n        should handle the structure you defined:\n        - If `_transform` returns a `jnp.ndarray`, `data` will be that array.\n        - If `_transform` returns a `tuple`, `data` will be that tuple.\n        - If `_transform` returns a `dict`, `data` will be that dictionary.\n        \"\"\"\n        # Sample a coefficient from the prior defined in __init__\n        coef = numpyro.sample(\"my_custom_coef\", self._prior)\n\n        # The effect's computation.\n        # This example creates a linear effect with the centered data.\n        effect = data * coef * self.multiplier\n\n        return effect\n\n    @classmethod\n    def get_test_params(cls, parameter_set: str = \"default\"):\n        \"\"\"\n        (Optional) Provide test parameters for Prophetverse's testing framework.\n        \"\"\"\n        return [{\"multiplier\": 2.0, \"prior_scale\": 1.0}]\n\n\n# --- Advanced: Additive vs Multiplicative Effects ---\n#\n# For effects that can switch between additive and multiplicative modes,\n# inherit from BaseAdditiveOrMultiplicativeEffect instead of BaseEffect.\n#\n# Example:\n# from prophetverse.effects.base import BaseAdditiveOrMultiplicativeEffect\n#\n# class AdstockEffect(BaseAdditiveOrMultiplicativeEffect):\n#     def __init__(self, effect_mode=\"multiplicative\", **kwargs):\n#         super().__init__(effect_mode=effect_mode, **kwargs)\n#\n#     def _predict(self, data, predicted_effects, **kwargs):\n#         # Your core logic here\n#         adstock_rate = numpyro.sample(\"adstock\", dist.Beta(1, 1))\n#         # The base class handles additive vs multiplicative application\n#         return apply_adstock(data, adstock_rate)\n\n# --- Tips for Creating Custom Effects ---\n#\n# 1. Start with MySimpleEffect template for basic transformations.\n# 2. Use MyCustomEffect template when you need parameter fitting.\n# 3. Check existing effects in prophetverse.effects for inspiration:\n#    - LinearEffect: Simple linear regression\n#    - HillEffect: Hill saturation transformation\n#    - GeometricAdstockEffect: Media adstock modeling\n# 4. Use descriptive parameter names in numpyro.sample().\n# 5. Test your effect with get_test_params() method.\n# 6. Consider panel data capabilities if working with multiple time series."
  },
  {
    "objectID": "tutorials/count_data.html",
    "href": "tutorials/count_data.html",
    "title": "Forecasting count data",
    "section": "",
    "text": "This tutorial shows how to use the prophetverse library to model count data, with a prophet-like model that uses Negative Binomial likelihood. Many timeseries are composed of counts, which are non-negative integers. For example, the number of cars that pass through a toll booth in a given hour, the number of people who visit a website in a given day, or the number of sales of a product. The original Prophet model struggles to handle this type of data, as it assumes that the data is continuous and normally distributed.\n# Disable warnings\nimport warnings\n\nwarnings.simplefilter(action=\"ignore\")\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom numpyro import distributions as dist\nfrom sktime.forecasting.compose import ForecastingPipeline\nfrom sktime.transformations.series.fourier import FourierFeatures\n\n\nfrom prophetverse.datasets.loaders import load_pedestrian_count\nimport numpyro\n\nnumpyro.enable_x64()",
    "crumbs": [
      "Count data forecasting"
    ]
  },
  {
    "objectID": "tutorials/count_data.html#import-dataset",
    "href": "tutorials/count_data.html#import-dataset",
    "title": "Forecasting count data",
    "section": "Import dataset",
    "text": "Import dataset\nWe use a dataset Melbourne Pedestrian Counts from forecastingdata, which contains the hourly pedestrian counts in Melbourne, Australia, from a set of sensors located in different parts of the city.\n\ny = load_pedestrian_count()\n\n# We take only one time series for simplicity\ny = y.loc[\"T2\"]\n\nsplit_index = 24 * 365\ny_train, y_test = y.iloc[:split_index], y.iloc[split_index + 1 : split_index * 2 + 1]\n\nLet’s plot a section of the time series to see how it looks like:\n\ndisplay(y_train.head())\ny_train.iloc[: 24 * 21].plot(figsize=(10, 3), marker=\"o\", color=\"black\", legend=True)\nplt.show()\n\n\n\n\n\n\n\n\npedestrian_count\n\n\ntimestamp\n\n\n\n\n\n2009-05-01 00:00\n52.0\n\n\n2009-05-01 01:00\n34.0\n\n\n2009-05-01 02:00\n19.0\n\n\n2009-05-01 03:00\n14.0\n\n\n2009-05-01 04:00\n15.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe full dataset is actually large, and plotting it all at once does not help a lot. Either way, let’s plot the full dataset to see how it looks like:\n\nax = y_train[\"pedestrian_count\"].rename(\"Train\").plot(figsize=(20, 7))\ny_test[\"pedestrian_count\"].rename(\"Test\").plot(ax=ax)\nax.legend()\nplt.show()",
    "crumbs": [
      "Count data forecasting"
    ]
  },
  {
    "objectID": "tutorials/count_data.html#fitting-models",
    "href": "tutorials/count_data.html#fitting-models",
    "title": "Forecasting count data",
    "section": "Fitting models",
    "text": "Fitting models\nThe series has some clear patterns: a daily seasonality, a weekly seasonality, and a yearly seasonality. It also has many zeros, and a model assuming normal distributed observations would not be able to capture this.\nFirst, let’s fit and forecast with the standard prophet, then see how the negative binomial model performs.",
    "crumbs": [
      "Count data forecasting"
    ]
  },
  {
    "objectID": "tutorials/count_data.html#prophet-with-normal-likelihood",
    "href": "tutorials/count_data.html#prophet-with-normal-likelihood",
    "title": "Forecasting count data",
    "section": "Prophet with normal likelihood",
    "text": "Prophet with normal likelihood\nIn this case, we will see how the model will output non-sensical negative values. The probabilistic intervals, mainly, will output values much lower than the support of the timeseries.\n\nfrom prophetverse.effects.fourier import LinearFourierSeasonality\nfrom prophetverse.effects.trend import FlatTrend\nfrom prophetverse.engine import MAPInferenceEngine\nfrom prophetverse.engine.optimizer import CosineScheduleAdamOptimizer, LBFGSSolver\n\nfrom prophetverse.sktime import Prophetverse\nfrom prophetverse.utils.regex import no_input_columns\n\n# Here we set the prior for the seasonality effect\n# And the coefficients for it\nexogenous_effects = [\n    (\n        \"seasonality\",\n        LinearFourierSeasonality(\n            sp_list=[24, 24 * 7, 24 * 365.5],\n            fourier_terms_list=[2, 2, 10],\n            freq=\"H\",\n            prior_scale=0.1,\n            effect_mode=\"multiplicative\",\n        ),\n        no_input_columns,\n    ),\n]\n\nmodel = Prophetverse(\n    trend=FlatTrend(),\n    exogenous_effects=exogenous_effects,\n    inference_engine=MAPInferenceEngine(),\n)\nmodel.fit(y=y_train)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[2,\n                                                                              2,\n                                                                              10],\n                                                          freq='H',\n                                                          prior_scale=0.1,\n                                                          sp_list=[24, 168,\n                                                                   8772.0]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(), trend=FlatTrend())Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[2,\n                                                                              2,\n                                                                              10],\n                                                          freq='H',\n                                                          prior_scale=0.1,\n                                                          sp_list=[24, 168,\n                                                                   8772.0]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(), trend=FlatTrend())effectsFlatTrendFlatTrend()LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[2, 2, 10], freq='H',\n                         prior_scale=0.1, sp_list=[24, 168, 8772.0])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nForecasting with the normal model\nBelow we see the negative predictions, which is clear a limitation of this gaussian likelihood for this kind of data.\n\nforecast_horizon = y_train.index[-100:].union(y_test.index[:300])\nfig, ax = plt.subplots(figsize=(10, 3))\npreds_normal = model.predict(fh=forecast_horizon)\npreds_normal[\"pedestrian_count\"].rename(\"Normal model\").plot.line(\n    ax=ax, legend=False, color=\"tab:blue\"\n)\nax.scatter(y_train.index, y_train, marker=\"o\", color=\"k\", s=2, alpha=0.5, label=\"Train\")\nax.scatter(\n    y_test.index, y_test, marker=\"o\", color=\"green\", s=2, alpha=0.5, label=\"Test\"\n)\nax.set_title(\"Prophet with normal likelihood\")\nax.legend()\nfig.show()\n\n\n\n\n\n\n\n\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.1, 0.9])\nfig, ax = plt.subplots(figsize=(10, 3))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(\n    forecast_horizon.to_timestamp(),\n    y.loc[forecast_horizon],\n    marker=\"o\",\n    color=\"k\",\n    s=2,\n    alpha=1,\n)\nax.axvline(y_train.index[-1].to_timestamp(), color=\"r\", linestyle=\"--\")\nfig.show()",
    "crumbs": [
      "Count data forecasting"
    ]
  },
  {
    "objectID": "tutorials/count_data.html#prophet-with-negative-binomial-likelihood",
    "href": "tutorials/count_data.html#prophet-with-negative-binomial-likelihood",
    "title": "Forecasting count data",
    "section": "Prophet with negative binomial likelihood",
    "text": "Prophet with negative binomial likelihood\nThe negative binomial likehood has support on the non-negative integers, which makes it perfect for count data. We change the likelihood of the model, and fit it again.\n\nmodel.set_params(likelihood=\"negbinomial\")\nmodel.fit(y=y_train)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[2,\n                                                                              2,\n                                                                              10],\n                                                          freq='H',\n                                                          prior_scale=0.1,\n                                                          sp_list=[24, 168,\n                                                                   8772.0]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(), likelihood='negbinomial',\n             trend=FlatTrend())Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[2,\n                                                                              2,\n                                                                              10],\n                                                          freq='H',\n                                                          prior_scale=0.1,\n                                                          sp_list=[24, 168,\n                                                                   8772.0]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(), likelihood='negbinomial',\n             trend=FlatTrend())effectsFlatTrendFlatTrend()LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative',\n                         fourier_terms_list=[2, 2, 10], freq='H',\n                         prior_scale=0.1, sp_list=[24, 168, 8772.0])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\n\nForecasting with the negative binomial model\n\nfig, ax = plt.subplots(figsize=(10, 3))\npreds_negbin = model.predict(fh=forecast_horizon)\npreds_negbin[\"pedestrian_count\"].rename(\"Neg. Binomial model\").plot.line(\n    ax=ax, legend=False, color=\"tab:purple\"\n)\nax.scatter(y_train.index, y_train, marker=\"o\", color=\"k\", s=2, alpha=0.5, label=\"Train\")\nax.scatter(\n    y_test.index, y_test, marker=\"o\", color=\"green\", s=2, alpha=0.5, label=\"Test\"\n)\nax.set_title(\"Prophet with Negative Binomial likelihood\")\nax.legend()\nfig.show()\n\n\n\n\n\n\n\n\n\nquantiles = model.predict_quantiles(fh=forecast_horizon, alpha=[0.1, 0.9])\nfig, ax = plt.subplots(figsize=(10, 3))\n# Plot area between quantiles\nax.fill_between(\n    quantiles.index.to_timestamp(),\n    quantiles.iloc[:, 0],\n    quantiles.iloc[:, -1],\n    alpha=0.5,\n)\nax.scatter(\n    forecast_horizon.to_timestamp(),\n    y.loc[forecast_horizon],\n    marker=\"o\",\n    color=\"k\",\n    s=2,\n    alpha=1,\n)\nax.axvline(y_train.index[-1].to_timestamp(), color=\"r\", linestyle=\"--\")\nfig.show()",
    "crumbs": [
      "Count data forecasting"
    ]
  },
  {
    "objectID": "tutorials/count_data.html#comparing-both-forecasts-side-by-side",
    "href": "tutorials/count_data.html#comparing-both-forecasts-side-by-side",
    "title": "Forecasting count data",
    "section": "Comparing both forecasts side by side",
    "text": "Comparing both forecasts side by side\nTo make our point clear, we plot both forecasts side by side. Isn’t it nice to have forecasts that make sense? :smile:\n\nfig, ax = plt.subplots(figsize=(9, 5))\npreds_negbin[\"pedestrian_count\"].rename(\"Neg. Binomial model\").plot.line(\n    ax=ax, legend=False, color=\"tab:purple\"\n)\npreds_normal[\"pedestrian_count\"].rename(\"Normal model\").plot.line(\n    ax=ax, legend=False, color=\"tab:blue\"\n)\nax.scatter(y_train.index, y_train, marker=\"o\", color=\"k\", s=6, alpha=0.5, label=\"Train\")\nax.scatter(\n    y_test.index, y_test, marker=\"o\", color=\"green\", s=6, alpha=0.5, label=\"Test\"\n)\nax.set_title(\"Forecasting pedestrian counts\")\n# Remove xlabel\nax.set_xlabel(\"\")\nax.axvline(\n    y_train.index[-1].to_timestamp(),\n    color=\"black\",\n    linestyle=\"--\",\n    alpha=0.3,\n    zorder=-1,\n)\nfig.legend(loc=\"center\", ncol=4, bbox_to_anchor=(0.5, 0.8))\nfig.tight_layout()\nfig.show()",
    "crumbs": [
      "Count data forecasting"
    ]
  },
  {
    "objectID": "tutorials/count_data.html#prophet-with-hurdle-negative-binomial-likelihood",
    "href": "tutorials/count_data.html#prophet-with-hurdle-negative-binomial-likelihood",
    "title": "Forecasting count data",
    "section": "Prophet with hurdle negative binomial likelihood",
    "text": "Prophet with hurdle negative binomial likelihood\nWe can also use Hurdle Likelihood, which is a two-part model that separately models the zero counts and the positive counts. This is useful when there is interest in studying the non-zero counts separately, or when the zero counts are generated by a different process than the positive counts.\nLet’s artificially add some zeros to the training data to illustrate this.\n\ndef _add_zeros(y):\n    y = y.copy()\n    y[y &lt; 500] = 0\n    return y\n\n\ny_train = _add_zeros(y_train)\ny_test = _add_zeros(y_test)\n\nNow we fit the model with the hurdle negative binomial likelihood.\n\nfrom prophetverse.effects.target.hurdle import HurdleTargetLikelihood\n\nfrom prophetverse.effects.constant import Constant\nimport jax.numpy as jnp\n\nseasonality = LinearFourierSeasonality(\n    sp_list=[24, 24 * 7, 24 * 365.5],\n    fourier_terms_list=[2, 2, 10],\n    freq=\"H\",\n    prior_scale=1,\n    effect_mode=\"additive\",\n)\nexogenous_effects = [\n    (\n        \"seasonality\",\n        seasonality,\n        no_input_columns,\n    ),\n    (\"zero_proba/constant_term\", Constant(prior=dist.Normal(0.5, 1)), no_input_columns),\n    (\"zero_proba/seasonality\", seasonality, no_input_columns),\n]\n\nmodel = Prophetverse(\n    trend=FlatTrend(),\n    exogenous_effects=exogenous_effects,\n    inference_engine=MAPInferenceEngine(),\n    likelihood=HurdleTargetLikelihood(\n        likelihood_family=\"negbinomial\",\n    ),\n)\nmodel.fit(y=y_train)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(fourier_terms_list=[2,\n                                                                              2,\n                                                                              10],\n                                                          freq='H',\n                                                          prior_scale=1,\n                                                          sp_list=[24, 168,\n                                                                   8772.0]),\n                                 '^$'),\n                                ('zero_proba/constant_term',\n                                 Constant(prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f01d0ab3b90 with batch shape () and event shape ()&gt;),\n                                 '^$'),\n                                ('zero_proba/seasonality',\n                                 LinearFourierSeasonality(fourier_terms_list=[2,\n                                                                              2,\n                                                                              10],\n                                                          freq='H',\n                                                          prior_scale=1,\n                                                          sp_list=[24, 168,\n                                                                   8772.0]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             likelihood=HurdleTargetLikelihood(likelihood_family='negbinomial'),\n             trend=FlatTrend())Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(fourier_terms_list=[2,\n                                                                              2,\n                                                                              10],\n                                                          freq='H',\n                                                          prior_scale=1,\n                                                          sp_list=[24, 168,\n                                                                   8772.0]),\n                                 '^$'),\n                                ('zero_proba/constant_term',\n                                 Constant(prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f01d0ab3b90 with batch shape () and event shape ()&gt;),\n                                 '^$'),\n                                ('zero_proba/seasonality',\n                                 LinearFourierSeasonality(fourier_terms_list=[2,\n                                                                              2,\n                                                                              10],\n                                                          freq='H',\n                                                          prior_scale=1,\n                                                          sp_list=[24, 168,\n                                                                   8772.0]),\n                                 '^$')],\n             inference_engine=MAPInferenceEngine(),\n             likelihood=HurdleTargetLikelihood(likelihood_family='negbinomial'),\n             trend=FlatTrend())effectsFlatTrendFlatTrend()LinearFourierSeasonalityLinearFourierSeasonality(fourier_terms_list=[2, 2, 10], freq='H', prior_scale=1,\n                         sp_list=[24, 168, 8772.0])ConstantConstant(prior=&lt;numpyro.distributions.continuous.Normal object at 0x7f01d0ab3b90 with batch shape () and event shape ()&gt;)LinearFourierSeasonalityLinearFourierSeasonality(fourier_terms_list=[2, 2, 10], freq='H', prior_scale=1,\n                         sp_list=[24, 168, 8772.0])inference_engineMAPInferenceEngineMAPInferenceEngine()\n\n\nWith predict_components, we can see the different components of the model, including the demand component and the gate (non-zero probability) component.\n\ny_pred_components = model.predict_components(fh=forecast_horizon)\ny_pred_components[[\"mean\", \"demand\", \"gate\"]].head()\n\n\n\n\n\n\n\n\nmean\ndemand\ngate\n\n\ntimestamp\n\n\n\n\n\n\n\n2010-04-26 20:00\n87.269\n380.611024\n0.227347\n\n\n2010-04-26 21:00\n19.723\n351.633911\n0.076361\n\n\n2010-04-26 22:00\n18.181\n587.727951\n0.026813\n\n\n2010-04-26 23:00\n7.854\n1004.863890\n0.009931\n\n\n2010-04-27 00:00\n9.424\n1475.831470\n0.003809\n\n\n\n\n\n\n\nPlotting the results:\n\nfig, ax = plt.subplots(figsize=(9, 5))\n\ny_pred_components[\"mean\"].rename(\"Hurdle Neg. Binomial model\").plot.line(\n    ax=ax, legend=False, color=\"tab:green\"\n)\ny_pred_components[\"demand\"].rename(\"Hurdle Neg. Binomial model - Demand component\").plot.line(\n    ax=ax, legend=False, color=\"tab:red\", linestyle=\"--\"\n)\nax.scatter(y_train.index, y_train, marker=\"o\", color=\"k\", s=6, alpha=0.5, label=\"Train\")\nax.scatter(\n    y_test.index, y_test, marker=\"o\", color=\"green\", s=6, alpha=0.5, label=\"Test\"\n)\nax.set_title(\"Forecasting pedestrian counts\")\n# Remove xlabel\nax.set_xlabel(\"\")\nax.axvline(\n    y_train.index[-1].to_timestamp(),\n    color=\"black\",\n    linestyle=\"--\",\n    alpha=0.3,\n    zorder=-1,\n)\nfig.legend(loc=\"center\", ncol=2, bbox_to_anchor=(0.5, 0.8))\nfig.tight_layout()\nfig.show()",
    "crumbs": [
      "Count data forecasting"
    ]
  },
  {
    "objectID": "tutorials/hierarchical.html",
    "href": "tutorials/hierarchical.html",
    "title": "Hierarchical Bayesian Model",
    "section": "",
    "text": "In this example, we will show how to forecast panel timeseries with the Prophetverse model.\nThe univariate Prophetverse model can seamlessly handle hierarchical timeseries due to the package’s compatibility with sktime.\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom prophetverse.datasets.loaders import load_tourism",
    "crumbs": [
      "Hierarchical Bayesian Model"
    ]
  },
  {
    "objectID": "tutorials/hierarchical.html#import-dataset",
    "href": "tutorials/hierarchical.html#import-dataset",
    "title": "Hierarchical Bayesian Model",
    "section": "Import dataset",
    "text": "Import dataset\nHere we use the tourism dataset with purpose-level aggregation.\n\ny = load_tourism(groupby=\"Purpose\")\ndisplay(y)\n\n\n\n\n\n\n\n\n\nTrips\n\n\nPurpose\nQuarter\n\n\n\n\n\nBusiness\n1998Q1\n7391.962068\n\n\n1998Q2\n7701.153191\n\n\n1998Q3\n8911.852065\n\n\n1998Q4\n7777.766525\n\n\n1999Q1\n6917.257864\n\n\n...\n...\n...\n\n\n__total\n2015Q4\n51518.858354\n\n\n2016Q1\n54984.720748\n\n\n2016Q2\n49583.595515\n\n\n2016Q3\n49392.159616\n\n\n2016Q4\n54034.155613\n\n\n\n\n380 rows × 1 columns\n\n\n\nWe define the helper function below to plot the predictions and the observations.\n\nLEVELS = y.index.get_level_values(0).unique()\n\n\ndef plot_preds(y=None, preds={}, axs=None):\n\n    if axs is None:\n        fig, axs = plt.subplots(\n            figsize=(12, 8), nrows=int(np.ceil(len(LEVELS) / 2)), ncols=2\n        )\n    ax_generator = iter(axs.flatten())\n    for level in LEVELS:\n        ax = next(ax_generator)\n        if y is not None:\n            y.loc[level].iloc[:, 0].rename(\"Observation\").plot(\n                ax=ax, label=\"truth\", color=\"black\"\n            )\n        for name, _preds in preds.items():\n            _preds.loc[level].iloc[:, 0].rename(name).plot(ax=ax, legend=True)\n        ax.set_title(level)\n\n    # Tight layout\n    plt.tight_layout()\n    return ax",
    "crumbs": [
      "Hierarchical Bayesian Model"
    ]
  },
  {
    "objectID": "tutorials/hierarchical.html#automatic-upcasting",
    "href": "tutorials/hierarchical.html#automatic-upcasting",
    "title": "Hierarchical Bayesian Model",
    "section": "Automatic upcasting",
    "text": "Automatic upcasting\nBecause of sktime’s amazing interface, we can use the univariate Prophet seamlessly with hierarchical data.\n\nimport jax.numpy as jnp\n\nfrom prophetverse.effects import LinearFourierSeasonality\nfrom prophetverse.effects.trend import PiecewiseLinearTrend, PiecewiseLogisticTrend\nfrom prophetverse.engine import MAPInferenceEngine, MCMCInferenceEngine\nfrom prophetverse.sktime.univariate import Prophetverse\nfrom prophetverse.utils import no_input_columns\nfrom prophetverse.engine.optimizer import LBFGSSolver\n\nmodel = Prophetverse(\n    trend=PiecewiseLogisticTrend(\n        changepoint_prior_scale=0.1,\n        changepoint_interval=8,\n        changepoint_range=-8,\n    ),\n    exogenous_effects=[\n        (\n            \"seasonality\",\n            LinearFourierSeasonality(\n                sp_list=[\"YE\"],\n                fourier_terms_list=[1],\n                freq=\"Q\",\n                prior_scale=0.1,\n                effect_mode=\"multiplicative\",\n            ),\n            no_input_columns,\n        )\n    ],\n    inference_engine=MCMCInferenceEngine(\n        num_warmup=500,\n        num_samples=1000,\n    ),\n)\nmodel.fit(y=y)\n\nProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[1],\n                                                          freq='Q',\n                                                          prior_scale=0.1,\n                                                          sp_list=['YE']),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=500),\n             trend=PiecewiseLogisticTrend(changepoint_interval=8,\n                                          changepoint_prior_scale=0.1,\n                                          changepoint_range=-8))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[1],\n                                                          freq='Q',\n                                                          prior_scale=0.1,\n                                                          sp_list=['YE']),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=500),\n             trend=PiecewiseLogisticTrend(changepoint_interval=8,\n                                          changepoint_prior_scale=0.1,\n                                          changepoint_range=-8))effectsPiecewiseLogisticTrendPiecewiseLogisticTrend(changepoint_interval=8, changepoint_prior_scale=0.1,\n                       changepoint_range=-8)LinearFourierSeasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[1],\n                         freq='Q', prior_scale=0.1, sp_list=['YE'])inference_engineMCMCInferenceEngineMCMCInferenceEngine(num_warmup=500)\n\n\nWe can see how, internally, sktime creates clones of the model for each timeseries instance:\n\nmodel.forecasters_\n\n\n\n\n\n\n\n\nforecasters\n\n\n\n\nBusiness\nProphetverse(exogenous_effects=[('seasonality'...\n\n\nHoliday\nProphetverse(exogenous_effects=[('seasonality'...\n\n\nOther\nProphetverse(exogenous_effects=[('seasonality'...\n\n\nVisiting\nProphetverse(exogenous_effects=[('seasonality'...\n\n\n__total\nProphetverse(exogenous_effects=[('seasonality'...\n\n\n\n\n\n\n\nTo call the same methods we used in the univariate case, we do not need to change a single line of code. The only difference is that the output will be a pd.DataFrame with more rows and index levels.\n\nforecast_horizon = pd.period_range(\"1997Q1\",\n                                   \"2020Q4\",\n                                   freq=\"Q\")\npreds = model.predict(fh=forecast_horizon)\ndisplay(preds.head())\n\n# Plot\nplot_preds(y, {\"Prophet\": preds})\nplt.show()\n\n\n\n\n\n\n\n\n\nTrips\n\n\nPurpose\nQuarter\n\n\n\n\n\nBusiness\n1997Q1\n7063.016602\n\n\n1997Q2\n7994.685059\n\n\n1997Q3\n8913.966797\n\n\n1997Q4\n7973.962891\n\n\n1998Q1\n7063.016602\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe same applies to the decomposition method:\n\ndecomposition = model.predict_components(fh=forecast_horizon)\ndecomposition.head()\n\n\n\n\n\n\n\n\n\nmean\nobs\nseasonality\ntrend\n\n\n\n\nBusiness\n1997Q1\n7063.016602\n7069.995605\n-925.291077\n7988.305176\n\n\n1997Q2\n7994.685059\n7956.181641\n6.377406\n7988.305176\n\n\n1997Q3\n8913.966797\n8932.327148\n925.661316\n7988.305176\n\n\n1997Q4\n7973.962891\n7947.743164\n-14.344172\n7988.305176\n\n\n1998Q1\n7063.016602\n7048.377930\n-925.291077\n7988.305176",
    "crumbs": [
      "Hierarchical Bayesian Model"
    ]
  },
  {
    "objectID": "tutorials/hierarchical.html#hierarchical-bayesian-model",
    "href": "tutorials/hierarchical.html#hierarchical-bayesian-model",
    "title": "Hierarchical Bayesian Model",
    "section": "Hierarchical Bayesian model",
    "text": "Hierarchical Bayesian model\nSometimes, we want to capture patterns shared between the different series, such as seasonality or trend. In this case, we can use a Bayesian Hierarchical Model.\nA Bayesian Hierarchical Model sets hyperpriors: global priors over the priors for each timeseries. These hyperpriors allow us to share information across the different series, which can lead to better forecasts, especially when some series have very few observations.\nTo do that, we just need to use PanelBHLinearEffect as the seasonality__linear_effect parameter in the model. This effect will automatically create a hierarchical model for the linear effects, allowing us to share information across the different series. Also, let us set the broadcast_mode to “off” to use a single model to all the series.\n\nfrom prophetverse.effects.linear import PanelBHLinearEffect\n\nfrom numpyro import distributions as dist\n\nmodel_hier = model.clone()\nmodel_hier.set_params(\n    seasonality__linear_effect=PanelBHLinearEffect(\n        scale_hyperprior=dist.HalfNormal(0.1)\n    ),\n    broadcast_mode=\"effect\",\n)\nmodel_hier.fit(y=y)\n\nProphetverse(broadcast_mode='effect',\n             exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[1],\n                                                          freq='Q',\n                                                          linear_effect=PanelBHLinearEffect(scale_hyperprior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f0074404290 with batch shape () and event shape ()&gt;),\n                                                          prior_scale=0.1,\n                                                          sp_list=['YE']),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=500),\n             trend=PiecewiseLogisticTrend(changepoint_interval=8,\n                                          changepoint_prior_scale=0.1,\n                                          changepoint_range=-8))Please rerun this cell to show the HTML repr or trust the notebook.ProphetverseProphetverse(broadcast_mode='effect',\n             exogenous_effects=[('seasonality',\n                                 LinearFourierSeasonality(effect_mode='multiplicative',\n                                                          fourier_terms_list=[1],\n                                                          freq='Q',\n                                                          linear_effect=PanelBHLinearEffect(scale_hyperprior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f0074404290 with batch shape () and event shape ()&gt;),\n                                                          prior_scale=0.1,\n                                                          sp_list=['YE']),\n                                 '^$')],\n             inference_engine=MCMCInferenceEngine(num_warmup=500),\n             trend=PiecewiseLogisticTrend(changepoint_interval=8,\n                                          changepoint_prior_scale=0.1,\n                                          changepoint_range=-8))effectsPiecewiseLogisticTrendPiecewiseLogisticTrend(changepoint_interval=8, changepoint_prior_scale=0.1,\n                       changepoint_range=-8)seasonalityLinearFourierSeasonality(effect_mode='multiplicative', fourier_terms_list=[1],\n                         freq='Q',\n                         linear_effect=PanelBHLinearEffect(scale_hyperprior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f0074404290 with batch shape () and event shape ()&gt;),\n                         prior_scale=0.1, sp_list=['YE'])linear_effect: PanelBHLinearEffectPanelBHLinearEffect(scale_hyperprior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f0074404290 with batch shape () and event shape ()&gt;)PanelBHLinearEffectPanelBHLinearEffect(scale_hyperprior=&lt;numpyro.distributions.continuous.HalfNormal object at 0x7f0074404290 with batch shape () and event shape ()&gt;)inference_engineMCMCInferenceEngineMCMCInferenceEngine(num_warmup=500)\n\n\n\npreds_hier = model_hier.predict(fh=forecast_horizon)\n\nplot_preds(\n    y,\n    preds={\n        \"Prophet\": preds,\n        \"HierarchicalProphet\": preds_hier,\n    },\n)",
    "crumbs": [
      "Hierarchical Bayesian Model"
    ]
  }
]